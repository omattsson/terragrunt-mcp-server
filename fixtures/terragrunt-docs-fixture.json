[
  {
    "title": "Quick Start",
    "url": "https://terragrunt.gruntwork.io/docs/getting-started/quick-start/",
    "content": "Quick Start Install TerragruntSection titled “Install Terragrunt” If you haven’t already installed Terragrunt, you can do so by following the instructions in the Install Terragrunt guide. Add terragrunt.hcl to your projectSection titled “Add terragrunt.hcl to your project” If you are currently using OpenTofu or Terraform, and you want to start using Terragrunt in your project, simply run the following where your OpenTofu project is located: touch terragrunt.hcl This creates an empty Terragrunt configuration file in the directory where you are using OpenTofu. You can now start using terragrunt instead of tofu or terraform to run your OpenTofu/Terraform commands as if you were simply using OpenTofu or Terraform. Depending on why you’re looking to adopt Terragrunt, this may be all you need to do! With just this empty file, you’ve already made it so that you no longer need to run tofu init or terraform init before running tofu apply or terraform apply. Terragrunt will automatically run init for you if necessary. This is a feature called Auto-init. This might not be very impressive so far, so you may be wondering why one might want to start using Terragrunt to manage their OpenTofu/Terraform projects. The next section will give you a very gentle introduction to using Terragrunt, and show you how you can start to leverage Terragrunt to manage your OpenTofu/Terraform projects more effectively. TutorialSection titled “Tutorial” What follows is a gentle step-by-step guide to integrating Terragrunt into a new (or existing) OpenTofu/Terraform project. For the sake of this tutorial, a minimal set of OpenTofu configurations will be used so that you can follow along. Following these steps will give you an idea of how to integrate Terragrunt into an existing project, even if yours is more complex. This tutorial will assume the following: You have OpenTofu or Terraform installed*. You have a basic understanding of what OpenTofu/Terraform do. You are using a Unix-like operating system. This tutorial will not assume the following: You have any subscriptions to any cloud providers. You have any experience with Terragrunt. You have any existing Terragrunt, OpenTofu or Terraform projects. * Note that if you have both OpenTofu and Terraform installed, you’ll want to read the tf-path docs to understand how Terragrunt determines which binary to use. If you would like a less gentle introduction geared towards users with an active AWS account, familiarity with OpenTofu/Terraform, and potentially a team actively using Terragrunt, consider starting with the Overview. If you start to feel lost, or don’t understand a concept, consider reading the Terminology page before continuing with this tutorial. It has a brief overview of most of the common terms used when discussing Terragrunt. Finally, note that all of the files created in this tutorial can be copied directly from the code block, none of them are partial files, so you don’t have to worry about figuring out where to put the code. Just copy and paste! You can also see what to expect in your filesystem at each step here. Create a new Terragrunt project Let’s say you have the following main.tf in directory foo: resource \"local_file\" \"file\" { content = \"Hello, World!\" filename = \"${path.module}/hi.txt\"} As we learned above, integrating this OpenTofu project with Terragrunt is as simple as creating a terragrunt.hcl file in the same directory: touch foo/terragrunt.hcl You can now run terragrunt commands within the foo directory, as if you were using tofu or terraform. $ cd foo$ terragrunt apply -auto-approve18:44:26.066 STDOUT tofu: Initializing the backend...18:44:26.067 STDOUT tofu: Initializing provider plugins...18:44:26.067 STDOUT tofu: - Finding latest version of hashicorp/local...18:44:26.717 STDOUT tofu: - Installing hashicorp/local v2.5.2...18:44:27.033 STDOUT tofu: - Installed hashicorp/local v2.5.2 (signed, key ID 0C0AF313E5FD9F80)18:44:27.033 STDOUT tofu: Providers are signed by their developers.18:44:27.033 STDOUT tofu: If you'd like to know more about provider signing, you can read about it here:18:44:27.033 STDOUT tofu: https://opentofu.org/docs/cli/plugins/signing/18:44:27.034 STDOUT tofu: OpenTofu has created a lock file .terraform.lock.hcl to record the provider18:44:27.034 STDOUT tofu: selections it made above. Include this file in your version control repository18:44:27.034 STDOUT tofu: so that OpenTofu can guarantee to make the same selections by default when18:44:27.034 STDOUT tofu: you run \"tofu init\" in the future.18:44:27.034 STDOUT tofu: OpenTofu has been successfully initialized!18:44:27.035 STDOUT tofu:18:44:27.035 STDOUT tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see18:44:27.035 STDOUT tofu: any changes that are required for your infrastructure. All OpenTofu commands18:44:27.035 STDOUT tofu: should now work.18:44:27.035 STDOUT tofu: If you ever set or change modules or backend configuration for OpenTofu,18:44:27.035 STDOUT tofu: rerun this command to reinitialize your working directory. If you forget, other18:44:27.035 STDOUT tofu: commands will detect it and remind you to do so if necessary.18:44:27.362 STDOUT tofu: OpenTofu used the selected providers to generate the following execution18:44:27.362 STDOUT tofu: plan. Resource actions are indicated with the following symbols:18:44:27.362 STDOUT tofu: + create18:44:27.362 STDOUT tofu: OpenTofu will perform the following actions:18:44:27.362 STDOUT tofu: # local_file.file will be created18:44:27.362 STDOUT tofu: + resource \"local_file\" \"file\" {18:44:27.362 STDOUT tofu: + content = \"Hello, World!\"18:44:27.362 STDOUT tofu: + content_base64sha256 = (known after apply)18:44:27.362 STDOUT tofu: + content_base64sha512 = (known after apply)18:44:27.362 STDOUT tofu: + content_md5 = (known after apply)18:44:27.362 STDOUT tofu: + content_sha1 = (known after apply)18:44:27.362 STDOUT tofu: + content_sha256 = (known after apply)18:44:27.362 STDOUT tofu: + content_sha512 = (known after apply)18:44:27.362 STDOUT tofu: + directory_permission = \"0777\"18:44:27.362 STDOUT tofu: + file_permission = \"0777\"18:44:27.362 STDOUT tofu: + filename = \"./hi.txt\"18:44:27.362 STDOUT tofu: + id = (known after apply)18:44:27.362 STDOUT tofu: }18:44:27.362 STDOUT tofu: Plan: 1 to add, 0 to change, 0 to destroy.18:44:27.362 STDOUT tofu:18:44:27.383 STDOUT tofu: local_file.file: Creating...18:44:27.384 STDOUT tofu: local_file.file: Creation complete after 0s [id=0a0a9f2a6772942557ab5355d76af442f8f65e01]18:44:27.392 STDOUT tofu:18:44:27.392 STDOUT tofu: Apply complete! Resources: 1 added, 0 changed, 0 destroyed.18:44:27.392 STDOUT tofu: You might notice that this is a little more verbose than the output you’re used to seeing from running tofu or terraform directly. This is because Terragrunt does a bit of work behind the scenes to make sure that you can scale your OpenTofu/Terraform usage without running into common problems. As you get more comfortable with using Terragrunt on larger projects, you may find the extra information helpful. If you prefer that Terragrunt terminal output look more like that from tofu or terraform, you can use the --log-format bare flag (or set the environment variable TG_LOG_FORMAT=bare) to reduce the verbosity of the output. e.g. $ terragrunt --log-format bare applylocal_file.file: Refreshing state... [id=0a0a9f2a6772942557ab5355d76af442f8f65e01] No changes. Your infrastructure matches the configuration. OpenTofu has compared your real infrastructure against your configuration andfound no differences, so no changes are needed. Apply complete! Resources: 0 added, 0 changed, 0 destroyed. The way dynamicity is handled in OpenTofu is via variable configuration blocks. Let’s add one to our main.tf so that we can control the content of the file we’re creating: variable \"content\" {} resource \"local_file\" \"file\" { content = var.content filename = \"${path.module}/hi.txt\"} Now, just like when using tofu alone, you can pass in the value for the content variable using the -var flag: terragrunt apply -auto-approve -var content='Hello, Terragrunt!' This is a common pattern when working with Infrastructure as Code (IaC). You typically create IaC that is relatively static, and then as you need to make configurations dynamic, you add variables to your configuration files to introduce dynamicity. Add a new Terragrunt unit In the context of Terragrunt, a “unit” is a directory that contains a terragrunt.hcl file, and it represents a single piece of infrastructure. You can think of a unit as a single instance of an OpenTofu/Terraform module. Let’s create a copy of the foo directory and call it bar: cd ..cp -r foo bar We now have two identical units in our project, foo and bar. We also have identical code in each of these directories, which is not ideal if we want to be able to avoid duplicating effort when we make changes to our infrastructure. Create a shared module To avoid this duplication, we can introduce a new shared directory, and reference that directory from both foo and bar. This way, we can make changes to our infrastructure in one place and have those changes apply to both units. Let’s create a new directory called shared: mkdir shared Now, move the main.tf file from foo to shared: mv foo/main.tf shared/main.tf Finally, let’s update the foo and bar directories to reference the shared directory. Update the main.tf files in both foo and bar to look like this: variable \"content\" {} module \"shared\" { source = \"../shared\" content = var.content} variable \"content\" {} module \"shared\" { source = \"../shared\" content = var.content} There’s now one place where the logic for the resource local_file.file is defined, and both foo and bar reference that logic. You can imagine that as your infrastructure grows, it can become more and more advantageous to put repeated logic into shared modules like this. This setup does have some problems, however. While you could keep navigating to the different units and running terragrunt apply in each one with the appropriate -var flags, this can quickly become tedious, as you have to know which units require which set of vars applied. You might decide to work around this by creating a file named terraform.tfvars in each unit directory, but this also comes with some limitations that Terragrunt can help you avoid. Use Terragrunt to manage your units Luckily, Terragrunt has a built-in feature to control the inputs passed to your OpenTofu/Terraform configurations. This feature is called (aptly enough) inputs. Let’s add inputs to both terragrunt.hcl files in the foo and bar directories: inputs = { content = \"Hello from foo, Terragrunt!\"} inputs = { content = \"Hello from bar, Terragrunt!\"} You don’t have to maintain the extra main.tf files just to instantiate the module blocks. You can use the terraform block to handle this for you. Update the terragrunt.hcl files in foo and bar to look like this: terraform { source = \"../shared\"} inputs = { content = \"Hello from foo, Terragrunt!\"} terraform { source = \"../shared\"} inputs = { content = \"Hello from bar, Terragrunt!\"} And you can delete the main.tf files from both foo and bar: rm foo/main.tf bar/main.tf This saves you some duplicated content, as you no longer need to maintain that extra content variable in each main.tf file. You can imagine that for especially large modules, the ability to define inputs in the terragrunt.hcl file can save you a lot of time and effort. The patterns for your infrastructure are exclusively defined in .tf files now, and the terragrunt.hcl files are used to manage the instances of those patterns as units. If you run terragrunt apply -auto-approve in the foo and bar directories, you’ll see that the content variable is set to the value you defined in the inputs block of the terragrunt.hcl file. You might also notice that there’s now a special .terragrunt-cache directory generated for you in each unit directory. This is where Terragrunt copies the contents of modules, and performs any necessary additional code generation to make sure that your OpenTofu/Terraform code is ready to be run. The .terragrunt-cache directory is typically added to .gitignore files, similar to the .terraform directory that OpenTofu generates. Use Terragrunt to manage your stacks In the context of Terragrunt, a “stack” is a collection of units that are managed together. You can think of a stack as a single environment, such as dev, staging, or prod, or an entire project. One of the main reasons users adopt Terragrunt is that it can help manage the complexity of managing multiple units across multiple environments. e.g. Let’s say we wanted to update both our foo and bar environments simultaneously. In the directory above foo and bar, run the following: $ terragrunt run --all apply08:42:00.150 INFO The stack at . will be processed in the following order for command apply:Group 1- Module ./bar- Module ./foo Are you sure you want to run 'terragrunt apply' in each folder of the stack described above? (y/n) y08:43:10.702 STDOUT [foo] tofu: local_file.file: Refreshing state... [id=c4ae21736a6297f44ea86791e528338e9d14a0e9]08:43:10.702 STDOUT [bar] tofu: local_file.file: Refreshing state... [id=f855394a0316da09618c8b1fde7b91e00e759f80]08:43:10.708 STDOUT [bar] tofu: No changes. Your infrastructure matches the configuration.08:43:10.708 STDOUT [bar] tofu: OpenTofu has compared your real infrastructure against your configuration and08:43:10.708 STDOUT [bar] tofu: found no differences, so no changes are needed.08:43:10.708 STDOUT [foo] tofu: No changes. Your infrastructure matches the configuration.08:43:10.708 STDOUT [foo] tofu: OpenTofu has compared your real infrastructure against your configuration and08:43:10.708 STDOUT [foo] tofu: found no differences, so no changes are needed.08:43:10.716 STDOUT [foo] tofu:08:43:10.716 STDOUT [foo] tofu: Apply complete! Resources: 0 added, 0 changed, 0 destroyed.08:43:10.716 STDOUT [foo] tofu:08:43:10.720 STDOUT [bar] tofu:08:43:10.720 STDOUT [bar] tofu: Apply complete! Resources: 0 added, 0 changed, 0 destroyed.08:43:10.720 STDOUT [bar] tofu: This is where that additional verbosity in Terragrunt logging is really handy. You can see that Terragrunt concurrently ran apply -auto-approve in both the foo and bar units. The extra logging for Terragrunt also included information on the names of the units that were processed, and disambiguated the output from each unit. When Terragrunt runs these commands, it creates a .terragrunt-cache directory in each unit’s directory. This cache directory serves as Terragrunt’s scratch directory where it: Downloads your remote OpenTofu/Terraform configurations Runs your OpenTofu/Terraform commands Stores downloaded modules and providers Stores generated files (in this case, the hi.txt file will be created under .terragrunt-cache/[HASH]/[HASH]/hi.txt rather than directly in the foo or bar directories) The .terragrunt-cache directory is typically added to .gitignore files, similar to the .terraform directory that OpenTofu generates. You can safely delete this folder at any time, and Terragrunt will recreate it as necessary. If you want to control where the files are created, you can modify the module to accept an output directory parameter. For example, you can update the shared/main.tf file to: variable \"content\" {}variable \"output_dir\" {} resource \"local_file\" \"file\" { content = var.content filename = \"${var.output_dir}/hi.txt\"} Then in your foo/terragrunt.hcl and bar/terragrunt.hcl files, you can use the get_terragrunt_dir() built-in function to get the directory where the terragrunt.hcl file is located: terraform { source = \"../shared\"} inputs = { output_dir = get_terragrunt_dir() content = \"Hello from bar, Terragrunt!\"} With this configuration, the hi.txt files will be created directly in the foo and bar directories instead of the .terragrunt-cache directory. Similar to the tofu CLI, there is a prompt to confirm that you are sure you want to run the command in each unit when performing a command that’s potentially destructive. You can skip this prompt by using the --non-interactive flag, just as you would with -auto-approve in OpenTofu. terragrunt run --all --non-interactive apply Use Terragrunt to manage your DAG In the context of Terragrunt, a Directed Acyclic Graph (DAG) is a data structure that represents the relationship between units in your stack, as determined by their dependencies. Don’t worry if that doesn’t make sense right now. The important thing to know is that Terragrunt uses the DAG to determine the order in which it performs runs across your stack. Once you see how Terragrunt uses the DAG to determine the order in which to run commands across your stack, you’ll understand why this is important. For example, let’s say that the content of the bar unit depended on the content of the foo unit. You can express this dependency first by adding an output block to the shared module: output \"content\" { value = local_file.file.content} Then, you can update the bar unit to depend on the foo unit by using the dependencies block in the terragrunt.hcl file: terraform { source = \"../shared\"} dependency \"foo\" { config_path = \"../foo\"} inputs = { content = \"Foo content: ${dependency.foo.outputs.content}\"} Being good citizens of the IaC world, we should run a plan before an apply to see what changes Terragrunt will make to our infrastructure (note that you will get an error here. This is expected, and we’ll fix it in the next step): $ terragrunt run --all plan08:57:09.271 INFO The stack at . will be processed in the following order for command plan:Group 1- Module ./foo Group 2- Module ./bar ... 08:57:09.936 ERROR [bar] Module ./bar has finished with an error08:57:09.936 ERROR error occurred: * ./foo/terragrunt.hcl is a dependency of ./bar/terragrunt.hcl but detected no outputs. Either the target module has not been applied yet, or the module has no outputs. If this dependency is accessed before the outputs are ready (which can happen during the planning phase of an unapplied stack), consider using mock_outputs: dependency \"foo\" { config_path = \"../foo\" mock_outputs = { foo_output = \"mock-foo-output\" } } For more info, see: https://terragrunt.gruntwork.io/docs/features/stacks/#unapplied-dependency-and-mock-outputs If you do not require outputs from your dependency, consider using the dependencies block instead: https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#dependencies Oh no! We got an error. This happens because the way in which dependencies are resolved by default in Terragrunt is to run terragrunt output within the dependency for use in the dependent unit. In this case, the foo unit has not been applied yet, so there are no outputs to fetch. You should notice, however, that Terragrunt has already figured out the order in which to run the plan command across the units in your stack. This is what we mean when we say that Terragrunt uses a DAG to determine the order of runs in your stack. Terragrunt analyzes the dependencies in your stack, and determines an order for runs so that outputs are ready to be used as inputs in dependent units. If you decided to run terragrunt run --all apply instead, you would instead see Terragrunt complete the apply in the foo unit first, and then complete the apply in the bar unit, as it’s aware that the bar unit might need some outputs from the foo unit. Use mocks to handle unavailable outputs In this scenario, most Terragrunt users leverage mock_outputs to handle unavailable outputs (see limitations on accessing exposed config). Given that it’s expected that the foo unit won’t be able to provide outputs until it’s applied, you can use the mock_outputs block to provide a placeholder value for the content output during the plan phase. terraform { source = \"../shared\"} dependency \"foo\" { config_path = \"../foo\" mock_outputs = { content = \"Mocked content from foo\" }} inputs = { content = \"Foo content: ${dependency.foo.outputs.content}\"} Re-running the plan command should now complete successfully: $ terragrunt run --all plan09:29:03.461 INFO The stack at . will be processed in the following order for command plan:Group 1- Module ./foo Group 2- Module ./bar ... 09:29:03.644 WARN [bar] Config ./foo/terragrunt.hcl is a dependency of ./bar/terragrunt.hcl that has no outputs, but mock outputs provided and returning those in dependency output. ... 09:29:03.898 STDOUT [bar] tofu: + resource \"local_file\" \"file\" {09:29:03.898 STDOUT [bar] tofu: + content = \"Foo content: Mocked content from foo\"09:29:03.898 STDOUT [bar] tofu: + content_base64sha256 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + content_base64sha512 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + content_md5 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + content_sha1 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + content_sha256 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + content_sha512 = (known after apply)09:29:03.898 STDOUT [bar] tofu: + directory_permission = \"0777\"09:29:03.898 STDOUT [bar] tofu: + file_permission = \"0777\"09:29:03.898 STDOUT [bar] tofu: + filename = \"./hi.txt\"09:29:03.898 STDOUT [bar] tofu: + id = (known after apply)09:29:03.898 STDOUT [bar] tofu: } If you’re concerned about the mock_outputs attribute resulting in invalid configurations, note that during an apply, the outputs of foo will be known, and Terragrunt won’t use mock_outputs to resolve the outputs of foo. $ terragrunt run --all --non-interactive apply ... 09:31:21.587 STDOUT [bar] tofu: + resource \"local_file\" \"file\" {09:31:21.587 STDOUT [bar] tofu: + content = \"Foo content: Hello from foo, Terragrunt!\"09:31:21.587 STDOUT [bar] tofu: + content_base64sha256 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + content_base64sha512 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + content_md5 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + content_sha1 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + content_sha256 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + content_sha512 = (known after apply)09:31:21.587 STDOUT [bar] tofu: + directory_permission = \"0777\"09:31:21.587 STDOUT [bar] tofu: + file_permission = \"0777\"09:31:21.587 STDOUT [bar] tofu: + filename = \"./hi.txt\"09:31:21.587 STDOUT [bar] tofu: + id = (known after apply)09:31:21.587 STDOUT [bar] tofu: } ... You can also be explicit about the fact that you only want to use mock_outputs during the plan phase by specifying that in your dependency configuration: terraform { source = \"../shared\"} dependency \"foo\" { config_path = \"../foo\" mock_outputs = { content = \"Mocked content from foo\" } mock_outputs_allowed_terraform_commands = [\"plan\"]} inputs = { content = \"Foo content: ${dependency.foo.outputs.content}\"} Something a little subtle just happened there. Note that the inputs attribute is dynamic. This addresses some of the limitations mentioned earlier about using terraform.tfvars files to manage inputs for units. Given that the bar unit is dependent on output values from the foo unit, you wouldn’t be able to use a terraform.tfvars file to populate this variable without some additional tooling to populate it dynamically. Terragrunt was spawned organically out of supporting Gruntwork customers using Terraform at scale, and features in the product are designed to address common problems like these that arise when managing OpenTofu/Terraform projects at scale in production. Continue learning and exploring Hopefully, following this simple tutorial has given you confidence in integrating Terragrunt into your existing OpenTofu/Terraform projects. Starting small, and gradually introducing more complex Terragrunt features is a great way to learn how Terragrunt can help you manage your infrastructure more effectively. The next step of the Getting Started guide is to follow the Overview guide. This guide will introduce you to more advanced Terragrunt features, and show you how to use Terragrunt to manage your infrastructure across multiple environments in a real-world AWS account. If you’re ready to get your hands dirty with more advanced Terragrunt features yourself, you can skip ahead to the Features section of the documentation. If you ever need help with a particular problem, take a look at the resources available to you in the Support section. You are especially encouraged to join the Terragrunt Discord server, and become part of the Terragrunt community. Edit page Next Overview",
    "section": "getting-started",
    "lastUpdated": "2025-10-23T16:40:56.824Z"
  },
  {
    "title": "Overview",
    "url": "https://terragrunt.gruntwork.io/docs/getting-started/overview/",
    "content": "Overview The following is a simple overview of the main features in Terragrunt. It includes configurations that are a bit more complex than the ones found in the Quick Start, but don’t panic! We’ll walk you through each one, and you don’t need to understand everything right away. Knowing that these features are available as you start to use Terragrunt can give you a tool to reach for when you encounter common problems that typically require one or more of these solutions. This guide is geared towards users who have either already gone through the Quick Start or are joining a team of users that are already using Terragrunt. As a consequence, we’ll be using more complex configurations, discussing more advanced features, and showing how to use Terragrunt to manage real AWS infrastructure. If you are unfamiliar with OpenTofu/Terraform, you may want to also read OpenTofu or Terraform documentation after reading this guide. Following AlongSection titled “Following Along” What follows isn’t a tutorial in the same sense as the Quick Start, but more of a guided tour of some of the more commonly used features of Terragrunt. You don’t need to follow along to understand the concepts, but if you want to, you can. The code samples provided here are available as individual “steps” here. If you would prefer it, you can clone the Terragrunt repository, and follow along with the examples in your own environment without any copy + paste. Just make sure to replace the values prefixed __FILL_IN_ with values relevant to your AWS account. If you don’t have an AWS account, you can either sign up for a free tier account at aws.amazon.com or adapt the examples to use a different cloud provider. ExampleSection titled “Example” Here is a typical terragrunt.hcl file you might find in a Terragrunt project*: # Configure the remote backendremote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { bucket = \"my-tofu-state\" key = \"tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} # Configure the AWS providergenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} # Configure the module## The URL used here is a shorthand for# \"tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=5.16.0\".## You can find the module at:# https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest## Note the extra `/` after the `tfr` protocol is required for the shorthand# notation.terraform { source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.16.0\"} # Configure the inputs for the moduleinputs = { name = \"my-vpc\" cidr = \"10.0.0.0/16\" azs = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"] private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = false enable_vpn_gateway = false tags = { IaC = \"true\" Environment = \"dev\" }} Try it outSection titled “Try it out” If you want to try this configuration locally: Copy the contents above into a terragrunt.hcl file in an empty directory. Change the value of bucket in the remote_state block to a unique name. This has to be globally unique, so you might want to include today’s date in the name. Ensure that you are authenticated with AWS and have the necessary permissions to create resources. Running aws sts get-caller-identity in the AWS CLI is a good way to confirm this. Run terragrunt apply -auto-approve in the directory where you created the terragrunt.hcl file. If you’re familiar with OpenTofu/Terraform, this should be a pretty familiar experience. For the most part, when you use Terragrunt, you are simply setting up configurations in terragrunt.hcl files that have analogues to what you would define with .tf files, then running terragrunt instead of tofu/terraform on the command line. terragrunt.hclSection titled “terragrunt.hcl” The terragrunt.hcl file above does the following: Remote state backend configurationSection titled “Remote state backend configuration” The remote_state configuration block controls how Terragrunt should store backend OpenTofu/Terraform state. In this example, Terragrunt is being configured to store state in an S3 bucket named my-tofu-state in the us-east-1 region. The state file will be named tofu.tfstate, and Terragrunt will use a DynamoDB table named my-lock-table for locking. If you run the following, you can see how Terragrunt generates a backend.tf file to tell OpenTofu/Terraform to do this: $ find .terragrunt-cache -name backend.tf -exec cat {} \\;# Generated by Terragrunt. Sig: nIlQXj57tbuaRZEaterraform { backend \"s3\" { bucket = \"my-tofu-state\" dynamodb_table = \"my-lock-table\" encrypt = true key = \"tofu.tfstate\" region = \"us-east-1\" }} Right before running any OpenTofu/Terraform command that might store state, Terragrunt will ensure that the appropriate backend.tf file is present in the working directory where OpenTofu/Terraform will run, so that state is persisted appropriately when tofu or terraform are invoked. Note that while following the example above, you didn’t need to manually create that my-tofu-state S3 bucket, the my-lock-table DynamoDB table, or run tofu/terraform init to perform initialization. These are just a few of the things that Terragrunt does automatically when orchestrating OpenTofu/Terraform commands because it knows how OpenTofu/Terraform work, and it can take care of some busy work for you to make your life easier. Provider configurationSection titled “Provider configuration” The generate block is used to inject arbitrary files into the OpenTofu/Terraform module before running any OpenTofu/Terraform commands. In this example, Terragrunt is being configured to inject a provider.tf file into the module that configures the AWS provider to use the us-east-1 region. If you run the following, you can see the provider.tf file that Terragrunt generates: $ find .terragrunt-cache -name provider.tf -exec cat {} \\;# Generated by Terragrunt. Sig: nIlQXj57tbuaRZEaprovider \"aws\" { region = \"us-east-1\"} This is the most common use case for the generate block, but you can use it to inject any file you want into the OpenTofu/Terraform module. This can be useful for injecting any configurations that aren’t part of the generic module you want to reuse, or aren’t easy to generate dynamically (such as provider blocks, which can’t be dynamic in OpenTofu/Terraform). You can imagine that it may be convenient to have one set of modules, but dynamically inject different provider configurations based on the AWS region you’re deploying to, etc. You want to be mindful not to do too much with this configuration block, as it can make your OpenTofu/Terraform code harder to reproduce, understand and maintain. But it can be a powerful tool when used judiciously. Module configurationSection titled “Module configuration” The terraform block is used to indicate where to source the OpenTofu/Terraform module from (it’s called terraform for historical reasons, but it controls behavior pertinent to both OpenTofu and Terraform). In this example, all it is doing is controlling where Terragrunt should fetch the OpenTofu/Terraform module from. The configuration block can do a lot more, but the source attribute is the most common attribute you’ll set on the terraform block. You’ll notice that in the examples above, we were using find to locate the .tf files being generated and placed within the OpenTofu/Terraform module being downloaded here within the .terragrunt-cache directory. This is because Terragrunt aims to operate as an orchestrator, at a level of abstraction higher than OpenTofu/Terraform. Over the years supporting customers managing IaC at scale, the patterns that we’ve seen emerge for really successful organizations is to treat OpenTofu/Terraform modules as versioned, generic, well tested patterns of infrastructure, and to deploy them in as close to the exact same way as possible across all uses of them. Terragrunt supports this pattern by treating each unit of Terragrunt configuration (a directory with a terragrunt.hcl file in it) as a hermetic container of infrastructure that can be reasoned about in isolation, and then composed together to form a larger system of one or more stacks (each stack being a collection of units). To that end, the way that Terragrunt loads OpenTofu/Terraform configurations is to download them into a subdirectory of the .terragrunt-cache directory, and then to orchestrate OpenTofu/Terraform commands from that directory. This ensures that the OpenTofu/Terraform modules are treated as immutable, versioned, and hermetic, and that the OpenTofu/Terraform runs are reliably reproducible. Directory.terragrunt-cache/ DirectorytnIp4Am20T3Q8-6FuPqfof-kRGU DirectoryThyYwttwki6d6AS3aD5OwoyqIWA CHANGELOG.mdLICENSEREADME.mdUPGRADE-3.0.mdUPGRADE-4.0.mdbackend.tfexamplesmain.tfmodulesoutputs.tfprovider.tfterragrunt.hclvariables.tfversions.tfvpc-flow-logs.tf Any file that isn’t part of the OpenTofu/Terraform module (like the backend.tf and provider.tf files Terragrunt generated) get a special little Generated by Terragrunt comment at the top of their files by default to make sure it’s clear that Terragrunt generated them (and that they might not be there for other users of the same module). Inputs configurationSection titled “Inputs configuration” The inputs block is used to indicate what variable values should be passed to OpenTofu/Terraform when running tofu or terraform commands. In this example, Terragrunt is being configured to pass in a bunch of variables to the OpenTofu/Terraform module. These variables are used to configure the VPC module, such as the name of the VPC, the CIDR block, the availability zones, the subnets, and so on. Under the hood, what happens here is that Terragrunt sets relevant TF_VAR_ prefixed environment variables, which are automatically detected by OpenTofu/Terraform as values for variables defined in .tf files. Further ReadingSection titled “Further Reading” You can learn more about all the configuration blocks and attributes available in Terragrunt in the docs. Core PatternsSection titled “Core Patterns” This statement above is kind of a lie: * Here is a typical terragrunt.hcl file you might find in a Terragrunt project. The truth is, you’ll almost never see configuration like that outside of some tests or examples. The reason for this is that one of the main responsibilities Terragrunt has is to scale IaC, and the configuration above would result in quite a lot of code duplication across a project. In an AWS project for example, you will probably use the same (or very similar) provider configuration across all your units, and you’ll probably use the same backend configuration across all your units (with the only exception being the key for where in S3 your state should be stored). Aware of this pattern, Terragrunt is designed to leverage a hierarchy of reusable configurations so that your code can be DRY (Don’t Repeat Yourself). The include blockSection titled “The include block” In almost every terragrunt.hcl file you see, there will be a section that looks like this: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} This block configures Terragrunt to include configuration found in a parent folder named root.hcl into it. This is a way to share configuration across all units of infrastructure in your project. The root label being applied here is the idiomatic way to reference the root.hcl file that is common to all other configurations in the project. This is a convention, not a requirement, but it’s a good one to follow to make your code more readable and maintainable. Rewriting the example above to use the include block so that it looks more like the kind of thing you’d see in a real project would look like this: remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { bucket = \"my-tofu-state\" key = \"tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.16.0\"} inputs = { name = \"my-vpc\" cidr = \"10.0.0.0/16\" azs = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"] private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = false enable_vpn_gateway = false tags = { IaC = \"true\" Environment = \"dev\" }} By doing this, you can see that it’s become easier to introduce new units of infrastructure, as you only need to define the unique parts of the configuration for that unit in the new terragrunt.hcl file. The shared configuration is inherited from the root.hcl file. When you see include blocks in Terragrunt, remember that they result in the configuration being inlined into the configuration file that includes them. For the most part, you can simply replace the relevant include block with the configuration it is including to see the full configuration that Terragrunt will use. The exception to this is when you are using directives that explicitly leverage the fact that configurations are being included. Building out the stackSection titled “Building out the stack” For example, say you wanted to add another unit of infrastructure into the stack that you’re building out here. You could create a new directory named ec2, and add a terragrunt.hcl file to it like this: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"tfr:///terraform-aws-modules/ec2-instance/aws?version=5.7.1\"} dependency \"vpc\" { config_path = \"../vpc\"} inputs = { name = \"single-instance\" instance_type = \"t2.micro\" monitoring = true subnet_id = dependency.vpc.outputs.private_subnets[0] tags = { IaC = \"true\" Environment = \"dev\" }} Key CollisionsSection titled “Key Collisions” If you tried to run terragrunt plan in that new ec2 directory, you’d get an error that looked like this: $ terragrunt plan...* Failed to execute \"tofu init\" in ./.terragrunt-cache/I6Os-7-mjDhv4uQ5iCoGcOrDYhI/pfgqyj3TsBEWff7a1El6tYu6LEE ╷ │ Error: Backend configuration changed │ │ A change in the backend configuration has been detected, which may require │ migrating existing state. │ │ If you wish to attempt automatic migration of the state, use \"tofu init │ -migrate-state\". │ If you wish to store the current configuration with no changes to the │ state, use \"tofu init -reconfigure\". ╵ exit status 1 What’s happening here is that when Terragrunt invoked OpenTofu/Terraform, it generated exactly the same backend.tf file for the new unit of infrastructure as it did for the VPC unit. You can see that in the newly generated backend.tf file in the .terragrunt-cache directory under ec2: $ find .terragrunt-cache -name backend.tf -exec cat {} \\;# Generated by Terragrunt. Sig: nIlQXj57tbuaRZEaterraform { backend \"s3\" { bucket = \"my-tofu-state\" dynamodb_table = \"my-lock-table\" encrypt = true key = \"tofu.tfstate\" region = \"us-east-1\" }} Dynamic keysSection titled “Dynamic keys” What most folks would really prefer here is to have the state for the ec2 unit stored in a different, but predictable, location relative to the vpc unit. The pattern that we’ve found to be most effective is to store state so that the location in the remote backend, like S3 mirrors the location of the unit on the filesystem. So this filesystem layout: Directoryroot.hcl Directoryvpc terragrunt.hclDirectoryec2 terragrunt.hcl Would result in this state layout in S3: Directorys3://my-tofu-state Directoryvpc tofu.tfstateDirectoryec2 tofu.tfstate To achieve this, we can take advantage of the path_relative_to_include() Terragrunt HCL function to generate a key dynamically based on the position of the unit relative to the root.hcl file within the filesystem. remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" # <-- region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} What this does is set the key attribute of the generated backend.tf file to be the relative path from the root.hcl file to the terragrunt.hcl file that is being processed. Migrating stateSection titled “Migrating state” You have to be careful when adjusting the key attribute of units (including when moving units around in the filesystem, if you use something like path_relative_to_include to drive the value of the key attribute) because it can result in state being stored in a different location in the remote backend. There’s native tooling in OpenTofu/Terraform to support these procedures, but you want to be confident you know what you’re doing when you run them. By default, Terragrunt will provision a remote backend that uses versioning, so you can always roll back to a previous state if you need to. # First, we'll migrate state to the new location$ terragrunt init -migrate-state# Then, let's take a look at the generated backend.tf file$ find .terragrunt-cache -name backend.tf -exec cat {} \\;# Generated by Terragrunt. Sig: nIlQXj57tbuaRZEaterraform { backend \"s3\" { bucket = \"my-tofu-state\" dynamodb_table = \"my-lock-table\" encrypt = true key = \"vpc/tofu.tfstate\" region = \"us-east-1\" }} Now, we can run the plan in the ec2 directory without any issues: # Within the ec2 directory$ terragrunt plan...$ find .terragrunt-cache -name backend.tf -exec cat {} \\;# Generated by Terragrunt. Sig: nIlQXj57tbuaRZEaterraform { backend \"s3\" { bucket = \"my-tofu-state\" dynamodb_table = \"my-lock-table\" encrypt = true key = \"ec2/tofu.tfstate\" region = \"us-east-1\" }} Following this pattern, you can create as many units of infrastructure in your project as you like without worrying about collisions in remote state keys. Note that while this is the idiomatic approach for defining the key attribute for your backend configuration, it is not a requirement. You can set the key attribute to any value you like, and you can use any Terragrunt HCL function to generate that value dynamically such that you avoid collisions in your remote state. Another completely valid approach, for example, is to utilize get_repo_root, which returns a path relative to the root of the git repository. This, of course, has the drawback that it will not work if you are not using git. Just make sure to test your configuration carefully, and document your approach so that others can understand what you’re doing. The dependency blockSection titled “The dependency block” You might have noticed that the ec2 unit of infrastructure has a dependency block in its configuration: # ...dependency \"vpc\" { config_path = \"../vpc\"} inputs = { name = \"single-instance\" instance_type = \"t2.micro\" monitoring = true subnet_id = dependency.vpc.outputs.private_subnets[0] tags = { IaC = \"true\" Environment = \"dev\" }} This block tells Terragrunt that the ec2 unit depends on the output of the vpc unit. You can also see that it references that dependency within the inputs block as dependency.vpc.outputs.private_subnets[0]. When Terragrunt is performing a run for a dependency, it will first run terragrunt output in the dependency, then expose the values from that output as values that can be used in the dependent unit. This is a very useful mechanism, as it keeps each unit isolated, while allowing for message passing between units when they need to interact. The Directed Acyclic Graph (DAG)Section titled “The Directed Acyclic Graph (DAG)” Dependencies also give Terragrunt a way to reason about the order in which units of infrastructure should be run. It uses what’s called a Directed Acyclic Graph (DAG) to determine the order in which units should be run, and then runs them in that order. For example, let’s go ahead and destroy all the infrastructure that we’ve created so far: # From the root directory$ terragrunt run --all destroy16:32:08.944 INFO The stack at . will be processed in the following order for command destroy:Group 1- Module ./ec2 Group 2- Module ./vpc WARNING: Are you sure you want to run `terragrunt destroy` in each folder of the stack described above? There is no undo! (y/n) First, notice that we’re using a special run --all command for Terragrunt. This command tells Terragrunt that we’re operating on a stack of units, and that we want to run a given OpenTofu/Terraform command on all of them. Second, notice that the ec2 unit is being run before the vpc unit. Terragrunt knows that the ec2 unit depends on the vpc unit, so it plans to run the ec2 unit first, followed by the vpc unit. This is a simple example, but as you build out more complex stacks of infrastructure, you’ll find that Terragrunt’s dependency resolution is a powerful tool for getting infrastructure provisioned correctly. Go ahead and answer y to the prompt to allow destruction to proceed, and notice that the logging has also changed slightly: 16:33:28.820 STDOUT [ec2] tofu: aws_instance.this[0]: Destruction complete after 1m11s16:33:28.936 STDOUT [ec2] tofu:16:33:28.936 STDOUT [ec2] tofu: Destroy complete! Resources: 1 destroyed.16:33:28.936 STDOUT [ec2] tofu:16:33:30.713 STDOUT [vpc] tofu: aws_vpc.this[0]: Refreshing state... [id=vpc-063d11b72a2c9f8b3]16:33:31.510 STDOUT [vpc] tofu: aws_default_security_group.this[0]: Refreshing state... [id=sg-060d402b95a2cd935]16:33:31.511 STDOUT [vpc] tofu: aws_default_route_table.default[0]: Refreshing state... [id=rtb-05adb3ee7f48640f0] Terragrunt will give you the contextual information you need to understand what’s happening in your stack as it’s being run. That [ec2] and [vpc] prefix is a great way to quickly disambiguate what’s happening in one unit of infrastructure from another. Mock outputsSection titled “Mock outputs” Now that the stack has been destroyed, take a look at the error you get when you try to run terragrunt run --all plan again: $ terragrunt run --all plan...16:50:22.153 STDOUT [vpc] tofu: Note: You didn't use the -out option to save this plan, so OpenTofu can't16:50:22.153 STDOUT [vpc] tofu: guarantee to take exactly these actions if you run \"tofu apply\" now.16:50:22.854 ERROR [ec2] Module ./ec2 has finished with an error16:50:22.855 ERROR error occurred: * ./vpc/terragrunt.hcl is a dependency of ./ec2/terragrunt.hcl but detected no outputs. Either the target module has not been applied yet, or the module has no outputs. If this is expected, set the skip_outputs flag to true on the dependency block. 16:50:22.855 ERROR Unable to determine underlying exit code, so Terragrunt will exit with error code 1 The error emitted here tells us that the vpc unit doesn’t have any outputs available for the ec2 unit to consume as a dependency. The pattern most commonly used to address this is to simply mock the unavailable output during plans. Adjust the vpc dependency in the ec2 unit like so: # ...dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { private_subnets = [\"mock-subnet\"] } mock_outputs_allowed_terraform_commands = [\"plan\"]}# ... Then run the plan again: $ terragrunt run --all plan...16:53:04.037 STDOUT [ec2] tofu: + source_dest_check = true16:53:04.037 STDOUT [ec2] tofu: + spot_instance_request_id = (known after apply)16:53:04.037 STDOUT [ec2] tofu: + subnet_id = \"mock-subnet\"16:53:04.037 STDOUT [ec2] tofu: + tags = {16:53:04.038 STDOUT [ec2] tofu: + \"Environment\" = \"dev\"... As you can see, the plan for the EC2 instance now includes a subnet_id value of mock-subnet, which is the value we provided in the mock_outputs block. Terragrunt only uses these mock values when the output is unavailable, so a terragrunt run --all apply would succeed, but it’s best practice to explicitly tell Terragrunt that it should only use these mock values during a plan (or any other command where you are okay with the output being mocked). Also note that when you run terragrunt run --all apply: $ terragrunt run --all apply16:57:32.297 INFO The stack at . will be processed in the following order for command apply:Group 1- Module ./vpc Group 2- Module ./ec2 Are you sure you want to run 'terragrunt apply' in each folder of the stack described above? (y/n) That the order of units has flipped. Terragrunt knows that during applies, dependencies actually need to be run before the dependent unit, so it’s flipped the order of the units in the stack, relative to destroys. You can answer y to allow the apply to proceed and see that the EC2 instance is placed into a real subnet (not the mock value) as expected. Configuration hierarchySection titled “Configuration hierarchy” Terragrunt also provides tooling for constructing a hierarchy of configurations that can be used to manage multiple environments, regions, or accounts. Say, for example, you wanted to provision the same resources you’ve provisioned so far, but in multiple AWS regions, with a filesystem layout like this: root.hclDirectoryus-east-1 Directoryvpc terragrunt.hclDirectoryec2 terragrunt.hclDirectoryus-west-2 Directoryvpc terragrunt.hclDirectoryec2 terragrunt.hcl With Terragrunt, that’s pretty easy to achieve. You would first create a us-east-1 directory like so: mkdir us-east-1 Then move the contents you have in the vpc and ec2 directories into the us-east-1 directory: mv vpc/ ec2/ us-east-1/ Remember that now you’ll need to migrate state, as changing the location of the units in the filesystem will result in a change in the remote state path: (In production scenarios, you likely want to carefully manage state by migrating over one unit at a time, but for the sake of this tutorial, you can learn about this shortcut) terragrunt run --all -- init -migrate-state We want the AWS region used by our units to be determined dynamically, so we can add a configuration file to the us-east-1 directory that looks like this: locals { region = \"us-east-1\"} Then update the root.hcl like so: locals { region_hcl = find_in_parent_folders(\"region.hcl\") region = read_terragrunt_config(local.region_hcl).locals.region} # Configure the remote backendremote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} # Configure the AWS providergenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"${local.region}\"}EOF} Now, when the configurations in the us-east-1 directory include the root.hcl, they’ll automatically parse the first region.hcl file they find while traversing up the filesystem, and use the region value defined in that file to set the AWS region for the provider. NOTE In the generate block, we’re using \"${local.region}\", rather than local.region. This is because the generate block is going to generate a file directly into the OpenTofu/Terraform module. We need to ensure that when the value is interpolated, it’s done so in a way that OpenTofu/Terraform can understand, so we wrap it in quotes. ALSO NOTE The remote_state block is still storing all state in the us-east-1 region by design. We don’t have to do this, and you could easily set it to store state in multiple regions. For the sake of simplicity, and demonstration, we’re keeping it in one region. Exposed includesSection titled “Exposed includes” Before moving on, take note of one thing, the azs attribute in the vpc unit of the us-east-1 stack is hardcoded to [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]. This would cause issues if we were to try to deploy the vpc unit in the us-west-2 stack, as those availability zones don’t exist in the us-west-2 region. What we need to do is make the azs attribute dynamic and use the resolved region to determine the correct availability zones. To do this, we can expose the attributes on the included root configuration by setting the expose attribute to true: include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} locals { region = include.root.locals.region} # Configure the module# The URL used here is a shorthand for# \"tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=5.16.0\".# Note the extra `/` after the protocol is required for the shorthand# notation.terraform { source = \"tfr:///terraform-aws-modules/vpc/aws?version=5.16.0\"} # Configure the inputs for the moduleinputs = { name = \"my-vpc\" cidr = \"10.0.0.0/16\" azs = [\"${local.region}a\", \"${local.region}b\", \"${local.region}c\"] # <-- private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = false enable_vpn_gateway = false tags = { IaC = \"true\" Environment = \"dev\" }} This makes it so that the values of the azs attribute are determined dynamically based on the region that the unit is being deployed to. Now that you’ve set up the us-east-1 directory, you can repeat the process for the us-west-2 directory: cp -R us-east-1/ us-west-2/ Then update the region.hcl file in the us-west-2 directory to set the region to us-west-2: locals { region = \"us-west-2\"} Tightening the blast radiusSection titled “Tightening the blast radius” Run the terragrunt run --all apply command after changing your current working directory to the us-west-2 directory: cd us-west-2terragrunt run --all apply You should see the VPC and EC2 instances being provisioned in the us-west-2 region. This showcases three superpowers you gain when you leverage Terragrunt: Automatic DAG Resolution: No configuration file had to be updated or modified to ensure that the ec2 unit was run after the vpc unit when provisioning the us-west-2 stack. Terragrunt automatically resolved the dependency graph and ran the units in the correct order. Dynamic Configuration: The code you copied from the us-east-1 directory to the us-west-2 directory didn’t need to be modified at all to provision resources in a different region (with the exception of naming the region in the region.hcl file). Terragrunt was able to dynamically resolve the correct configuration based on context, and apply it to the OpenTofu/Terraform modules as generic patterns of infrastructure. Reduced Blast Radius: By applying Terragrunt within the us-west-2 directory, you were able to confidently target only the resources in that region, without affecting the resources in the us-east-1 region. This is a powerful tool for safely managing multiple environments, regions, or accounts with a single codebase. Your current working directory when using Terragrunt is your blast radius, and Terragrunt makes it easy to manage that blast radius effectively. CleanupSection titled “Cleanup” If you still have all the resources that were provisioned as part of this tutorial active, this is a reminder that you might want to clean them up. To destroy all the resources you’ve provisioned thus far, run the following: # From the root directory$ terragrunt run --all destroy In real-world scenarios, it’s generally advised that you plan your destroys first before cleaning them up: # From the root directory$ terragrunt run --all -- plan -destroy You won’t need to run any more Terragrunt commands for the rest of this guide. Recommended Repository PatternsSection titled “Recommended Repository Patterns” Outside of the patterns used for setting up Terragrunt configurations within a project, there are are also some patterns that we recommend for managing one or more repositories used to manage infrastructure. At Gruntwork, we refer to this as your “Infrastructure Estate”. These recommendations are merely guidelines, and you should adapt them to suit your team’s needs and constraints. infrastructure-liveSection titled “infrastructure-live” The core of the infrastructure you manage with a Terragrunt is typically stored in a repository named infrastructure-live (or some variant of it). This repository is where you store the Terragrunt configurations used for infrastructure that is intended to be “live” (i.e. provisioned and active). Most successful teams stick to Trunk Based Development, perform plans on any change being proposed via a pull request / merge request, and only apply changes to live infrastructure after a successful plan and review. This repository is generally concerned with the configuration of reliably reproducible, immutable and versioned infrastructure. You generally don’t author OpenTofu/Terraform code directly into it, and you apply appropriate branch protection rules to ensure that changes are merged only if they get the appropriate sign-off from responsible parties. What’s on the default branch for this repository is generally considered the source of truth for the infrastructure you have provisioned. That default branch is generally the only version that matters when considering the state of your infrastructure. You can see an example of this in the terragrunt-infrastructure-live-stacks-example repository maintained by Gruntwork. infrastructure-modulesSection titled “infrastructure-modules” The patterns for your infrastructure you want to reliably reproduce. This repository is where you store the OpenTofu/Terraform modules that you use in your infrastructure-live repository. This repository is generally concerned with maintaining versioned, well tested and vetted patterns of infrastructure, ready to be consumed by the infrastructure-live repository. You typically integrate this repository with tools like Terratest to ensure that every change to a module is well tested and reliable. Semantic Versioning is widely used to manage communicating the impact of updates to this repository, and you typically pin the version of a consumed module in the infrastructure-live repository to a specific tag. You can see an example of this in the terragrunt-infrastructure-catalog-example repository maintained by Gruntwork. Atomic DeploymentsSection titled “Atomic Deployments” Following the repository patterns outlined above, you typically see Terragrunt repositories that have configurations which look like this: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"github.com:foo/infrastructure-modules.git//app?ref=v0.0.1\"} inputs = { instance_count = 3 instance_type = \"t2.micro\"} Where app is an opinionated module in the infrastructure-modules repository, maintained by the team managing infrastructure for the foo organization. The code in that module might be hand-rolled, it may wrap a community maintained module, or it might wrap a module like one found in the Gruntwork IaC Library. Regardless, the module is something that the team managing infrastructure for an organization has vetted for deployment. When deploying a change to live infrastructure, the team would typically make a change like the following: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"github.com:foo/infrastructure-modules.git//app?ref=v0.0.2\" # <--} inputs = { instance_count = 1 instance_type = \"t3.micro\"} Given that all the changes here are part of one atomic deployment, it’s fairly easy to determine the impact of the change, and to roll back if necessary. After that, they would propagate the change up however many intermediary environments they have, and finally to production. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"github.com:foo/infrastructure-modules.git//app?ref=v0.0.2\" # <--} inputs = { instance_count = 3 instance_type = \"t3.large\"} Note that the two terragrunt.hcl files here have different inputs values, as those values are specific to the environment they are being deployed to. The end result of this process is that infrastructure changes are atomic and reproduceable, and that the infrastructure being deployed is versioned and immutable. If at any point during this process a change is found to be problematic, the team can simply roll back to the previous version of the module for a single unit in a given environment. That’s the power of reducing your blast radius with Terragrunt! Keep It Simple, SillySection titled “Keep It Simple, Silly” One last pattern to internalize is the general tendency to prefer simple configurations over complex ones when possible. Terragrunt provides a lot of power and flexibility, but it’s generally best to use that power to make your configurations more readable and maintainable. Keep in mind that you’re writing code that will be read by other humans, and that you might not be around to explain any complexity you introduce. As an example, consider one potential solution to a step outlined in the Exposed includes section, the requirement to update the region local in the region.hcl file: locals { region = \"us-west-2\"} You might think to yourself “Hey, I know a lot about Terragrunt functionality, I can make this more dynamic, such that I don’t even need to create a region.hcl file!” and come up with a solution like this: locals { region = split(\"/\", path_relative_to_include())[0]} # Configure the remote backendremote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} # Configure the AWS providergenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"${local.region}\"}EOF} This would result in the region local being set to the name of the first directory in the path to the terragrunt.hcl file that is being run during an include (us-east-1 and us-west-1 in each respective stack). This would allow you to remove the region.hcl file from both the us-east-1 and us-west-2 directories. Consider, though, that this might make the configuration harder to understand for someone who is not as familiar with Terragrunt as you are. You’ve now tightly coupled the name of the directory to the region that the infrastructure is being deployed to, and you’ve made it harder for someone to understand if they run into issues. Say a user tries to deploy infrastructure while on a Windows machine, where the path separator is \\ instead of /. Using this configuration would result in the region local being set to something like us-east-1\\vpc, which is confusing and not what you want. In this case, you might prefer to have kept the region.hcl file, as it makes the configuration more explicit and easier to understand. On the other hand, maybe you work with a team that’s very comfortable with Terragrunt, exclusively using Unix-based systems, and you’ve all agreed and documented this as a good pattern to follow. In that case, this might be a perfectly acceptable solution. You have to exercise your best judgment when deciding how much complexity to introduce into your Terragrunt configurations. As a general rule, the best patterns to follow are the ones that are easiest to reproduce, understand, and maintain. Next stepsSection titled “Next steps” Now that you’ve learned the basics of Terragrunt, here is some further reading to learn more: Features: Learn about the core features Terragrunt supports. Documentation: Check out the detailed Terragrunt reference documentation. Fundamentals of DevOps and Software Delivery: Learn the fundamentals of DevOps and Software Delivery from one of the founders of Gruntwork! Terraform: Up & Running: Terragrunt is a direct implementation of many of the ideas from this book. Edit page Previous Quick Start Next Install",
    "section": "getting-started",
    "lastUpdated": "2025-10-23T16:40:56.912Z"
  },
  {
    "title": "Install",
    "url": "https://terragrunt.gruntwork.io/docs/getting-started/install/",
    "content": "Install Download from releases pageSection titled “Download from releases page” Go to the Releases Page. Download the binary for your operating system: e.g., if you’re on a Mac, download terragrunt_darwin_amd64; if you’re on Windows, download terragrunt_windows_amd64.exe, etc. Optionally, follow the instructions below on verifying the checksum. Rename the downloaded file to terragrunt. Add execute permissions to the binary: e.g., On Linux and Mac: chmod u+x terragrunt. Put the binary somewhere on your PATH: e.g., On Linux and Mac: mv terragrunt /usr/local/bin/terragrunt. Verifying the checksumSection titled “Verifying the checksum” When you download the binary from the releases page, you can also use the checksum file to verify the integrity of the binary. This can be useful for ensuring that you have an intact binary and that it has not been tampered with. To verify the integrity of the file, do the following: Have the binary downloaded, and accessible. Generate the SHA256 checksum of the binary. Download the SHA256SUMS file from the releases page. Find the expected checksum for the binary you downloaded. If the checksums match, the binary is intact and has not been tampered with. Convenience ScriptsSection titled “Convenience Scripts” Linux (x86) macOS (ARM) Windows Linux (ARM) macOS (x86) set -euo pipefail OS=\"linux\"ARCH=\"amd64\"VERSION=\"v0.91.3\"BINARY_NAME=\"terragrunt_${OS}_${ARCH}\" # Download the binarycurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/$BINARY_NAME\" -o \"$BINARY_NAME\" # Generate the checksumCHECKSUM=\"$(sha256sum \"$BINARY_NAME\" | awk '{print $1}')\" # Download the checksum filecurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/SHA256SUMS\" -o SHA256SUMS # Grab the expected checksumEXPECTED_CHECKSUM=\"$(grep \"$BINARY_NAME\" <SHA256SUMS | awk '{print $1}')\" # Compare the checksumsif [ \"$CHECKSUM\" == \"$EXPECTED_CHECKSUM\" ]; then echo \"Checksums match!\"else echo \"Checksums do not match!\"fi set -euo pipefail OS=\"darwin\"ARCH=\"arm64\"VERSION=\"v0.91.3\"BINARY_NAME=\"terragrunt_${OS}_${ARCH}\" # Download the binarycurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/$BINARY_NAME\" -o \"$BINARY_NAME\" # Generate the checksumCHECKSUM=\"$(shasum -a 256 \"$BINARY_NAME\" | awk '{print $1}')\" # Download the checksum filecurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/SHA256SUMS\" -o SHA256SUMS # Grab the expected checksumEXPECTED_CHECKSUM=\"$(grep \"$BINARY_NAME\" <SHA256SUMS | awk '{print $1}')\" # Compare the checksumsif [ \"$CHECKSUM\" == \"$EXPECTED_CHECKSUM\" ]; then echo \"Checksums match!\"else echo \"Checksums do not match!\"fi $os = \"windows\"$arch = \"amd64\"$version = \"v0.91.3\"$binaryName = \"terragrunt_${os}_${arch}.exe\"try { $ProgressPreference = 'SilentlyContinue' # Download binary and checksum $baseUrl = \"https://github.com/gruntwork-io/terragrunt/releases/download/$version\" Write-Host \"Downloading Terragrunt $version...\" Invoke-WebRequest -Uri \"$baseUrl/$binaryName\" -OutFile $binaryName -UseBasicParsing Invoke-WebRequest -Uri \"$baseUrl/SHA256SUMS\" -OutFile \"SHA256SUMS\" -UseBasicParsing $actualChecksum = (Get-FileHash -Algorithm SHA256 $binaryName).Hash.ToLower() $expectedChecksum = (Get-Content \"SHA256SUMS\" | Select-String -Pattern $binaryName).Line.Split()[0].ToLower() if ($actualChecksum -ne $expectedChecksum) { Write-Error \"Checksum verification failed\" exit 1 } Write-Host \"Terragrunt $version has been downloaded and verified successfully\"}catch { Write-Error \"Failed to download: $_\" exit 1}finally { $ProgressPreference = 'Continue'} set -euo pipefail OS=\"linux\"ARCH=\"arm64\"VERSION=\"v0.91.3\"BINARY_NAME=\"terragrunt_${OS}_${ARCH}\" # Download the binarycurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/$BINARY_NAME\" -o \"$BINARY_NAME\" # Generate the checksumCHECKSUM=\"$(sha256sum \"$BINARY_NAME\" | awk '{print $1}')\" # Download the checksum filecurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/SHA256SUMS\" -o SHA256SUMS # Grab the expected checksumEXPECTED_CHECKSUM=\"$(grep \"$BINARY_NAME\" <SHA256SUMS | awk '{print $1}')\" # Compare the checksumsif [ \"$CHECKSUM\" == \"$EXPECTED_CHECKSUM\" ]; then echo \"Checksums match!\"else echo \"Checksums do not match!\"fi set -euo pipefail OS=\"darwin\"ARCH=\"x86\"VERSION=\"v0.91.3\"BINARY_NAME=\"terragrunt_${OS}_${ARCH}\" # Download the binarycurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/$BINARY_NAME\" -o \"$BINARY_NAME\" # Generate the checksumCHECKSUM=\"$(shasum -a 256 \"$BINARY_NAME\" | awk '{print $1}')\" # Download the checksum filecurl -sL \"https://github.com/gruntwork-io/terragrunt/releases/download/$VERSION/SHA256SUMS\" -o SHA256SUMS # Grab the expected checksumEXPECTED_CHECKSUM=\"$(grep \"$BINARY_NAME\" <SHA256SUMS | awk '{print $1}')\" # Compare the checksumsif [ \"$CHECKSUM\" == \"$EXPECTED_CHECKSUM\" ]; then echo \"Checksums match!\"else echo \"Checksums do not match!\"fi Install via a package managerSection titled “Install via a package manager” Note that all the different package managers are third party. The third party Terragrunt packages may not be updated with the latest version, but are often close. Please check your version against the latest available on the Releases Page. If you want the latest version, the recommended installation option is to download from the releases page. Windows: You can install Terragrunt on Windows using Chocolatey choco install terragrunt macOS: You can install Terragrunt on macOS using Homebrew: brew install terragrunt Linux (Homebrew): Most Linux users can use Homebrew: brew install terragrunt Linux (Pacman): Arch Linux users can use pacman: pacman -S terragrunt Linux (Gentoo): Gentoo users can use emerge: emerge -a app-admin/terragrunt-bin FreeBSD: You can install Terragrunt on FreeBSD using Pkg: pkg install terragrunt Install via tool managerSection titled “Install via tool manager” A best practice when using Terragrunt is to pin the version you are using to ensure that you, your colleagues and your CI/CD pipelines are all using the same version. This also allows you to easily upgrade to new versions and rollback to previous versions if needed. You can use a tool manager to install and manage Terragrunt versions. mise: You can install Terragrunt using mise mise install terragrunt v0.91.3 asdf: You can install Terragrunt using asdf asdf plugin add terragruntasdf install terragrunt v0.91.3 Both of these tools allow you to pin the version of Terragrunt you are using in a .tool-versions (and .mise.toml for mise) file in your project directory. Colleagues and CI/CD pipelines can then install the associated tool manager, and run using the pinned version. Note that the tools Terragrunt integrates with, such as OpenTofu and Terraform, can also be managed by these tool managers, so you can also pin the versions of those tools in the same file. Also note that the asdf plugin that asdf relies on is maintained by a third party: https://github.com/ohmer/asdf-terragrunt Gruntwork makes no guarantees about the safety or reliability of third-party plugins. The asdf plugin relied upon by mise is maintained by Gruntwork, as requested by the community: https://github.com/gruntwork-io/asdf-terragrunt Building from sourceSection titled “Building from source” If you’d like to build from source, you can use go to build Terragrunt yourself, and install it: git clone https://github.com/gruntwork-io/terragrunt.gitcd terragrunt# Feel free to checkout a particular tag, etc if you want here.go install Enable tab completionSection titled “Enable tab completion” If you use either Bash or Zsh, you can enable tab completion for Terragrunt commands. To enable autocomplete, first ensure that a config file exists for your chosen shell. For Bash shell. touch ~/.bashrc For Zsh shell. touch ~/.zshrc Then install the autocomplete package. terragrunt --install-autocomplete Once the autocomplete support is installed, you will need to restart your shell. Terragrunt GitHub ActionSection titled “Terragrunt GitHub Action” Terragrunt is also available as a GitHub Action. Instructions on how to use it can be found at https://github.com/gruntwork-io/terragrunt-action. Gruntwork PipelinesSection titled “Gruntwork Pipelines” Gruntwork offers a commercial CI/CD solution for Terragrunt called Pipelines. Pipelines is a fully managed CI/CD service that is designed to work seamlessly with Terragrunt. It provides an out of the box solution for running Terragrunt in CI/CD without the need to setup and maintain your own CI/CD infrastructure. Edit page Previous Overview Next Terminology",
    "section": "getting-started",
    "lastUpdated": "2025-10-23T16:40:56.961Z"
  },
  {
    "title": "Terminology",
    "url": "https://terragrunt.gruntwork.io/docs/getting-started/terminology/",
    "content": "Terminology PreambleSection titled “Preamble” Infrastructure as Code (IaC) tooling necessarily requires a lot of terminology to describe various concepts and features due to the breadth of the domain. Whenever possible, Terragrunt terminology attempts to align with wider industry standards, but there are always exceptions. There are going to be times when certain terms are used in different tools, but have special meaning in Terragrunt, and there are times when the same term might have different meaning in different contexts. This document aims to provide a quick reference for the most important and commonly used terms in Terragrunt and generally in Gruntwork products. Whenever terminology used in Terragrunt deviates from this document, it should either be explained or adjusted to align with this document. TermsSection titled “Terms” TerragruntSection titled “Terragrunt” Terragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale. It differs from many other IaC tools in that it is designed to be an orchestrator for OpenTofu/Terraform execution, rather than primarily provisioning infrastructure itself. Terragrunt users write OpenTofu/Terraform code to define high-level patterns of infrastructure that they want to create, then use Terragrunt to dynamically apply those generic patterns in particular ways. Because of this separation of concerns, most of what Terragrunt does is designed to extend the capabilities of OpenTofu/Terraform, rather than replace them. Most of the features of Terragrunt are designed to make it easier to manage large infrastructure estates, or to provide additional capabilities that are inconvenient or impossible to achieve with OpenTofu/Terraform alone. OpenTofuSection titled “OpenTofu” OpenTofu is an open-source Infrastructure as Code tool spawned as a fork of Terraform after the license change from the Mozilla Public License (MPL) to the Business Source License (BSL). OpenTofu was created as a drop-in replacement for Terraform (as it was forked from the same MPL source code), and is designed to be fully compatible with Terraform configurations and modules. You may notice that Terragrunt documentation uses the phrase “OpenTofu/Terraform” to refer to the IaC tooling that Terragrunt orchestrates. This is because Terragrunt is generally agnostic to the specific IaC tooling that is being used to drive infrastructure updates. When relevant, Terragrunt documentation endeavors to explicitly indicate that functionality is specific to one tool or the other. From the perspective of Terragrunt, the two are usually interchangeable, though Terragrunt will default to using OpenTofu if both are available. Note that some documentation refers to Terraform alone in some instances as a consequence of the historical context in which Terragrunt was created, as it predates the creation of OpenTofu. Conversely, some documentation may refer to OpenTofu alone simply because of the fact that OpenTofu is the default IaC tool that Terragrunt uses. UnitSection titled “Unit” A unit is a single instance of infrastructure managed by Terragrunt. It has its own state, and can be detected by the presence of a terragrunt.hcl file in a directory. Units typically represent a minimal useful piece of infrastructure that should be independently managed. e.g. A unit might represent a single VPC, a single database, or a single server. While not a requirement, a general tendency experienced when working with Terragrunt is that units tend to decrease in size. This is because Terragrunt makes it easy to segment pieces of infrastructure into their own state, and to have them interact with each other through the use of dependency blocks. Smaller units are quicker to update, easier to reason about and safer to work with. A common pattern used in the repository structure for Terragrunt projects is to have a single terragrunt.hcl file located at the root of the repository, and multiple subdirectories each containing their own terragrunt.hcl file. This is typically done to promote code-reuse, as it allows for any configuration common to all units to be defined in the root terragrunt.hcl file, and for unit-specific configuration to be defined in child directories. In this pattern, the root terragrunt.hcl file is not considered a unit, while all the child directories containing terragrunt.hcl files are. Note that units don’t technically need to call their configuration files terragrunt.hcl (that’s configurable via the —config), and users don’t technically need to use a root terragrunt.hcl file or to name it that. This is the most common pattern followed by the community, however, and deviation from this pattern should be justified in the context of the project. It can help others with Terragrunt experience understand the project more easily if industry standard patterns are followed. StackSection titled “Stack” A stack is a collection of units managed by Terragrunt. There is (as of writing) work underway to provide a top level artifact for interacting with stacks via a terragrunt.stack.hcl file, but as of now, stacks are generally defined by a directory with a tree of units. Units within a stack can be dependent on each other, and can be updated in a specific order to ensure that dependencies are resolved in the correct order. Stacks typically represent a collection of units that need to be managed in concert. e.g. A stack might represent a collection of units that together form a single application environment, a business unit, or a region. The design of terragrunt.stack.hcl files is to ensure that they function entirely as a convenient shorthand for an equivalent directory structure of units. This is to ensure that users are able to easily transition between the two paradigms, and are able to decide for themselves which approach to structuring infrastructure is most appropriate for their use case. ModuleSection titled “Module” A module is an OpenTofu/Terraform construct defined using a collection of OpenTofu/Terraform configurations ending in .tf (or .tofu in the case of OpenTofu) that represent a general pattern of infrastructure that can be instantiated multiple times. Modules typically represent a generic pattern of infrastructure that can be instantiated multiple times, with different configurations exposed as variables. e.g. A module might represent a generic pattern for a VPC, a database, or a server. Note that this differs from a unit, which represents a single instance of a provisioned VPC, database, or server. Modules can be located either in the local filesystem, in a remote repository, or in any of these supported locations. To integrate a module into a Terragrunt unit, reference the module using the source attribute of the terraform block. Terragrunt users typically spend a good deal of time authoring modules, as they are the primary way of defining the infrastructure patterns that Terragrunt is going to be orchestrating. Using tooling like Terratest can help to ensure that modules are well-tested and reliable. A common pattern in Terragrunt usage is to only ever provision versioned, immutable modules. This is because Terragrunt is designed to be able to manage infrastructure over long periods of time, and it is important to be able to reproduce the state of infrastructure at any point in time. ResourceSection titled “Resource” A resource is a low level building block of infrastructure that is defined in OpenTofu/Terraform configurations. Resources are typically defined in modules, but don’t have to be. Terragrunt can provision resources defined with .tf files that are not part of a module, located adjacent to the terragrunt.hcl file of a unit. e.g. A resource might represent a single S3 bucket, or a single load balancer. Resources generally correspond to the smallest piece of infrastructure that can be managed by OpenTofu/Terraform, and each resource has a specific address in state. StateSection titled “State” Terragrunt stores the current state of infrastructure in one or more OpenTofu/Terraform state files. State is an extremely important concept in the context of OpenTofu/Terraform, and it’s helpful to read the relevant documentation there to understand what Terragrunt does to it. Terragrunt has myriad capabilities that are designed to make working with state easier, including tooling to bootstrap state backend resources on demand, managing unit interaction with external state, and segmenting state. The most common way in which state is segmented in Terragrunt projects is to take advantage of filesystem directory structures. Most Terragrunt projects are configured to store state in remote backends like S3 with keys that correspond to the relative path to the unit directory within a project, relative to the root terragrunt.hcl file. Directed Acyclic Graph (DAG)Section titled “Directed Acyclic Graph (DAG)” The way in which units are resolved within a stack is via a Directed Acyclic Graph (DAG). This graph is also used to determine the order in which resources are resolved within a unit. Dependencies in a DAG determine the order in which resources are created, updated, or destroyed. For creations and updates, resources are updated such that dependencies are always resolved before their dependents. For destructions, resources are destroyed such that dependents are always destroyed before their dependencies. This is still true even when working with multiple units in a stack. Terragrunt will resolve the dependencies of all units in a stack (resolving the DAG within each unit first), and then apply the changes to all units in the stack in the correct order. Note that DAGs are Acyclic, meaning that there are no loops in the graph. This is because loops would create circular dependencies, which would make it impossible to determine the correct order to resolve resources. Don’t Repeat Yourself (DRY)Section titled “Don’t Repeat Yourself (DRY)” The Don’t Repeat Yourself (DRY) principle is a software development principle that states that duplication in code should be avoided. Early on, a lot of Terragrunt functionality was designed to make it easier to follow the DRY principle. This was because Terraform users at the time found that they were often repeating the same, or very similar code across multiple configurations. Examples of this included the limitation that remote state and provider configurations needed to be repeated in every root module, and that there were limitations in the dynamicity of these configurations. Over time, Terragrunt has evolved to provide more features that make it easier to manage infrastructure at scale, and the focus has shifted to offering more tooling for orchestrating infrastructure, rather than simply making it easier to avoid repeating yourself. Many of the features still serve to make it easier to follow the DRY principle, but this is no longer the primary focus of the tool. Much of the marketing around Terragrunt still emphasizes the DRY principle, as it is a useful way to explain the value of Terragrunt to new users. However, you might miss the forest for the trees if you focus too much on the DRY principle when evaluating Terragrunt. Terragrunt is a powerful tool that can be used to manage infrastructure at scale, and it is worth evaluating it based on its capabilities to do so. Blast RadiusSection titled “Blast Radius” Blast Radius is a term used in software development to describe the potential impact of a change, derived from the term used to describe the potential impact of an explosion. In the context of infrastructure management, blast radius is used to describe the potential impact (negative or positive) of a change to infrastructure. The larger the blast radius, the more potential impact a change has. Terragrunt was born out of a need to reduce the blast radii of infrastructure changes. By making it easier to segment state in infrastructure, and to manage dependencies between units, Terragrunt makes it easier to reason about the impact of changes to infrastructure, and to ensure that changes can be made safely. When using Terragrunt, there is very frequently a mapping between your filesystem and the infrastructure you have provisioned with OpenTofu/Terraform. As such, when changing your current working directory in a Terragrunt project, you end up implicitly changing the blast radius of Terragrunt commands. The more units you have as children of your current working directory (the units in your stack), the more infrastructure you are likely to impact with a Terragrunt command. As an adage, you can generally think of this property as: “Your current working directory is your blast radius”. RunSection titled “Run” A run is a single invocation of OpenTofu/Terraform by Terragrunt. Runs are the primary way that Terragrunt does work. When you run terragrunt plan or terragrunt apply, Terragrunt will invoke OpenTofu/Terraform to drive the infrastructure update accordingly. Note that runs abstract away a lot of the complexity that comes from working with OpenTofu/Terraform directly. Terragrunt might automatically perform some code generation, provision requisite resources, or add/modify to the underlying OpenTofu/Terraform configuration to ensure that day to day operations are as smooth as possible. The way in which these complexities are abstracted is via Terragrunt configuration files (terragrunt.hcl), which can be used to define how Terragrunt should forward commands to OpenTofu/Terraform. There is an explicit list of supported shortcuts that Terragrunt will forward to OpenTofu/Terraform by default. For all other commands that need to be forwarded to OpenTofu/Terraform, use the run command (e.g., terragrunt run -- workspace ls). In the simplest case, a run in a unit with an empty terragrunt.hcl file will be equivalent to running OpenTofu/Terraform directly in the unit directory (with some small additional features like automatic initialization and logging adjustments). ExecutionSection titled “Execution” An execution is a single command run by Terragrunt, which does not necessarily have anything to do with OpenTofu/Terraform. Ways in which Terragrunt can perform executions are limited to features like hooks, run_cmd, etc. These utilities are part of what makes Terragrunt so powerful, as they allow users to move infrastructure management complexity out of modules. Run QueueSection titled “Run Queue” The Run Queue is the queue of all units that Terragrunt will do work on over one or more runs. Certain commands like run —all populate the Run Queue with all units in a stack, while other commands like plan or apply will only populate the Run Queue with the unit that the command was run in. Certain flags like —include-dir can be used to adjust the Run Queue to include additional units. Conversely, there are flags like —exclude-dir that can be used to adjust the Run Queue to exclude units. Terragrunt will always attempt to run until the Run Queue is empty. Runner PoolSection titled “Runner Pool” The Runner Pool is the pool of available resources that Terragrunt can use to execute runs. Units are dequeued from the Runner Pool into the Runner Pool depending on factors like parallelism and the DAG. Units are only considered “running” when they are in the Runner Pool. DependencySection titled “Dependency” A dependency is a relationship between two units in a stack that results in data being passed from the dependency to the dependent unit. Dependencies are defined in Terragrunt configuration files using the dependency block. Dependencies are important for resolving the DAG, and the DAG is one of the most important properties to understand with Terragrunt. In an effort to avoid confusing users, Terragrunt maintainers attempt to overload the term “dependency” as little as possible. Other relationships may be described as “reading” or “including” to avoid any ambiguity as to what is relevant to the DAG. IncludeSection titled “Include” The term “include” is used in two different contexts in Terragrunt. Include in configuration: This is when one configuration file is included as partial configuration in another configuration file. This is done using the include block in Terragrunt configuration files. Include in the Run Queue: This is when a unit is included in the Run Queue. There are multiple ways for a unit to be included in the Run Queue. ExcludeSection titled “Exclude” The term “exclude” is only used in the context of excluding units from the Run Queue. VariableSection titled “Variable” A variable is a named dynamic value that is exposed by OpenTofu/Terraform configurations. To avoid ambiguity, Terragrunt maintainers try to avoid using the term “variable” in Terragrunt documentation. InputSection titled “Input” An input is a value configured in Terragrunt configurations to set the value of OpenTofu/Terraform variables. Inputs are defined in Terragrunt configuration files using the inputs attribute. Under the hood, these inputs result in TF_VAR_ prefixed environment variables being populated before initiating a run. OutputSection titled “Output” An output is a value that is returned by OpenTofu/Terraform after a run is completed. By default, Terragrunt will interact with OpenTofu/Terraform in order to retrieve these outputs via dependency blocks. Terragrunt does have the ability to mock outputs, which is useful when dependencies do not yet have outputs to be consumed (e.g. during the run of a unit with a dependency that has not been applied). Terragrunt also has the ability to fetch outputs without interacting with OpenTofu/Terraform via —fetch-dependency-output-from-state for dependencies where state is stored in AWS. This is an experimental feature, and more tooling is planned to make this easier to use. FeatureSection titled “Feature” A feature is a configuration that can be dynamically controlled in Terragrunt configurations. They operate very similarly to variables, but are designed to be used to dynamically adjust the behavior of Terragrunt configurations, rather than OpenTofu/Terraform configurations. Features can be adjusted using feature flags, which are set in Terragrunt configurations using the feature block and the feature flag attribute. Like all good feature flags, you are encouraged to use them with good judgement and to avoid using them as a crutch to avoid making decisions about permanent adjustments to your infrastructure. IaC EngineSection titled “IaC Engine” IaC Engines (typically abbreviated “Engines”) are a way to extend the capabilities of Terragrunt by allowing users to control exactly how Terragrunt performs runs. Engines allow Terragrunt users to author custom logic for how runs are to be executed in plugins, including defining exactly how OpenTofu/Terraform is to be invoked, where OpenTofu/Terraform is to be invoked, etc. Infrastructure EstateSection titled “Infrastructure Estate” An infrastructure estate is all the infrastructure that a person or organization manages. This can be as small as a single resource, or as large as a collection of repositories containing one or more stacks. Generally speaking, the larger the infrastructure estate, the more important it is to have good tooling for managing it. Terragrunt is designed to be able to manage infrastructure estates of any size, and is used by organizations of all sizes to manage their infrastructure efficiently. CLI RedesignSection titled “CLI Redesign” Note that some of the language used in this page may be adjusted in the near future due to RFC #3445. To make terminology and overall UI/UX of using Terragrunt more consistent and easier to understand, the RFC proposes a number of changes to the CLI. This includes renaming some flags, reorganizing some commands, and adjusting some terminology. As of this writing, the RFC is still in the proposal stage, so share your thoughts on the RFC if you have any opinions on the proposed changes. Edit page Previous Install Next Introduction",
    "section": "getting-started",
    "lastUpdated": "2025-10-23T16:40:57.006Z"
  },
  {
    "title": "Introduction",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/",
    "content": "Introduction A common challenge that emerges as infrastructure grows is the “Terralith,” a portmanteau of Terraform and Monolith. This pattern, also referred to as a “Megamodule” or an “All In One State” configuration, describes a scenario where a large, complex infrastructure estate is managed within a single state file. Imagine you’re a platform engineer, and what once felt like an instant tofu apply to update your infrastructure now drags on for minutes. You once had confidence that you could reliably update exactly the infrastructure you cared about changing in every tofu apply, but things have gotten complicated. You now have to sift through a massive wall of plan text to confirm that your intended tag update on a resource doesn’t bring down production. You’re seeing irrelevant timestamp updates, changes introduced out-of-band from colleagues in the AWS console, and more. Maybe it’s faster (and safer) to just go ahead and make the update out-of-band yourself instead of dealing with this monstrosity, exacerbating the issue. This is the scenario that platform engineers find themselves in when they’re struggling to deal with a Terralith. This isn’t a hypothetical scenario, it’s daily life for teams that take what once worked perfectly fine at smaller scales, and incrementally added more and more complexity and technical debt until they find themselves in the position where they can no effectively longer use the Infrastructure as Code (IaC) that served them so well in the past. This is a comprehensive, hands-on guide that will demonstrate how you can naturally find yourself managing a Terralith, and how you can get yourself out, using Terragrunt. Along the way, you’ll learn skills like: Strategies for effectively organizing your IaC for maximum productivity and safety. Principles of state manipulation, and a look under the hood as to how OpenTofu/Terraform store state. Modern best practices for authoring scalable and reliable Terragrunt configuration. The guide will do this by having you: Set up a local development environment for managing IaC. Build a fun toy project, and provision it in a real AWS account. Expand that toy project until you can start to see the impact of managing infrastructure following a Terralith architecture pattern. Break down that Terralith to gain improvements in scalability and reliability. Add Terragrunt to improve your ability to orchestrate your IaC. Leverage more of Terragrunt to further improve your DevEx with IaC. This guide will not assume a significant amount of technical skill with Terragrunt, OpenTofu, AWS or NodeJS, but you will use these tools along the way. The guide will gently guide you through their usage, focusing on teaching you lessons as they pertain to IaC. In the next step of this guide, we’ll make sure you have all of these tools installed (and that you’re signed up for an AWS account). The guide will assume that you’re comfortable using a terminal, and that you have access to a Unix-like environment, either by using a Linux/macOS workstation, or by using Windows Subsystem for Linux (WSL) on Windows. It will also assume that you’re OK with not worrying about certain technical details like how the NodeJS application that you deploy as part of this guide works, as some of those technical details will be glossed over in the interest of focus on the technical details that are relevant in this guide. While not a requirement, it would be good to have a basic understanding of how Git works, so that you can commit updates to your copy of the project as you go along. If you get lost or confused at any point, ask for help in the Terragrunt Discord! There are plenty of passionate Terragrunt community members that are more than happy to help. Edit page Next Overview",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.040Z"
  },
  {
    "title": "Overview",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/overview/",
    "content": "Overview To demonstrate the journey from a Terralith to a scalable Terragrunt setup, we will build and deploy a complete, real-world application early on in this guide, then spend the rest of the guide refactoring the IaC that manages the infrastructure hosting it. The architecture for our sample project is a simple serverless web application hosted in AWS, which consists of three main components: A Lambda-backed website. An S3 bucket to store static assets. A DynamoDB table to store metadata on those assets. This application will allow users to view and vote on their favorite AI-generated images of cats. We’ve intentionally chosen these AWS serverless offerings as they are cost-effective and should be very cheap, if not free, for anyone following along with a new AWS account. What You’ll NeedSection titled “What You’ll Need” To provision the application we build as part of this guide, you will need an AWS account, and permissions to provision resources within it. If you don’t have one, you can follow the official instructions to sign up for one for free. To manage the development dependencies for this project, this guide uses mise, a tool that helps manage project-specific runtimes and tools. You are welcome to install the required tools manually, but using mise (or another tool manager) is recommended when working with IaC, as reproducibility is paramount for ensuring that you can work effectively with colleagues (including future you) on shared infrastructure. If you are happy to install development dependencies with mise, you can install it using the official Mise installation guide. If you would like to manually install all the development dependencies for this guide, you can install them here: Terragrunt OpenTofu NodeJS AWS CLI We will build this project from the ground up, but if you get lost at any point or want to skip ahead, you can find the completed project (as of each step) here. Note that the content shown in code fences in this project will always be displayed in totality, so you can either copy them directly into the filename that’s labeled at the top of the code fence for a file, or run the command directly in your terminal for commands. If a command starts with a $, the intent of the code fence is to demonstrate expected output, so you aren’t expected to copy and paste it directly into your terminal. Edit page Previous Introduction Next Setup",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.075Z"
  },
  {
    "title": "Setup",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/setup/",
    "content": "Setup During the setup phase, you’re going to: Setup a Git repository where this project will live. Install all required dependencies (if you haven’t already). Set up the NodeJS application that you’re going to deploy to the cloud. Acquire the assets used for the application. Let’s get building! Create the Git repositorySection titled “Create the Git repository” Create a new Git project where we’re going to store the IaC for our live infrastructure. mkdir terralith-to-terragruntcd terralith-to-terragruntgit init Install dependencies with miseSection titled “Install dependencies with mise” (Assuming you are using it) Use mise to download, install and pin the version of tools you’re going to use in this project. mise use terragrunt@0.83.2mise use opentofu@1.10.3mise use aws@2.27.63mise use node@22.17.1 You should now have a local mise.toml file that looks like the following with all the tools pinned that you need. [tools]aws = \"2.27.63\"node = \"22.17.1\"opentofu = \"1.10.3\"terragrunt = \"0.83.2\" Setting up the appSection titled “Setting up the app” Now that you have the tools installed that are going to be used for this project, you’ll want to setup the application we’re going to be managing throughout the project. It’s not a very interesting application (it was vibe coded pretty quickly), and the details of how it works aren’t that important to the topic of this blog post. Corners were also cut when designing the application to minimize the resources you have to provision, so don’t design any of your applications based on what you see there. Create the application directory structureSection titled “Create the application directory structure” First, create the application directory structure: mkdir -p app/best-catcd app/best-cat Next, copy the application files into the new directory you just created. package.json index.js template.html styles.css script.js package-lock.json { \"name\": \"best-cat\", \"version\": \"1.0.0\", \"description\": \"Vote for the best cat\", \"main\": \"index.js\", \"type\": \"module\", \"scripts\": { \"package\": \"zip -r ../../dist/best-cat.zip . -x '*.git*' 'node_modules/.cache/*'\" }, \"dependencies\": { \"@aws-sdk/client-s3\": \"^3.420.0\", \"@aws-sdk/client-dynamodb\": \"^3.420.0\", \"@aws-sdk/lib-dynamodb\": \"^3.420.0\", \"@aws-sdk/s3-request-presigner\": \"^3.420.0\", \"handlebars\": \"^4.7.8\" }, \"engines\": { \"node\": \">=22.0.0\" }} import { S3Client, ListObjectsV2Command, GetObjectCommand } from '@aws-sdk/client-s3';import { DynamoDBClient } from '@aws-sdk/client-dynamodb';import { DynamoDBDocumentClient, GetCommand, UpdateCommand, ScanCommand } from '@aws-sdk/lib-dynamodb';import { getSignedUrl } from '@aws-sdk/s3-request-presigner';import { readFileSync } from 'fs';import { join, dirname } from 'path';import { fileURLToPath } from 'url';import Handlebars from 'handlebars'; const s3Client = new S3Client({ maxAttempts: 3, requestHandler: { keepAlive: true }});const dynamoClient = new DynamoDBClient({ maxAttempts: 3, requestHandler: { keepAlive: true }});const dynamodb = DynamoDBDocumentClient.from(dynamoClient); // Get the directory path for ES modulesconst __filename = fileURLToPath(import.meta.url);const __dirname = dirname(__filename); // Load static filesconst templateHtml = readFileSync(join(__dirname, 'template.html'), 'utf8');const stylesCss = readFileSync(join(__dirname, 'styles.css'), 'utf8');const scriptJs = readFileSync(join(__dirname, 'script.js'), 'utf8'); // Compile Handlebars templateconst template = Handlebars.compile(templateHtml); // Server-side cache for presigned URLsconst presignedUrlCache = new Map(); // Server-side cache for S3 list responseconst s3ListCache = { data: null, lastUpdated: 0, ttl: 10 * 1000 // 10 seconds}; // Cache cleanup interval (every 5 minutes)const CACHE_CLEANUP_INTERVAL = 5 * 60 * 1000; // Initialize cache cleanupsetInterval(cleanupExpiredCache, CACHE_CLEANUP_INTERVAL); // Function to clean up expired cache entriesfunction cleanupExpiredCache() { const now = Date.now(); // Clean up presigned URL cache for (const [key, cacheEntry] of presignedUrlCache.entries()) { if (now > cacheEntry.expiresAt) { presignedUrlCache.delete(key); } } // Clean up S3 list cache if expired if (s3ListCache.data && (now - s3ListCache.lastUpdated) > s3ListCache.ttl) { s3ListCache.data = null; s3ListCache.lastUpdated = 0; }} // Function to get or generate presigned URL with cachingasync function getCachedPresignedUrl(bucketName, imageKey) { const cacheKey = `${bucketName}:${imageKey}`; const now = Date.now(); // Check if we have a valid cached URL const cached = presignedUrlCache.get(cacheKey); if (cached && now < cached.expiresAt) { return cached.url; } // Generate new presigned URL const getObjectCommand = new GetObjectCommand({ Bucket: bucketName, Key: imageKey }); const presignedUrl = await getSignedUrl(s3Client, getObjectCommand, { expiresIn: 3600 }); // Cache the URL with expiration presignedUrlCache.set(cacheKey, { url: presignedUrl, expiresAt: now + (3600 * 1000) // 1 hour from now }); return presignedUrl;} // Function to get cached S3 list dataasync function getCachedS3List(bucketName) { const now = Date.now(); // Check if we have valid cached data if (s3ListCache.data && (now - s3ListCache.lastUpdated) < s3ListCache.ttl) { console.log('Using cached S3 list data'); return s3ListCache.data; } // Fetch fresh data from S3 console.log('Fetching fresh S3 list data'); const listCommand = new ListObjectsV2Command({ Bucket: bucketName, MaxKeys: 100 }); const listResponse = await s3Client.send(listCommand); // Cache the response if we got data if (listResponse.Contents && listResponse.Contents.length > 0) { s3ListCache.data = listResponse; s3ListCache.lastUpdated = now; console.log(`Cached S3 list with ${listResponse.Contents.length} objects`); } return listResponse;} export async function handler(event) { const bucketName = process.env.S3_BUCKET_NAME; const tableName = process.env.DYNAMODB_TABLE_NAME; try { // Parse the request - Lambda function URLs have a different event structure const method = event.requestContext?.http?.method || event.httpMethod; const path = event.rawPath || event.path || '/'; console.log('Request:', { method, path, event }); // Serve static files if (method === 'GET' && path === '/styles.css') { return { statusCode: 200, headers: { 'Content-Type': 'text/css', 'Cache-Control': 'public, max-age=3600' }, body: stylesCss }; } if (method === 'GET' && path === '/script.js') { return { statusCode: 200, headers: { 'Content-Type': 'application/javascript', 'Cache-Control': 'public, max-age=3600' }, body: scriptJs }; } // Serve images with caching headers if (method === 'GET' && path.startsWith('/image/')) { const imageKey = path.substring(7); // Remove '/image/' prefix try { const presignedUrl = await getCachedPresignedUrl(bucketName, imageKey); return { statusCode: 302, // Redirect to presigned URL headers: { 'Location': presignedUrl, 'Cache-Control': 'public, max-age=3600, immutable' } }; } catch (error) { console.error('Error generating presigned URL:', error); return { statusCode: 404, headers: { 'Content-Type': 'text/html' }, body: '<h1>Image not found</h1>' }; } } // Main page - serve HTML if (method === 'GET' && (path === '/' || path === '')) { // Get all images from S3 (with caching) const listResponse = await getCachedS3List(bucketName); // Get all vote counts from DynamoDB const scanCommand = new ScanCommand({ TableName: tableName }); const votesResponse = await dynamodb.send(scanCommand); const votesMap = {}; if (votesResponse.Items) { votesResponse.Items.forEach(item => { votesMap[item.image_id] = item.votes || 0; }); } // Filter and prepare image data first const imageObjects = (listResponse.Contents || []).filter(obj => obj.Key && obj.Key.match(/\\.(jpg|jpeg|png|gif|webp)$/i) ); // Generate presigned URLs in parallel for better performance const imagesWithUrls = await Promise.all( imageObjects.map(async (obj) => { const presignedUrl = await getCachedPresignedUrl(bucketName, obj.Key); return { key: obj.Key, keyId: obj.Key.replace(/[^a-zA-Z0-9]/g, '-'), url: presignedUrl, votes: votesMap[obj.Key] || 0 }; }) ); // Sort images by vote count (highest votes first) imagesWithUrls.sort((a, b) => b.votes - a.votes); // Render the template with data const html = template({ images: imagesWithUrls }); return { statusCode: 200, headers: { 'Content-Type': 'text/html', 'Cache-Control': 'public, max-age=60, s-maxage=300' }, body: html }; } // API endpoint for voting if (method === 'POST' && path.startsWith('/vote/')) { const parts = path.split('/'); const imageId = parts[2]; const voteType = parts[3]; // 'up' or 'down' const voteIncrement = voteType === 'up' ? 1 : -1; // Update vote count in DynamoDB const updateCommand = new UpdateCommand({ TableName: tableName, Key: { image_id: imageId }, UpdateExpression: 'SET votes = if_not_exists(votes, :zero) + :inc', ExpressionAttributeValues: { ':inc': voteIncrement, ':zero': 0 }, ReturnValues: 'UPDATED_NEW' }); const result = await dynamodb.send(updateCommand); return { statusCode: 200, headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ message: 'Vote recorded successfully', image_id: imageId, new_vote_count: result.Attributes.votes }) }; } // API endpoint to get current vote counts if (method === 'GET' && path === '/api/votes') { const scanCommand = new ScanCommand({ TableName: tableName }); const votes = await dynamodb.send(scanCommand); return { statusCode: 200, headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ votes: votes.Items || [] }) }; } // For any other GET request, serve the main page (helpful for debugging) if (method === 'GET') { console.log('Serving main page for path:', path); // Get all images from S3 (with caching) const listResponse = await getCachedS3List(bucketName); // Get all vote counts from DynamoDB const scanCommand = new ScanCommand({ TableName: tableName }); const votesResponse = await dynamodb.send(scanCommand); const votesMap = {}; if (votesResponse.Items) { votesResponse.Items.forEach(item => { votesMap[item.image_id] = item.votes || 0; }); } // Filter and prepare image data efficiently const imageObjects = (listResponse.Contents || []).filter(obj => obj.Key && obj.Key.match(/\\.(jpg|jpeg|png|gif|webp)$/i) ); const imagesWithUrls = imageObjects.map(obj => ({ key: obj.Key, keyId: obj.Key.replace(/[^a-zA-Z0-9]/g, '-'), votes: votesMap[obj.Key] || 0 })); // Sort images by vote count (highest votes first) imagesWithUrls.sort((a, b) => b.votes - a.votes); // Render the template with data const html = template({ images: imagesWithUrls }); return { statusCode: 200, headers: { 'Content-Type': 'text/html', 'Cache-Control': 'public, max-age=60, s-maxage=300' }, body: html }; } // Default response for unknown endpoints return { statusCode: 404, headers: { 'Content-Type': 'text/html' }, body: ` <html> <head><title>404 - Not Found</title></head> <body> <h1>404 - Page Not Found</h1> <p>The requested page could not be found.</p> <p>Method: ${method}, Path: ${path}</p> <a href=\"/\">Go back to the main page</a> </body> </html> ` }; } catch (error) { console.error('Error:', error); return { statusCode: 500, headers: { 'Content-Type': 'text/html' }, body: ` <html> <head><title>500 - Server Error</title></head> <body> <h1>500 - Internal Server Error</h1> <p>An error occurred while processing your request.</p> <a href=\"/\">Go back to the main page</a> </body> </html> ` }; }} <!DOCTYPE html><html lang=\"en\"><head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Best Cat Voting</title> <link rel=\"stylesheet\" href=\"/styles.css\"></head><body> <div class=\"container\"> <h1>🐱 Vote for the Best Cat! 🐱</h1> <div class=\"images-grid\" id=\"imagesGrid\"> {{#if images.length}} {{#each images}} <div class=\"image-card\" data-image-key=\"{{this.key}}\"> <div class=\"image-container\"> <img src=\"/image/{{this.key}}\" alt=\"Cat image\" loading=\"lazy\" decoding=\"async\"> </div> <div class=\"voting-section\"> <div class=\"vote-count\" id=\"votes-{{this.keyId}}\">{{this.votes}}</div> <div class=\"vote-buttons\"> <button class=\"vote-btn up\" onclick=\"vote('{{this.key}}', 'up')\" title=\"Vote Up\">⬆️</button> <button class=\"vote-btn down\" onclick=\"vote('{{this.key}}', 'down')\" title=\"Vote Down\">⬇️</button> </div> </div> </div> {{/each}} {{else}} <div class=\"no-images\">No images found. Upload some cat pictures to get started!</div> {{/if}} </div> </div> <script src=\"/script.js\"></script></body></html> * { margin: 0; padding: 0; box-sizing: border-box;} body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; margin: 0; padding: 0; display: flex; align-items: center; justify-content: center;} .container { max-width: 1200px; width: 100%; padding: 20px; text-align: center;} h1 { text-align: center; color: white; margin-bottom: 40px; font-size: 3rem; text-shadow: 2px 2px 4px rgba(0,0,0,0.3); font-weight: 700; letter-spacing: 1px;} .images-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(320px, 1fr)); gap: 25px; margin-top: 30px; justify-items: center;} .image-card { background: white; border-radius: 20px; overflow: hidden; box-shadow: 0 15px 35px rgba(0,0,0,0.15); transition: all 0.3s ease; width: 100%; max-width: 350px;} .image-card:hover { transform: translateY(-8px); box-shadow: 0 20px 50px rgba(0,0,0,0.25);} .image-container { position: relative; width: 100%; height: 250px; overflow: hidden;} .image-container img { width: 100%; height: 100%; object-fit: cover; transition: transform 0.3s ease;} .image-card:hover .image-container img { transform: scale(1.05);} .voting-section { padding: 20px; text-align: center;} .vote-count { font-size: 1.8rem; font-weight: bold; color: #333; margin-bottom: 20px; background: linear-gradient(135deg, #667eea, #764ba2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;} .vote-buttons { display: flex; justify-content: center; gap: 10px;} .vote-btn { background: none; border: 2px solid #e0e0e0; border-radius: 50%; width: 55px; height: 55px; font-size: 1.6rem; cursor: pointer; transition: all 0.3s ease; display: flex; align-items: center; justify-content: center; position: relative; overflow: hidden;} .vote-btn:hover { border-color: #667eea; background: linear-gradient(135deg, #667eea, #764ba2); color: white; transform: scale(1.1); box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);} .vote-btn:active { transform: scale(0.95);} .vote-btn.up:hover { border-color: #4CAF50; background: linear-gradient(135deg, #4CAF50, #45a049); box-shadow: 0 5px 15px rgba(76, 175, 80, 0.4);} .vote-btn.down:hover { border-color: #f44336; background: linear-gradient(135deg, #f44336, #d32f2f); box-shadow: 0 5px 15px rgba(244, 67, 54, 0.4);} .vote-btn:disabled { cursor: not-allowed; opacity: 0.6; transform: none !important;} .vote-btn:disabled:hover { border-color: #e0e0e0; background: none; box-shadow: none; transform: none;} .loading { text-align: center; color: white; font-size: 1.2rem; margin: 50px 0;} .no-images { text-align: center; color: white; font-size: 1.4rem; margin: 60px auto; line-height: 1.6; background: rgba(255, 255, 255, 0.1); padding: 40px; border-radius: 15px; backdrop-filter: blur(10px); border: 1px solid rgba(255, 255, 255, 0.2); max-width: 500px; width: 100%; grid-column: 1 / -1; justify-self: center;} @media (max-width: 768px) { .images-grid { grid-template-columns: repeat(auto-fill, minmax(280px, 1fr)); gap: 20px; } h1 { font-size: 2.2rem; margin-bottom: 30px; } .vote-btn { width: 50px; height: 50px; font-size: 1.4rem; } .no-images { font-size: 1.2rem; padding: 30px; margin: 40px 0; }} // Global variable to store the base URL for API callslet baseUrl = ''; // Initialize the applicationdocument.addEventListener('DOMContentLoaded', function() { // Set the base URL from the current location baseUrl = window.location.origin; // Add click effects to vote buttons const voteButtons = document.querySelectorAll('.vote-btn'); voteButtons.forEach(button => { button.addEventListener('click', function() { // Create a subtle click effect this.style.transform = 'scale(0.9)'; setTimeout(() => { this.style.transform = ''; }, 150); }); });}); // Function to handle voting asynchronouslyasync function vote(imageKey, voteType) { const voteElement = document.getElementById(`votes-${imageKey.replace(/[^a-zA-Z0-9]/g, '-')}`); const voteButtons = document.querySelectorAll(`[data-image-key=\"${imageKey}\"] .vote-btn`); if (!voteElement) return; // Get current vote count const currentVotes = parseInt(voteElement.textContent) || 0; const voteIncrement = voteType === 'up' ? 1 : -1; // Optimistically update the UI immediately const newVoteCount = currentVotes + voteIncrement; voteElement.textContent = newVoteCount; // Add immediate visual feedback voteElement.style.transform = 'scale(1.2)'; voteElement.style.color = voteType === 'up' ? '#4CAF50' : '#f44336'; // Disable vote buttons to prevent double-clicking voteButtons.forEach(btn => { btn.disabled = true; btn.style.opacity = '0.6'; }); // Send vote request asynchronously const votePromise = fetch(`${baseUrl}/vote/${imageKey}/${voteType}`, { method: 'POST', headers: { 'Content-Type': 'application/json' } }); try { const response = await votePromise; if (response.ok) { const result = await response.json(); // Update with actual server response voteElement.textContent = result.new_vote_count; // Success animation voteElement.style.transform = 'scale(1.1)'; setTimeout(() => { voteElement.style.transform = 'scale(1)'; voteElement.style.color = ''; }, 300); } else { // Revert optimistic update on error voteElement.textContent = currentVotes; console.error('Vote failed:', response.statusText); // Show error feedback voteElement.style.transform = 'scale(1.1)'; voteElement.style.color = '#f44336'; setTimeout(() => { voteElement.style.transform = 'scale(1)'; voteElement.style.color = ''; }, 300); // Show user-friendly error message showNotification('Vote failed. Please try again.', 'error'); } } catch (error) { // Revert optimistic update on network error voteElement.textContent = currentVotes; console.error('Error voting:', error); // Show error feedback voteElement.style.transform = 'scale(1.1)'; voteElement.style.color = '#f44336'; setTimeout(() => { voteElement.style.transform = 'scale(1)'; voteElement.style.color = ''; }, 300); // Show user-friendly error message showNotification('Network error. Please check your connection.', 'error'); } finally { // Re-enable vote buttons voteButtons.forEach(btn => { btn.disabled = false; btn.style.opacity = '1'; }); }} // Function to show notificationsfunction showNotification(message, type = 'info') { // Remove existing notifications const existingNotification = document.querySelector('.notification'); if (existingNotification) { existingNotification.remove(); } // Create notification element const notification = document.createElement('div'); notification.className = `notification notification-${type}`; notification.textContent = message; // Add styles notification.style.cssText = ` position: fixed; top: 20px; right: 20px; padding: 12px 20px; border-radius: 8px; color: white; font-weight: 500; z-index: 1000; transform: translateX(100%); transition: transform 0.3s ease; max-width: 300px; word-wrap: break-word; `; // Set background color based on type if (type === 'error') { notification.style.backgroundColor = '#f44336'; } else if (type === 'success') { notification.style.backgroundColor = '#4CAF50'; } else { notification.style.backgroundColor = '#2196F3'; } // Add to page document.body.appendChild(notification); // Animate in setTimeout(() => { notification.style.transform = 'translateX(0)'; }, 100); // Auto-remove after 3 seconds setTimeout(() => { notification.style.transform = 'translateX(100%)'; setTimeout(() => { if (notification.parentNode) { notification.remove(); } }, 300); }, 3000);} { \"name\": \"best-cat\", \"version\": \"1.0.0\", \"lockfileVersion\": 3, \"requires\": true, \"packages\": { \"\": { \"name\": \"best-cat\", \"version\": \"1.0.0\", \"dependencies\": { \"@aws-sdk/client-dynamodb\": \"^3.420.0\", \"@aws-sdk/client-s3\": \"^3.420.0\", \"@aws-sdk/lib-dynamodb\": \"^3.420.0\", \"@aws-sdk/s3-request-presigner\": \"^3.420.0\", \"handlebars\": \"^4.7.8\" }, \"engines\": { \"node\": \">=22.0.0\" } }, \"node_modules/@aws-crypto/crc32\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/crc32/-/crc32-5.2.0.tgz\", \"integrity\": \"sha512-nLbCWqQNgUiwwtFsen1AdzAtvuLRsQS8rYgMuxCrdKf9kOssamGLuPwyTY9wyYblNr9+1XM8v6zoDTPPSIeANg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/util\": \"^5.2.0\", \"@aws-sdk/types\": \"^3.222.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=16.0.0\" } }, \"node_modules/@aws-crypto/crc32c\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/crc32c/-/crc32c-5.2.0.tgz\", \"integrity\": \"sha512-+iWb8qaHLYKrNvGRbiYRHSdKRWhto5XlZUEBwDjYNf+ly5SVYG6zEoYIdxvf5R3zyeP16w4PLBn3rH1xc74Rag==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/util\": \"^5.2.0\", \"@aws-sdk/types\": \"^3.222.0\", \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-crypto/sha1-browser\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/sha1-browser/-/sha1-browser-5.2.0.tgz\", \"integrity\": \"sha512-OH6lveCFfcDjX4dbAvCFSYUjJZjDr/3XJ3xHtjn3Oj5b9RjojQo8npoLeA/bNwkOkrSQ0wgrHzXk4tDRxGKJeg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/supports-web-crypto\": \"^5.2.0\", \"@aws-crypto/util\": \"^5.2.0\", \"@aws-sdk/types\": \"^3.222.0\", \"@aws-sdk/util-locate-window\": \"^3.0.0\", \"@smithy/util-utf8\": \"^2.0.0\", \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-crypto/sha1-browser/node_modules/@smithy/is-array-buffer\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/is-array-buffer/-/is-array-buffer-2.2.0.tgz\", \"integrity\": \"sha512-GGP3O9QFD24uGeAXYUjwSTXARoqpZykHadOmA8G5vfJPK0/DC67qa//0qvqrJzL1xc8WQWX7/yc7fwudjPHPhA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha1-browser/node_modules/@smithy/util-buffer-from\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-buffer-from/-/util-buffer-from-2.2.0.tgz\", \"integrity\": \"sha512-IJdWBbTcMQ6DA0gdNhh/BwrLkDR+ADW5Kr1aZmd4k3DIF6ezMV4R2NIAmT08wQJ3yUK82thHWmC/TnK/wpMMIA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/is-array-buffer\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha1-browser/node_modules/@smithy/util-utf8\": { \"version\": \"2.3.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-utf8/-/util-utf8-2.3.0.tgz\", \"integrity\": \"sha512-R8Rdn8Hy72KKcebgLiv8jQcQkXoLMOGGv5uI1/k0l+snqkOzQ1R0ChUBCxWMlBsFMekWjq0wRudIweFs7sKT5A==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-buffer-from\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha256-browser\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/sha256-browser/-/sha256-browser-5.2.0.tgz\", \"integrity\": \"sha512-AXfN/lGotSQwu6HNcEsIASo7kWXZ5HYWvfOmSNKDsEqC4OashTp8alTmaz+F7TC2L083SFv5RdB+qU3Vs1kZqw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/sha256-js\": \"^5.2.0\", \"@aws-crypto/supports-web-crypto\": \"^5.2.0\", \"@aws-crypto/util\": \"^5.2.0\", \"@aws-sdk/types\": \"^3.222.0\", \"@aws-sdk/util-locate-window\": \"^3.0.0\", \"@smithy/util-utf8\": \"^2.0.0\", \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-crypto/sha256-browser/node_modules/@smithy/is-array-buffer\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/is-array-buffer/-/is-array-buffer-2.2.0.tgz\", \"integrity\": \"sha512-GGP3O9QFD24uGeAXYUjwSTXARoqpZykHadOmA8G5vfJPK0/DC67qa//0qvqrJzL1xc8WQWX7/yc7fwudjPHPhA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha256-browser/node_modules/@smithy/util-buffer-from\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-buffer-from/-/util-buffer-from-2.2.0.tgz\", \"integrity\": \"sha512-IJdWBbTcMQ6DA0gdNhh/BwrLkDR+ADW5Kr1aZmd4k3DIF6ezMV4R2NIAmT08wQJ3yUK82thHWmC/TnK/wpMMIA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/is-array-buffer\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha256-browser/node_modules/@smithy/util-utf8\": { \"version\": \"2.3.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-utf8/-/util-utf8-2.3.0.tgz\", \"integrity\": \"sha512-R8Rdn8Hy72KKcebgLiv8jQcQkXoLMOGGv5uI1/k0l+snqkOzQ1R0ChUBCxWMlBsFMekWjq0wRudIweFs7sKT5A==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-buffer-from\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/sha256-js\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/sha256-js/-/sha256-js-5.2.0.tgz\", \"integrity\": \"sha512-FFQQyu7edu4ufvIZ+OadFpHHOt+eSTBaYaki44c+akjg7qZg9oOQeLlk77F6tSYqjDAFClrHJk9tMf0HdVyOvA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/util\": \"^5.2.0\", \"@aws-sdk/types\": \"^3.222.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=16.0.0\" } }, \"node_modules/@aws-crypto/supports-web-crypto\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/supports-web-crypto/-/supports-web-crypto-5.2.0.tgz\", \"integrity\": \"sha512-iAvUotm021kM33eCdNfwIN//F77/IADDSs58i+MDaOqFrVjZo9bAal0NK7HurRuWLLpF1iLX7gbWrjHjeo+YFg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-crypto/util\": { \"version\": \"5.2.0\", \"resolved\": \"https://registry.npmjs.org/@aws-crypto/util/-/util-5.2.0.tgz\", \"integrity\": \"sha512-4RkU9EsI6ZpBve5fseQlGNUWKMa1RLPQ1dnjnQoe07ldfIzcsGb5hC5W0Dm7u423KWzawlrpbjXBrXCEv9zazQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"^3.222.0\", \"@smithy/util-utf8\": \"^2.0.0\", \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-crypto/util/node_modules/@smithy/is-array-buffer\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/is-array-buffer/-/is-array-buffer-2.2.0.tgz\", \"integrity\": \"sha512-GGP3O9QFD24uGeAXYUjwSTXARoqpZykHadOmA8G5vfJPK0/DC67qa//0qvqrJzL1xc8WQWX7/yc7fwudjPHPhA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/util/node_modules/@smithy/util-buffer-from\": { \"version\": \"2.2.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-buffer-from/-/util-buffer-from-2.2.0.tgz\", \"integrity\": \"sha512-IJdWBbTcMQ6DA0gdNhh/BwrLkDR+ADW5Kr1aZmd4k3DIF6ezMV4R2NIAmT08wQJ3yUK82thHWmC/TnK/wpMMIA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/is-array-buffer\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-crypto/util/node_modules/@smithy/util-utf8\": { \"version\": \"2.3.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-utf8/-/util-utf8-2.3.0.tgz\", \"integrity\": \"sha512-R8Rdn8Hy72KKcebgLiv8jQcQkXoLMOGGv5uI1/k0l+snqkOzQ1R0ChUBCxWMlBsFMekWjq0wRudIweFs7sKT5A==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-buffer-from\": \"^2.2.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=14.0.0\" } }, \"node_modules/@aws-sdk/client-dynamodb\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/client-dynamodb/-/client-dynamodb-3.857.0.tgz\", \"integrity\": \"sha512-LnTNiaycQwUE2fpQZW7DU4hgyVOAvBTQd1pouo3V7gklUtCafTf/q7vLMEVoe0uG2YYs1HXUlHo+GT71LM+Xew==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/sha256-browser\": \"5.2.0\", \"@aws-crypto/sha256-js\": \"5.2.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/credential-provider-node\": \"3.857.0\", \"@aws-sdk/middleware-endpoint-discovery\": \"3.840.0\", \"@aws-sdk/middleware-host-header\": \"3.840.0\", \"@aws-sdk/middleware-logger\": \"3.840.0\", \"@aws-sdk/middleware-recursion-detection\": \"3.840.0\", \"@aws-sdk/middleware-user-agent\": \"3.857.0\", \"@aws-sdk/region-config-resolver\": \"3.840.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-endpoints\": \"3.848.0\", \"@aws-sdk/util-user-agent-browser\": \"3.840.0\", \"@aws-sdk/util-user-agent-node\": \"3.857.0\", \"@smithy/config-resolver\": \"^4.1.4\", \"@smithy/core\": \"^3.7.2\", \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/hash-node\": \"^4.0.4\", \"@smithy/invalid-dependency\": \"^4.0.4\", \"@smithy/middleware-content-length\": \"^4.0.4\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/middleware-retry\": \"^4.1.18\", \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/middleware-stack\": \"^4.0.4\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-body-length-node\": \"^4.0.0\", \"@smithy/util-defaults-mode-browser\": \"^4.0.25\", \"@smithy/util-defaults-mode-node\": \"^4.0.25\", \"@smithy/util-endpoints\": \"^3.0.6\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-retry\": \"^4.0.6\", \"@smithy/util-utf8\": \"^4.0.0\", \"@smithy/util-waiter\": \"^4.0.6\", \"@types/uuid\": \"^9.0.1\", \"tslib\": \"^2.6.2\", \"uuid\": \"^9.0.1\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/client-dynamodb/node_modules/uuid\": { \"version\": \"9.0.1\", \"resolved\": \"https://registry.npmjs.org/uuid/-/uuid-9.0.1.tgz\", \"integrity\": \"sha512-b+1eJOlsR9K8HJpow9Ok3fiWOWSIcIzXodvv0rQjVoOVNpWMpxf1wZNpt4y9h10odCNrqnYp1OBzRktckBe3sA==\", \"funding\": [ \"https://github.com/sponsors/broofa\", \"https://github.com/sponsors/ctavan\" ], \"license\": \"MIT\", \"bin\": { \"uuid\": \"dist/bin/uuid\" } }, \"node_modules/@aws-sdk/client-s3\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/client-s3/-/client-s3-3.857.0.tgz\", \"integrity\": \"sha512-kdNgv0QUIDc3nBStIXa22lX7WbfFmChTDHzONa53ZPIaP2E8CkPJJeZS55VRzHE7FytF34uP+6q1jDysdSTeYA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/sha1-browser\": \"5.2.0\", \"@aws-crypto/sha256-browser\": \"5.2.0\", \"@aws-crypto/sha256-js\": \"5.2.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/credential-provider-node\": \"3.857.0\", \"@aws-sdk/middleware-bucket-endpoint\": \"3.840.0\", \"@aws-sdk/middleware-expect-continue\": \"3.840.0\", \"@aws-sdk/middleware-flexible-checksums\": \"3.857.0\", \"@aws-sdk/middleware-host-header\": \"3.840.0\", \"@aws-sdk/middleware-location-constraint\": \"3.840.0\", \"@aws-sdk/middleware-logger\": \"3.840.0\", \"@aws-sdk/middleware-recursion-detection\": \"3.840.0\", \"@aws-sdk/middleware-sdk-s3\": \"3.857.0\", \"@aws-sdk/middleware-ssec\": \"3.840.0\", \"@aws-sdk/middleware-user-agent\": \"3.857.0\", \"@aws-sdk/region-config-resolver\": \"3.840.0\", \"@aws-sdk/signature-v4-multi-region\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-endpoints\": \"3.848.0\", \"@aws-sdk/util-user-agent-browser\": \"3.840.0\", \"@aws-sdk/util-user-agent-node\": \"3.857.0\", \"@aws-sdk/xml-builder\": \"3.821.0\", \"@smithy/config-resolver\": \"^4.1.4\", \"@smithy/core\": \"^3.7.2\", \"@smithy/eventstream-serde-browser\": \"^4.0.4\", \"@smithy/eventstream-serde-config-resolver\": \"^4.1.2\", \"@smithy/eventstream-serde-node\": \"^4.0.4\", \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/hash-blob-browser\": \"^4.0.4\", \"@smithy/hash-node\": \"^4.0.4\", \"@smithy/hash-stream-node\": \"^4.0.4\", \"@smithy/invalid-dependency\": \"^4.0.4\", \"@smithy/md5-js\": \"^4.0.4\", \"@smithy/middleware-content-length\": \"^4.0.4\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/middleware-retry\": \"^4.1.18\", \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/middleware-stack\": \"^4.0.4\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-body-length-node\": \"^4.0.0\", \"@smithy/util-defaults-mode-browser\": \"^4.0.25\", \"@smithy/util-defaults-mode-node\": \"^4.0.25\", \"@smithy/util-endpoints\": \"^3.0.6\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-retry\": \"^4.0.6\", \"@smithy/util-stream\": \"^4.2.3\", \"@smithy/util-utf8\": \"^4.0.0\", \"@smithy/util-waiter\": \"^4.0.6\", \"@types/uuid\": \"^9.0.1\", \"tslib\": \"^2.6.2\", \"uuid\": \"^9.0.1\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/client-s3/node_modules/uuid\": { \"version\": \"9.0.1\", \"resolved\": \"https://registry.npmjs.org/uuid/-/uuid-9.0.1.tgz\", \"integrity\": \"sha512-b+1eJOlsR9K8HJpow9Ok3fiWOWSIcIzXodvv0rQjVoOVNpWMpxf1wZNpt4y9h10odCNrqnYp1OBzRktckBe3sA==\", \"funding\": [ \"https://github.com/sponsors/broofa\", \"https://github.com/sponsors/ctavan\" ], \"license\": \"MIT\", \"bin\": { \"uuid\": \"dist/bin/uuid\" } }, \"node_modules/@aws-sdk/client-sso\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/client-sso/-/client-sso-3.857.0.tgz\", \"integrity\": \"sha512-0jXF4YJ3mGspNsxOU1rdk1uTtm/xadSWvgU+JQb2YCnallEDeT/Kahlyg4GOzPDj0UnnYWsD2s1Hx82O08SbiQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/sha256-browser\": \"5.2.0\", \"@aws-crypto/sha256-js\": \"5.2.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/middleware-host-header\": \"3.840.0\", \"@aws-sdk/middleware-logger\": \"3.840.0\", \"@aws-sdk/middleware-recursion-detection\": \"3.840.0\", \"@aws-sdk/middleware-user-agent\": \"3.857.0\", \"@aws-sdk/region-config-resolver\": \"3.840.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-endpoints\": \"3.848.0\", \"@aws-sdk/util-user-agent-browser\": \"3.840.0\", \"@aws-sdk/util-user-agent-node\": \"3.857.0\", \"@smithy/config-resolver\": \"^4.1.4\", \"@smithy/core\": \"^3.7.2\", \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/hash-node\": \"^4.0.4\", \"@smithy/invalid-dependency\": \"^4.0.4\", \"@smithy/middleware-content-length\": \"^4.0.4\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/middleware-retry\": \"^4.1.18\", \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/middleware-stack\": \"^4.0.4\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-body-length-node\": \"^4.0.0\", \"@smithy/util-defaults-mode-browser\": \"^4.0.25\", \"@smithy/util-defaults-mode-node\": \"^4.0.25\", \"@smithy/util-endpoints\": \"^3.0.6\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-retry\": \"^4.0.6\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/core\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/core/-/core-3.857.0.tgz\", \"integrity\": \"sha512-mgtjKignFcCl19TS6vKbC3e9jtogg6S38a0HFFWjcqMCUAskM+ZROickVTKsYeAk7FoYa2++YkM0qz8J/yteVA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/xml-builder\": \"3.821.0\", \"@smithy/core\": \"^3.7.2\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/signature-v4\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-utf8\": \"^4.0.0\", \"fast-xml-parser\": \"5.2.5\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-env\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-env/-/credential-provider-env-3.857.0.tgz\", \"integrity\": \"sha512-i9NjopufQc7mrJr2lVU4DU5cLGJQ1wNEucnP6XcpCozbJfGJExU9o/VY27qU/pI8V0zK428KXuABuN70Qb+xkw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-http\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-http/-/credential-provider-http-3.857.0.tgz\", \"integrity\": \"sha512-Ig1dwbn+vO7Fo+2uznZ6Pv0xoLIWz6ndzJygn2eR2MRi6LvZSnTZqbeovjJeoEzWO2xFdK++SyjS7aEuAMAmzw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-stream\": \"^4.2.3\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-ini\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-ini/-/credential-provider-ini-3.857.0.tgz\", \"integrity\": \"sha512-w24ABs913sweDFz0aX/PGEfK1jgpV21a2E8p78ueSkQ7Fb7ELVzsv1C16ESFDDF++P4KVkxNQrjRuKw/5+T7ug==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/credential-provider-env\": \"3.857.0\", \"@aws-sdk/credential-provider-http\": \"3.857.0\", \"@aws-sdk/credential-provider-process\": \"3.857.0\", \"@aws-sdk/credential-provider-sso\": \"3.857.0\", \"@aws-sdk/credential-provider-web-identity\": \"3.857.0\", \"@aws-sdk/nested-clients\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/credential-provider-imds\": \"^4.0.6\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-node\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-node/-/credential-provider-node-3.857.0.tgz\", \"integrity\": \"sha512-4ulf6NmbGrE1S+8eAHZQ/krvd441KdKvpT3bFoTsg+89YlGwobW+C+vy94qQBx0iKbqkILbLeFF2F/Bf/ACnmw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/credential-provider-env\": \"3.857.0\", \"@aws-sdk/credential-provider-http\": \"3.857.0\", \"@aws-sdk/credential-provider-ini\": \"3.857.0\", \"@aws-sdk/credential-provider-process\": \"3.857.0\", \"@aws-sdk/credential-provider-sso\": \"3.857.0\", \"@aws-sdk/credential-provider-web-identity\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/credential-provider-imds\": \"^4.0.6\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-process\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-process/-/credential-provider-process-3.857.0.tgz\", \"integrity\": \"sha512-WLSLM4+vDyrjT+aeaiUHkAxUXUSQSXIQT8ZoS7RHo2BvTlpBOJY9nxvcmKWNCQ2hW2AhVjqBeMjVz3u3fFhoJQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-sso\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-sso/-/credential-provider-sso-3.857.0.tgz\", \"integrity\": \"sha512-OfbkZ//9+nC2HH+3cbjjQz4d4ODQsFml38mPvwq7FSiVrUR7hxgE7OQael4urqKVWLEqFt6/PCr+QZq0J4dJ1A==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/client-sso\": \"3.857.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/token-providers\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/credential-provider-web-identity\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/credential-provider-web-identity/-/credential-provider-web-identity-3.857.0.tgz\", \"integrity\": \"sha512-aj1QbOyhu+bl+gsgIpMuvVRJa1LkgwHzyu6lzjCrPxuPO6ytHDMmii+QUyM9P5K3Xk6fT/JGposhMFB5AtI+Og==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/nested-clients\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/endpoint-cache\": { \"version\": \"3.804.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/endpoint-cache/-/endpoint-cache-3.804.0.tgz\", \"integrity\": \"sha512-TQVDkA/lV6ua75ELZaichMzlp6x7tDa1bqdy/+0ZftmODPtKXuOOEcJxmdN7Ui/YRo1gkRz2D9txYy7IlNg1Og==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"mnemonist\": \"0.38.3\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/lib-dynamodb\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/lib-dynamodb/-/lib-dynamodb-3.857.0.tgz\", \"integrity\": \"sha512-5CDHFlcAmr35xKOB7QyxopaBcQeAx0lJs395GKhkvKbJTK+Sp5qg91uWmb7+wKh1zL/rDUd4uwGRlSDhJcDRfQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/util-dynamodb\": \"3.857.0\", \"@smithy/core\": \"^3.7.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" }, \"peerDependencies\": { \"@aws-sdk/client-dynamodb\": \"^3.857.0\" } }, \"node_modules/@aws-sdk/middleware-bucket-endpoint\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-bucket-endpoint/-/middleware-bucket-endpoint-3.840.0.tgz\", \"integrity\": \"sha512-+gkQNtPwcSMmlwBHFd4saVVS11In6ID1HczNzpM3MXKXRBfSlbZJbCt6wN//AZ8HMklZEik4tcEOG0qa9UY8SQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-arn-parser\": \"3.804.0\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-config-provider\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-endpoint-discovery\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-endpoint-discovery/-/middleware-endpoint-discovery-3.840.0.tgz\", \"integrity\": \"sha512-IJDShY5NOg9luTE8h4o2Bm+gsPnHIU0tVWCjMz8vZMLevYjKdIsatcXiu3huTOjKSnelzC9fBHfU6KKsHmjjBQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/endpoint-cache\": \"3.804.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-expect-continue\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-expect-continue/-/middleware-expect-continue-3.840.0.tgz\", \"integrity\": \"sha512-iJg2r6FKsKKvdiU4oCOuCf7Ro/YE0Q2BT/QyEZN3/Rt8Nr4SAZiQOlcBXOCpGvuIKOEAhvDOUnW3aDHL01PdVw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-flexible-checksums\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-flexible-checksums/-/middleware-flexible-checksums-3.857.0.tgz\", \"integrity\": \"sha512-6iHar8RMW1JHYHlho3AQXEwvMmFSfxZHaj6d+TR/os0YrmQFBkLqpK8OBmJ750qa0S6QB22s+8kmbH4BCpeccg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/crc32\": \"5.2.0\", \"@aws-crypto/crc32c\": \"5.2.0\", \"@aws-crypto/util\": \"5.2.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/is-array-buffer\": \"^4.0.0\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-stream\": \"^4.2.3\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-host-header\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-host-header/-/middleware-host-header-3.840.0.tgz\", \"integrity\": \"sha512-ub+hXJAbAje94+Ya6c6eL7sYujoE8D4Bumu1NUI8TXjUhVVn0HzVWQjpRLshdLsUp1AW7XyeJaxyajRaJQ8+Xg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-location-constraint\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-location-constraint/-/middleware-location-constraint-3.840.0.tgz\", \"integrity\": \"sha512-KVLD0u0YMF3aQkVF8bdyHAGWSUY6N1Du89htTLgqCcIhSxxAJ9qifrosVZ9jkAzqRW99hcufyt2LylcVU2yoKQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-logger\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-logger/-/middleware-logger-3.840.0.tgz\", \"integrity\": \"sha512-lSV8FvjpdllpGaRspywss4CtXV8M7NNNH+2/j86vMH+YCOZ6fu2T/TyFd/tHwZ92vDfHctWkRbQxg0bagqwovA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-recursion-detection\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-recursion-detection/-/middleware-recursion-detection-3.840.0.tgz\", \"integrity\": \"sha512-Gu7lGDyfddyhIkj1Z1JtrY5NHb5+x/CRiB87GjaSrKxkDaydtX2CU977JIABtt69l9wLbcGDIQ+W0uJ5xPof7g==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-sdk-s3\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-sdk-s3/-/middleware-sdk-s3-3.857.0.tgz\", \"integrity\": \"sha512-qKbr6I6+4kRvI9guR1xnTX3dS37JaIM042/uLYzb65/dUfOm7oxBTDW0+7Jdu92nj5bAChYloKQGEsr7dwKxeg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-arn-parser\": \"3.804.0\", \"@smithy/core\": \"^3.7.2\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/signature-v4\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-config-provider\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-stream\": \"^4.2.3\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-ssec\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-ssec/-/middleware-ssec-3.840.0.tgz\", \"integrity\": \"sha512-CBZP9t1QbjDFGOrtnUEHL1oAvmnCUUm7p0aPNbIdSzNtH42TNKjPRN3TuEIJDGjkrqpL3MXyDSmNayDcw/XW7Q==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/middleware-user-agent\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/middleware-user-agent/-/middleware-user-agent-3.857.0.tgz\", \"integrity\": \"sha512-JPqTxJGwc5QyxpCpDuOi64+z+9krpkv9FidnWjPqqNwLy25Da8espksTzptPivsMzUukdObFWJsDG89/8/I6TQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-endpoints\": \"3.848.0\", \"@smithy/core\": \"^3.7.2\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/nested-clients\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/nested-clients/-/nested-clients-3.857.0.tgz\", \"integrity\": \"sha512-3P1GP34hu3Yb7C8bcIqIGASMt/MT/1Lxwy37UJwCn4IrccrvYM3i8y5XX4wW8sn1J5832wB4kdb4HTYbEz6+zw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/sha256-browser\": \"5.2.0\", \"@aws-crypto/sha256-js\": \"5.2.0\", \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/middleware-host-header\": \"3.840.0\", \"@aws-sdk/middleware-logger\": \"3.840.0\", \"@aws-sdk/middleware-recursion-detection\": \"3.840.0\", \"@aws-sdk/middleware-user-agent\": \"3.857.0\", \"@aws-sdk/region-config-resolver\": \"3.840.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-endpoints\": \"3.848.0\", \"@aws-sdk/util-user-agent-browser\": \"3.840.0\", \"@aws-sdk/util-user-agent-node\": \"3.857.0\", \"@smithy/config-resolver\": \"^4.1.4\", \"@smithy/core\": \"^3.7.2\", \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/hash-node\": \"^4.0.4\", \"@smithy/invalid-dependency\": \"^4.0.4\", \"@smithy/middleware-content-length\": \"^4.0.4\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/middleware-retry\": \"^4.1.18\", \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/middleware-stack\": \"^4.0.4\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-body-length-node\": \"^4.0.0\", \"@smithy/util-defaults-mode-browser\": \"^4.0.25\", \"@smithy/util-defaults-mode-node\": \"^4.0.25\", \"@smithy/util-endpoints\": \"^3.0.6\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-retry\": \"^4.0.6\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/region-config-resolver\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/region-config-resolver/-/region-config-resolver-3.840.0.tgz\", \"integrity\": \"sha512-Qjnxd/yDv9KpIMWr90ZDPtRj0v75AqGC92Lm9+oHXZ8p1MjG5JE2CW0HL8JRgK9iKzgKBL7pPQRXI8FkvEVfrA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-config-provider\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/s3-request-presigner\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/s3-request-presigner/-/s3-request-presigner-3.857.0.tgz\", \"integrity\": \"sha512-ysBzl3mMH68XGArmfaIjx88fJRaeA1jBzzRoX/3VKh0I4a8gXtRqWgttTm9YS/tidfFN5qfHeQgc286VMOVFqg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/signature-v4-multi-region\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@aws-sdk/util-format-url\": \"3.840.0\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/signature-v4-multi-region\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/signature-v4-multi-region/-/signature-v4-multi-region-3.857.0.tgz\", \"integrity\": \"sha512-KVpHAtRjv4oNydwXwAEf2ur4BOAWjjZiT/QtLtTKYbEbnXW1eOFZg4kWwJwHa/T/w2zfPMVf6LhZvyFwLU9XGg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/middleware-sdk-s3\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/signature-v4\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/token-providers\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/token-providers/-/token-providers-3.857.0.tgz\", \"integrity\": \"sha512-4DBZw+QHpsnpYLXzQtDYCEP9KFFQlYAmNnrCK1bsWoKqnUgjKgwr9Re0yhtNiieHhEE4Lhu+E+IAiNwDx2ClVw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/core\": \"3.857.0\", \"@aws-sdk/nested-clients\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/types\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/types/-/types-3.840.0.tgz\", \"integrity\": \"sha512-xliuHaUFZxEx1NSXeLLZ9Dyu6+EJVQKEoD+yM+zqUo3YDZ7medKJWY6fIOKiPX/N7XbLdBYwajb15Q7IL8KkeA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/util-arn-parser\": { \"version\": \"3.804.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-arn-parser/-/util-arn-parser-3.804.0.tgz\", \"integrity\": \"sha512-wmBJqn1DRXnZu3b4EkE6CWnoWMo1ZMvlfkqU5zPz67xx1GMaXlDCchFvKAXMjk4jn/L1O3tKnoFDNsoLV1kgNQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/util-dynamodb\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-dynamodb/-/util-dynamodb-3.857.0.tgz\", \"integrity\": \"sha512-j7JOyrHtzYqQQxCixiR6OS+hBBLLKAshPgwOjpmGEEUZhiEnT7cENYu9S/HyEfCh1WkfOUYS+3ApgJv1J65+3w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" }, \"peerDependencies\": { \"@aws-sdk/client-dynamodb\": \"^3.857.0\" } }, \"node_modules/@aws-sdk/util-endpoints\": { \"version\": \"3.848.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-endpoints/-/util-endpoints-3.848.0.tgz\", \"integrity\": \"sha512-fY/NuFFCq/78liHvRyFKr+aqq1aA/uuVSANjzr5Ym8c+9Z3HRPE9OrExAHoMrZ6zC8tHerQwlsXYYH5XZ7H+ww==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-endpoints\": \"^3.0.6\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/util-format-url\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-format-url/-/util-format-url-3.840.0.tgz\", \"integrity\": \"sha512-VB1PWyI1TQPiPvg4w7tgUGGQER1xxXPNUqfh3baxUSFi1Oh8wHrDnFywkxLm3NMmgDmnLnSZ5Q326qAoyqKLSg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/querystring-builder\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/util-locate-window\": { \"version\": \"3.804.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-locate-window/-/util-locate-window-3.804.0.tgz\", \"integrity\": \"sha512-zVoRfpmBVPodYlnMjgVjfGoEZagyRF5IPn3Uo6ZvOZp24chnW/FRstH7ESDHDDRga4z3V+ElUQHKpFDXWyBW5A==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@aws-sdk/util-user-agent-browser\": { \"version\": \"3.840.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-user-agent-browser/-/util-user-agent-browser-3.840.0.tgz\", \"integrity\": \"sha512-JdyZM3EhhL4PqwFpttZu1afDpPJCCc3eyZOLi+srpX11LsGj6sThf47TYQN75HT1CarZ7cCdQHGzP2uy3/xHfQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/types\": \"3.840.0\", \"@smithy/types\": \"^4.3.1\", \"bowser\": \"^2.11.0\", \"tslib\": \"^2.6.2\" } }, \"node_modules/@aws-sdk/util-user-agent-node\": { \"version\": \"3.857.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/util-user-agent-node/-/util-user-agent-node-3.857.0.tgz\", \"integrity\": \"sha512-xWNfAnD2t5yACGW1wM3iLoy2FvRM8N/XjkjgJE1O35gBHn00evtLC9q4nkR4x7+vXdZb8cVw4Y6GmcfMckgFQg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-sdk/middleware-user-agent\": \"3.857.0\", \"@aws-sdk/types\": \"3.840.0\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" }, \"peerDependencies\": { \"aws-crt\": \">=1.0.0\" }, \"peerDependenciesMeta\": { \"aws-crt\": { \"optional\": true } } }, \"node_modules/@aws-sdk/xml-builder\": { \"version\": \"3.821.0\", \"resolved\": \"https://registry.npmjs.org/@aws-sdk/xml-builder/-/xml-builder-3.821.0.tgz\", \"integrity\": \"sha512-DIIotRnefVL6DiaHtO6/21DhJ4JZnnIwdNbpwiAhdt/AVbttcE4yw925gsjur0OGv5BTYXQXU3YnANBYnZjuQA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/abort-controller\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/abort-controller/-/abort-controller-4.0.4.tgz\", \"integrity\": \"sha512-gJnEjZMvigPDQWHrW3oPrFhQtkrgqBkyjj3pCIdF3A5M6vsZODG93KNlfJprv6bp4245bdT32fsHK4kkH3KYDA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/chunked-blob-reader\": { \"version\": \"5.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/chunked-blob-reader/-/chunked-blob-reader-5.0.0.tgz\", \"integrity\": \"sha512-+sKqDBQqb036hh4NPaUiEkYFkTUGYzRsn3EuFhyfQfMy6oGHEUJDurLP9Ufb5dasr/XiAmPNMr6wa9afjQB+Gw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/chunked-blob-reader-native\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/chunked-blob-reader-native/-/chunked-blob-reader-native-4.0.0.tgz\", \"integrity\": \"sha512-R9wM2yPmfEMsUmlMlIgSzOyICs0x9uu7UTHoccMyt7BWw8shcGM8HqB355+BZCPBcySvbTYMs62EgEQkNxz2ig==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-base64\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/config-resolver\": { \"version\": \"4.1.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/config-resolver/-/config-resolver-4.1.4.tgz\", \"integrity\": \"sha512-prmU+rDddxHOH0oNcwemL+SwnzcG65sBF2yXRO7aeXIn/xTlq2pX7JLVbkBnVLowHLg4/OL4+jBmv9hVrVGS+w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-config-provider\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/core\": { \"version\": \"3.7.2\", \"resolved\": \"https://registry.npmjs.org/@smithy/core/-/core-3.7.2.tgz\", \"integrity\": \"sha512-JoLw59sT5Bm8SAjFCYZyuCGxK8y3vovmoVbZWLDPTH5XpPEIwpFd9m90jjVMwoypDuB/SdVgje5Y4T7w50lJaw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-body-length-browser\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-stream\": \"^4.2.3\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/credential-provider-imds\": { \"version\": \"4.0.6\", \"resolved\": \"https://registry.npmjs.org/@smithy/credential-provider-imds/-/credential-provider-imds-4.0.6.tgz\", \"integrity\": \"sha512-hKMWcANhUiNbCJouYkZ9V3+/Qf9pteR1dnwgdyzR09R4ODEYx8BbUysHwRSyex4rZ9zapddZhLFTnT4ZijR4pw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/eventstream-codec\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/eventstream-codec/-/eventstream-codec-4.0.4.tgz\", \"integrity\": \"sha512-7XoWfZqWb/QoR/rAU4VSi0mWnO2vu9/ltS6JZ5ZSZv0eovLVfDfu0/AX4ub33RsJTOth3TiFWSHS5YdztvFnig==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@aws-crypto/crc32\": \"5.2.0\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-hex-encoding\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/eventstream-serde-browser\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/eventstream-serde-browser/-/eventstream-serde-browser-4.0.4.tgz\", \"integrity\": \"sha512-3fb/9SYaYqbpy/z/H3yIi0bYKyAa89y6xPmIqwr2vQiUT2St+avRt8UKwsWt9fEdEasc5d/V+QjrviRaX1JRFA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/eventstream-serde-universal\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/eventstream-serde-config-resolver\": { \"version\": \"4.1.2\", \"resolved\": \"https://registry.npmjs.org/@smithy/eventstream-serde-config-resolver/-/eventstream-serde-config-resolver-4.1.2.tgz\", \"integrity\": \"sha512-JGtambizrWP50xHgbzZI04IWU7LdI0nh/wGbqH3sJesYToMi2j/DcoElqyOcqEIG/D4tNyxgRuaqBXWE3zOFhQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/eventstream-serde-node\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/eventstream-serde-node/-/eventstream-serde-node-4.0.4.tgz\", \"integrity\": \"sha512-RD6UwNZ5zISpOWPuhVgRz60GkSIp0dy1fuZmj4RYmqLVRtejFqQ16WmfYDdoSoAjlp1LX+FnZo+/hkdmyyGZ1w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/eventstream-serde-universal\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/eventstream-serde-universal\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/eventstream-serde-universal/-/eventstream-serde-universal-4.0.4.tgz\", \"integrity\": \"sha512-UeJpOmLGhq1SLox79QWw/0n2PFX+oPRE1ZyRMxPIaFEfCqWaqpB7BU9C8kpPOGEhLF7AwEqfFbtwNxGy4ReENA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/eventstream-codec\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/fetch-http-handler\": { \"version\": \"5.1.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/fetch-http-handler/-/fetch-http-handler-5.1.0.tgz\", \"integrity\": \"sha512-mADw7MS0bYe2OGKkHYMaqarOXuDwRbO6ArD91XhHcl2ynjGCFF+hvqf0LyQcYxkA1zaWjefSkU7Ne9mqgApSgQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/querystring-builder\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-base64\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/hash-blob-browser\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/hash-blob-browser/-/hash-blob-browser-4.0.4.tgz\", \"integrity\": \"sha512-WszRiACJiQV3QG6XMV44i5YWlkrlsM5Yxgz4jvsksuu7LDXA6wAtypfPajtNTadzpJy3KyJPoWehYpmZGKUFIQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/chunked-blob-reader\": \"^5.0.0\", \"@smithy/chunked-blob-reader-native\": \"^4.0.0\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/hash-node\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/hash-node/-/hash-node-4.0.4.tgz\", \"integrity\": \"sha512-qnbTPUhCVnCgBp4z4BUJUhOEkVwxiEi1cyFM+Zj6o+aY8OFGxUQleKWq8ltgp3dujuhXojIvJWdoqpm6dVO3lQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"@smithy/util-buffer-from\": \"^4.0.0\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/hash-stream-node\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/hash-stream-node/-/hash-stream-node-4.0.4.tgz\", \"integrity\": \"sha512-wHo0d8GXyVmpmMh/qOR0R7Y46/G1y6OR8U+bSTB4ppEzRxd1xVAQ9xOE9hOc0bSjhz0ujCPAbfNLkLrpa6cevg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/invalid-dependency\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/invalid-dependency/-/invalid-dependency-4.0.4.tgz\", \"integrity\": \"sha512-bNYMi7WKTJHu0gn26wg8OscncTt1t2b8KcsZxvOv56XA6cyXtOAAAaNP7+m45xfppXfOatXF3Sb1MNsLUgVLTw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/is-array-buffer\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/is-array-buffer/-/is-array-buffer-4.0.0.tgz\", \"integrity\": \"sha512-saYhF8ZZNoJDTvJBEWgeBccCg+yvp1CX+ed12yORU3NilJScfc6gfch2oVb4QgxZrGUx3/ZJlb+c/dJbyupxlw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/md5-js\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/md5-js/-/md5-js-4.0.4.tgz\", \"integrity\": \"sha512-uGLBVqcOwrLvGh/v/jw423yWHq/ofUGK1W31M2TNspLQbUV1Va0F5kTxtirkoHawODAZcjXTSGi7JwbnPcDPJg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/middleware-content-length\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/middleware-content-length/-/middleware-content-length-4.0.4.tgz\", \"integrity\": \"sha512-F7gDyfI2BB1Kc+4M6rpuOLne5LOcEknH1n6UQB69qv+HucXBR1rkzXBnQTB2q46sFy1PM/zuSJOB532yc8bg3w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/middleware-endpoint\": { \"version\": \"4.1.17\", \"resolved\": \"https://registry.npmjs.org/@smithy/middleware-endpoint/-/middleware-endpoint-4.1.17.tgz\", \"integrity\": \"sha512-S3hSGLKmHG1m35p/MObQCBCdRsrpbPU8B129BVzRqRfDvQqPMQ14iO4LyRw+7LNizYc605COYAcjqgawqi+6jA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/core\": \"^3.7.2\", \"@smithy/middleware-serde\": \"^4.0.8\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"@smithy/url-parser\": \"^4.0.4\", \"@smithy/util-middleware\": \"^4.0.4\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/middleware-retry\": { \"version\": \"4.1.18\", \"resolved\": \"https://registry.npmjs.org/@smithy/middleware-retry/-/middleware-retry-4.1.18.tgz\", \"integrity\": \"sha512-bYLZ4DkoxSsPxpdmeapvAKy7rM5+25gR7PGxq2iMiecmbrRGBHj9s75N74Ylg+aBiw9i5jIowC/cLU2NR0qH8w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/service-error-classification\": \"^4.0.6\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-retry\": \"^4.0.6\", \"tslib\": \"^2.6.2\", \"uuid\": \"^9.0.1\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/middleware-retry/node_modules/uuid\": { \"version\": \"9.0.1\", \"resolved\": \"https://registry.npmjs.org/uuid/-/uuid-9.0.1.tgz\", \"integrity\": \"sha512-b+1eJOlsR9K8HJpow9Ok3fiWOWSIcIzXodvv0rQjVoOVNpWMpxf1wZNpt4y9h10odCNrqnYp1OBzRktckBe3sA==\", \"funding\": [ \"https://github.com/sponsors/broofa\", \"https://github.com/sponsors/ctavan\" ], \"license\": \"MIT\", \"bin\": { \"uuid\": \"dist/bin/uuid\" } }, \"node_modules/@smithy/middleware-serde\": { \"version\": \"4.0.8\", \"resolved\": \"https://registry.npmjs.org/@smithy/middleware-serde/-/middleware-serde-4.0.8.tgz\", \"integrity\": \"sha512-iSSl7HJoJaGyMIoNn2B7czghOVwJ9nD7TMvLhMWeSB5vt0TnEYyRRqPJu/TqW76WScaNvYYB8nRoiBHR9S1Ddw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/middleware-stack\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/middleware-stack/-/middleware-stack-4.0.4.tgz\", \"integrity\": \"sha512-kagK5ggDrBUCCzI93ft6DjteNSfY8Ulr83UtySog/h09lTIOAJ/xUSObutanlPT0nhoHAkpmW9V5K8oPyLh+QA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/node-config-provider\": { \"version\": \"4.1.3\", \"resolved\": \"https://registry.npmjs.org/@smithy/node-config-provider/-/node-config-provider-4.1.3.tgz\", \"integrity\": \"sha512-HGHQr2s59qaU1lrVH6MbLlmOBxadtzTsoO4c+bF5asdgVik3I8o7JIOzoeqWc5MjVa+vD36/LWE0iXKpNqooRw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/shared-ini-file-loader\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/node-http-handler\": { \"version\": \"4.1.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/node-http-handler/-/node-http-handler-4.1.0.tgz\", \"integrity\": \"sha512-vqfSiHz2v8b3TTTrdXi03vNz1KLYYS3bhHCDv36FYDqxT7jvTll1mMnCrkD+gOvgwybuunh/2VmvOMqwBegxEg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/abort-controller\": \"^4.0.4\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/querystring-builder\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/property-provider\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/property-provider/-/property-provider-4.0.4.tgz\", \"integrity\": \"sha512-qHJ2sSgu4FqF4U/5UUp4DhXNmdTrgmoAai6oQiM+c5RZ/sbDwJ12qxB1M6FnP+Tn/ggkPZf9ccn4jqKSINaquw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/protocol-http\": { \"version\": \"5.1.2\", \"resolved\": \"https://registry.npmjs.org/@smithy/protocol-http/-/protocol-http-5.1.2.tgz\", \"integrity\": \"sha512-rOG5cNLBXovxIrICSBm95dLqzfvxjEmuZx4KK3hWwPFHGdW3lxY0fZNXfv2zebfRO7sJZ5pKJYHScsqopeIWtQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/querystring-builder\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/querystring-builder/-/querystring-builder-4.0.4.tgz\", \"integrity\": \"sha512-SwREZcDnEYoh9tLNgMbpop+UTGq44Hl9tdj3rf+yeLcfH7+J8OXEBaMc2kDxtyRHu8BhSg9ADEx0gFHvpJgU8w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"@smithy/util-uri-escape\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/querystring-parser\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/querystring-parser/-/querystring-parser-4.0.4.tgz\", \"integrity\": \"sha512-6yZf53i/qB8gRHH/l2ZwUG5xgkPgQF15/KxH0DdXMDHjesA9MeZje/853ifkSY0x4m5S+dfDZ+c4x439PF0M2w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/service-error-classification\": { \"version\": \"4.0.6\", \"resolved\": \"https://registry.npmjs.org/@smithy/service-error-classification/-/service-error-classification-4.0.6.tgz\", \"integrity\": \"sha512-RRoTDL//7xi4tn5FrN2NzH17jbgmnKidUqd4KvquT0954/i6CXXkh1884jBiunq24g9cGtPBEXlU40W6EpNOOg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/shared-ini-file-loader\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/shared-ini-file-loader/-/shared-ini-file-loader-4.0.4.tgz\", \"integrity\": \"sha512-63X0260LoFBjrHifPDs+nM9tV0VMkOTl4JRMYNuKh/f5PauSjowTfvF3LogfkWdcPoxsA9UjqEOgjeYIbhb7Nw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/signature-v4\": { \"version\": \"5.1.2\", \"resolved\": \"https://registry.npmjs.org/@smithy/signature-v4/-/signature-v4-5.1.2.tgz\", \"integrity\": \"sha512-d3+U/VpX7a60seHziWnVZOHuEgJlclufjkS6zhXvxcJgkJq4UWdH5eOBLzHRMx6gXjsdT9h6lfpmLzbrdupHgQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/is-array-buffer\": \"^4.0.0\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-hex-encoding\": \"^4.0.0\", \"@smithy/util-middleware\": \"^4.0.4\", \"@smithy/util-uri-escape\": \"^4.0.0\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/smithy-client\": { \"version\": \"4.4.9\", \"resolved\": \"https://registry.npmjs.org/@smithy/smithy-client/-/smithy-client-4.4.9.tgz\", \"integrity\": \"sha512-mbMg8mIUAWwMmb74LoYiArP04zWElPzDoA1jVOp3or0cjlDMgoS6WTC3QXK0Vxoc9I4zdrX0tq6qsOmaIoTWEQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/core\": \"^3.7.2\", \"@smithy/middleware-endpoint\": \"^4.1.17\", \"@smithy/middleware-stack\": \"^4.0.4\", \"@smithy/protocol-http\": \"^5.1.2\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-stream\": \"^4.2.3\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/types\": { \"version\": \"4.3.1\", \"resolved\": \"https://registry.npmjs.org/@smithy/types/-/types-4.3.1.tgz\", \"integrity\": \"sha512-UqKOQBL2x6+HWl3P+3QqFD4ncKq0I8Nuz9QItGv5WuKuMHuuwlhvqcZCoXGfc+P1QmfJE7VieykoYYmrOoFJxA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/url-parser\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/url-parser/-/url-parser-4.0.4.tgz\", \"integrity\": \"sha512-eMkc144MuN7B0TDA4U2fKs+BqczVbk3W+qIvcoCY6D1JY3hnAdCuhCZODC+GAeaxj0p6Jroz4+XMUn3PCxQQeQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/querystring-parser\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-base64\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-base64/-/util-base64-4.0.0.tgz\", \"integrity\": \"sha512-CvHfCmO2mchox9kjrtzoHkWHxjHZzaFojLc8quxXY7WAAMAg43nuxwv95tATVgQFNDwd4M9S1qFzj40Ul41Kmg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-buffer-from\": \"^4.0.0\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-body-length-browser\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-body-length-browser/-/util-body-length-browser-4.0.0.tgz\", \"integrity\": \"sha512-sNi3DL0/k64/LO3A256M+m3CDdG6V7WKWHdAiBBMUN8S3hK3aMPhwnPik2A/a2ONN+9doY9UxaLfgqsIRg69QA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-body-length-node\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-body-length-node/-/util-body-length-node-4.0.0.tgz\", \"integrity\": \"sha512-q0iDP3VsZzqJyje8xJWEJCNIu3lktUGVoSy1KB0UWym2CL1siV3artm+u1DFYTLejpsrdGyCSWBdGNjJzfDPjg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-buffer-from\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-buffer-from/-/util-buffer-from-4.0.0.tgz\", \"integrity\": \"sha512-9TOQ7781sZvddgO8nxueKi3+yGvkY35kotA0Y6BWRajAv8jjmigQ1sBwz0UX47pQMYXJPahSKEKYFgt+rXdcug==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/is-array-buffer\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-config-provider\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-config-provider/-/util-config-provider-4.0.0.tgz\", \"integrity\": \"sha512-L1RBVzLyfE8OXH+1hsJ8p+acNUSirQnWQ6/EgpchV88G6zGBTDPdXiiExei6Z1wR2RxYvxY/XLw6AMNCCt8H3w==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-defaults-mode-browser\": { \"version\": \"4.0.25\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-defaults-mode-browser/-/util-defaults-mode-browser-4.0.25.tgz\", \"integrity\": \"sha512-pxEWsxIsOPLfKNXvpgFHBGFC3pKYKUFhrud1kyooO9CJai6aaKDHfT10Mi5iiipPXN/JhKAu3qX9o75+X85OdQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"bowser\": \"^2.11.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-defaults-mode-node\": { \"version\": \"4.0.25\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-defaults-mode-node/-/util-defaults-mode-node-4.0.25.tgz\", \"integrity\": \"sha512-+w4n4hKFayeCyELZLfsSQG5mCC3TwSkmRHv4+el5CzFU8ToQpYGhpV7mrRzqlwKkntlPilT1HJy1TVeEvEjWOQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/config-resolver\": \"^4.1.4\", \"@smithy/credential-provider-imds\": \"^4.0.6\", \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/property-provider\": \"^4.0.4\", \"@smithy/smithy-client\": \"^4.4.9\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-endpoints\": { \"version\": \"3.0.6\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-endpoints/-/util-endpoints-3.0.6.tgz\", \"integrity\": \"sha512-YARl3tFL3WgPuLzljRUnrS2ngLiUtkwhQtj8PAL13XZSyUiNLQxwG3fBBq3QXFqGFUXepIN73pINp3y8c2nBmA==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/node-config-provider\": \"^4.1.3\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-hex-encoding\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-hex-encoding/-/util-hex-encoding-4.0.0.tgz\", \"integrity\": \"sha512-Yk5mLhHtfIgW2W2WQZWSg5kuMZCVbvhFmC7rV4IO2QqnZdbEFPmQnCcGMAX2z/8Qj3B9hYYNjZOhWym+RwhePw==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-middleware\": { \"version\": \"4.0.4\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-middleware/-/util-middleware-4.0.4.tgz\", \"integrity\": \"sha512-9MLKmkBmf4PRb0ONJikCbCwORACcil6gUWojwARCClT7RmLzF04hUR4WdRprIXal7XVyrddadYNfp2eF3nrvtQ==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-retry\": { \"version\": \"4.0.6\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-retry/-/util-retry-4.0.6.tgz\", \"integrity\": \"sha512-+YekoF2CaSMv6zKrA6iI/N9yva3Gzn4L6n35Luydweu5MMPYpiGZlWqehPHDHyNbnyaYlz/WJyYAZnC+loBDZg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/service-error-classification\": \"^4.0.6\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-stream\": { \"version\": \"4.2.3\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-stream/-/util-stream-4.2.3.tgz\", \"integrity\": \"sha512-cQn412DWHHFNKrQfbHY8vSFI3nTROY1aIKji9N0tpp8gUABRilr7wdf8fqBbSlXresobM+tQFNk6I+0LXK/YZg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/fetch-http-handler\": \"^5.1.0\", \"@smithy/node-http-handler\": \"^4.1.0\", \"@smithy/types\": \"^4.3.1\", \"@smithy/util-base64\": \"^4.0.0\", \"@smithy/util-buffer-from\": \"^4.0.0\", \"@smithy/util-hex-encoding\": \"^4.0.0\", \"@smithy/util-utf8\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-uri-escape\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-uri-escape/-/util-uri-escape-4.0.0.tgz\", \"integrity\": \"sha512-77yfbCbQMtgtTylO9itEAdpPXSog3ZxMe09AEhm0dU0NLTalV70ghDZFR+Nfi1C60jnJoh/Re4090/DuZh2Omg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-utf8\": { \"version\": \"4.0.0\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-utf8/-/util-utf8-4.0.0.tgz\", \"integrity\": \"sha512-b+zebfKCfRdgNJDknHCob3O7FpeYQN6ZG6YLExMcasDHsCXlsXCEuiPZeLnJLpwa5dvPetGlnGCiMHuLwGvFow==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/util-buffer-from\": \"^4.0.0\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@smithy/util-waiter\": { \"version\": \"4.0.6\", \"resolved\": \"https://registry.npmjs.org/@smithy/util-waiter/-/util-waiter-4.0.6.tgz\", \"integrity\": \"sha512-slcr1wdRbX7NFphXZOxtxRNA7hXAAtJAXJDE/wdoMAos27SIquVCKiSqfB6/28YzQ8FCsB5NKkhdM5gMADbqxg==\", \"license\": \"Apache-2.0\", \"dependencies\": { \"@smithy/abort-controller\": \"^4.0.4\", \"@smithy/types\": \"^4.3.1\", \"tslib\": \"^2.6.2\" }, \"engines\": { \"node\": \">=18.0.0\" } }, \"node_modules/@types/uuid\": { \"version\": \"9.0.8\", \"resolved\": \"https://registry.npmjs.org/@types/uuid/-/uuid-9.0.8.tgz\", \"integrity\": \"sha512-jg+97EGIcY9AGHJJRaaPVgetKDsrTgbRjQ5Msgjh/DQKEFl0DtyRr/VCOyD1T2R1MNeWPK/u7JoGhlDZnKBAfA==\", \"license\": \"MIT\" }, \"node_modules/bowser\": { \"version\": \"2.11.0\", \"resolved\": \"https://registry.npmjs.org/bowser/-/bowser-2.11.0.tgz\", \"integrity\": \"sha512-AlcaJBi/pqqJBIQ8U9Mcpc9i8Aqxn88Skv5d+xBX006BY5u8N3mGLHa5Lgppa7L/HfwgwLgZ6NYs+Ag6uUmJRA==\", \"license\": \"MIT\" }, \"node_modules/fast-xml-parser\": { \"version\": \"5.2.5\", \"resolved\": \"https://registry.npmjs.org/fast-xml-parser/-/fast-xml-parser-5.2.5.tgz\", \"integrity\": \"sha512-pfX9uG9Ki0yekDHx2SiuRIyFdyAr1kMIMitPvb0YBo8SUfKvia7w7FIyd/l6av85pFYRhZscS75MwMnbvY+hcQ==\", \"funding\": [ { \"type\": \"github\", \"url\": \"https://github.com/sponsors/NaturalIntelligence\" } ], \"license\": \"MIT\", \"dependencies\": { \"strnum\": \"^2.1.0\" }, \"bin\": { \"fxparser\": \"src/cli/cli.js\" } }, \"node_modules/handlebars\": { \"version\": \"4.7.8\", \"resolved\": \"https://registry.npmjs.org/handlebars/-/handlebars-4.7.8.tgz\", \"integrity\": \"sha512-vafaFqs8MZkRrSX7sFVUdo3ap/eNiLnb4IakshzvP56X5Nr1iGKAIqdX6tMlm6HcNRIkr6AxO5jFEoJzzpT8aQ==\", \"license\": \"MIT\", \"dependencies\": { \"minimist\": \"^1.2.5\", \"neo-async\": \"^2.6.2\", \"source-map\": \"^0.6.1\", \"wordwrap\": \"^1.0.0\" }, \"bin\": { \"handlebars\": \"bin/handlebars\" }, \"engines\": { \"node\": \">=0.4.7\" }, \"optionalDependencies\": { \"uglify-js\": \"^3.1.4\" } }, \"node_modules/minimist\": { \"version\": \"1.2.8\", \"resolved\": \"https://registry.npmjs.org/minimist/-/minimist-1.2.8.tgz\", \"integrity\": \"sha512-2yyAR8qBkN3YuheJanUpWC5U3bb5osDywNB8RzDVlDwDHbocAJveqqj1u8+SVD7jkWT4yvsHCpWqqWqAxb0zCA==\", \"license\": \"MIT\", \"funding\": { \"url\": \"https://github.com/sponsors/ljharb\" } }, \"node_modules/mnemonist\": { \"version\": \"0.38.3\", \"resolved\": \"https://registry.npmjs.org/mnemonist/-/mnemonist-0.38.3.tgz\", \"integrity\": \"sha512-2K9QYubXx/NAjv4VLq1d1Ly8pWNC5L3BrixtdkyTegXWJIqY+zLNDhhX/A+ZwWt70tB1S8H4BE8FLYEFyNoOBw==\", \"license\": \"MIT\", \"dependencies\": { \"obliterator\": \"^1.6.1\" } }, \"node_modules/neo-async\": { \"version\": \"2.6.2\", \"resolved\": \"https://registry.npmjs.org/neo-async/-/neo-async-2.6.2.tgz\", \"integrity\": \"sha512-Yd3UES5mWCSqR+qNT93S3UoYUkqAZ9lLg8a7g9rimsWmYGK8cVToA4/sF3RrshdyV3sAGMXVUmpMYOw+dLpOuw==\", \"license\": \"MIT\" }, \"node_modules/obliterator\": { \"version\": \"1.6.1\", \"resolved\": \"https://registry.npmjs.org/obliterator/-/obliterator-1.6.1.tgz\", \"integrity\": \"sha512-9WXswnqINnnhOG/5SLimUlzuU1hFJUc8zkwyD59Sd+dPOMf05PmnYG/d6Q7HZ+KmgkZJa1PxRso6QdM3sTNHig==\", \"license\": \"MIT\" }, \"node_modules/source-map\": { \"version\": \"0.6.1\", \"resolved\": \"https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz\", \"integrity\": \"sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==\", \"license\": \"BSD-3-Clause\", \"engines\": { \"node\": \">=0.10.0\" } }, \"node_modules/strnum\": { \"version\": \"2.1.1\", \"resolved\": \"https://registry.npmjs.org/strnum/-/strnum-2.1.1.tgz\", \"integrity\": \"sha512-7ZvoFTiCnGxBtDqJ//Cu6fWtZtc7Y3x+QOirG15wztbdngGSkht27o2pyGWrVy0b4WAy3jbKmnoK6g5VlVNUUw==\", \"funding\": [ { \"type\": \"github\", \"url\": \"https://github.com/sponsors/NaturalIntelligence\" } ], \"license\": \"MIT\" }, \"node_modules/tslib\": { \"version\": \"2.8.1\", \"resolved\": \"https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz\", \"integrity\": \"sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==\", \"license\": \"0BSD\" }, \"node_modules/uglify-js\": { \"version\": \"3.19.3\", \"resolved\": \"https://registry.npmjs.org/uglify-js/-/uglify-js-3.19.3.tgz\", \"integrity\": \"sha512-v3Xu+yuwBXisp6QYTcH4UbH+xYJXqnq2m/LtQVWKWzYc1iehYnLixoQDN9FH6/j9/oybfd6W9Ghwkl8+UMKTKQ==\", \"license\": \"BSD-2-Clause\", \"optional\": true, \"bin\": { \"uglifyjs\": \"bin/uglifyjs\" }, \"engines\": { \"node\": \">=0.8.0\" } }, \"node_modules/wordwrap\": { \"version\": \"1.0.0\", \"resolved\": \"https://registry.npmjs.org/wordwrap/-/wordwrap-1.0.0.tgz\", \"integrity\": \"sha512-gvVzJFlPycKc5dZN4yPkP8w7Dc37BtP1yczEneOb4uq34pXZcvrtRTmWV8W+Ume+XCxKgbjM+nevkyFPMybd4Q==\", \"license\": \"MIT\" } }} You should end up with an app/best-cat directory that looks like this: Directoryapp Directorybest-cat index.jspackage-lock.jsonpackage.jsonscript.jsstyles.csstemplate.html Packaging the appSection titled “Packaging the app” Once you have the app stored in app/best-cat, you’ll want to create the dist directory, then package the application for delivery to a lambda function. mkdir distcd app/best-catnpm inpm run package I also recommend adding the following .gitignore file to your dist directory so you don’t accidentally commit any other content in this directory to your repository: *!.gitignore Generating assetsSection titled “Generating assets” You’ll also want some assets to use in this project to make it more fun. I generated a bunch of cat pictures using Gemini, but feel free to use stock photos or something else to generate the assets. I would recommend that you place the images in the same location I did (dist/static), so that the convenience scripts I wrote work out of the box without modification. This is what my dist directory looks like after following these steps: Directorydist best-cat.zipDirectorystatic 01-cat.png02-cat.png03-cat.png04-cat.png05-cat.png06-cat.png07-cat.png08-cat.png09-cat.png10-cat.png Our end goal is to host a site that looks like this in AWS using these artifacts: Edit page Previous Overview Next Step 1: Starting the Terralith",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.254Z"
  },
  {
    "title": "Step 1: Starting the Terralith",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-1-starting-the-terralith/",
    "content": "Step 1: Starting the Terralith During this step we’re going to take the contents of the dist directory, and deploy the application in AWS using OpenTofu. The content that we’re going to create here is not intended to be a best-practices IaC setup, but rather what one might do if they were to naively put together the IaC for this application from scratch without knowledge of best practices, or planning for future modifications. This will be reiterated at the end of this step, but note that this statement is not intended to be judgemental. There are perfectly valid reasons for building an MVP quickly without worrying about architecting the most optimally scaling IaC setup from day 1, and everyone has to start somewhere in their IaC journey. As you progress through this guide, you’ll be exposed to the challenges that might arise if you design your IaC as prescribed at each step, and this guide will do its best to highlight those trade-offs. The goal is for you to be able to make judgements around the IaC design that is best suited to your infrastructure management. TutorialSection titled “Tutorial” To start, create the live directory where you’re going to be provisioning some infrastructure. Creating a dedicated live directory/repository separate from reusable infrastructure patterns is an established recommended best practice from Gruntwork that will become important in later iterations. mkdir live To get started, we’ll want to define our providers, which is how OpenTofu will actually effectuate infrastructure in AWS. provider \"aws\" { region = var.aws_region} Don’t worry about that var.aws_region bit there. We’ll define it later. When we do, we’ll be defining the region in which AWS will have resources provisioned. It’s best practice to tell OpenTofu what your version constraints are, so that your IaC is reliably reusable. You’ll pin version >= 1.10 of OpenTofu here, as you’re going to be using an OpenTofu 1.10+ feature later on, and you’ll pin version ~> 6.0 because any change that makes the resource definitions we define here break should be in a future major release of the aws provider. terraform { required_version = \">= 1.10\" required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 6.0\" } }} Now define the resources that are provisioned in this step. First, we’ll add the database that we use to provision resources in this project. For this guide, we’ll be using DynamoDB, which is a fast, NoSQL database offered by AWS. The primary construct in DynamoDB is a table. All DynamoDB tables have a name, and a primary key (also called the hash key). This is what we’ll use to uniquely reference items in the table. In this project, DynamoDB tables will be used to store the metadata about cats and the number of votes they’ve gotten as being the best cat. resource \"aws_dynamodb_table\" \"asset_metadata\" { name = \"${var.name}-asset-metadata\" billing_mode = \"PAY_PER_REQUEST\" hash_key = \"image_id\" attribute { name = \"image_id\" type = \"S\" } tags = { Name = \"${var.name}-asset-metadata\" }} Next, we’ll add our object store. For this guide, we’ll be using S3, which is a cheap, scalable object store provided by AWS. The primary construct of S3 is a bucket. All buckets must have a name that is globally unique, so we’ll want to make sure that the value we select later on for name is appropriately unique. resource \"aws_s3_bucket\" \"static_assets\" { bucket = \"${var.name}-static-assets\" force_destroy = var.force_destroy} That force_destroy attribute there is important. It determines whether we can destroy the S3 bucket without getting rid of all its contents. You typically want this to be set to false in production environments, but it can be convenient to set this to true in test/ephemeral environments, where you expect the bucket to be short-lived. In addition to provisioning resources using OpenTofu, you can also lookup data using data configuration blocks. These can be useful ways to access frequently needed data in AWS resources, like AWS account IDs. data \"aws_caller_identity\" \"current\" {} Using that data block, let’s provision the IAM role that’s used for our Lambda function. We’ll want it to trust the Lambda service, so that the Lambda function is allowed to assume it, and have permissions to: Get and list the objects in S3 (our cat images). Interact with the DynamoDB table used for storing metadata on our assets (the votes for best cat). Basic permissions required to operate a Lambda function (the ability to log to CloudWatch Logs). resource \"aws_iam_role\" \"lambda_role\" { name = \"${var.name}-lambda-role\" assume_role_policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Action = \"sts:AssumeRole\" Effect = \"Allow\" Principal = { Service = \"lambda.amazonaws.com\" } } ] })} resource \"aws_iam_policy\" \"lambda_s3_read\" { name = \"${var.name}-lambda-s3-read\" description = \"Policy for Lambda to read from S3 bucket\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"s3:GetObject\", \"s3:ListBucket\" ] Resource = [ aws_s3_bucket.static_assets.arn, \"${aws_s3_bucket.static_assets.arn}/*\" ] } ] })} resource \"aws_iam_policy\" \"lambda_dynamodb\" { name = \"${var.name}-lambda-dynamodb\" description = \"Policy for Lambda to read/write to DynamoDB table\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"dynamodb:GetItem\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\", \"dynamodb:Query\", \"dynamodb:Scan\" ] Resource = aws_dynamodb_table.asset_metadata.arn } ] })} resource \"aws_iam_policy\" \"lambda_basic_execution\" { name = \"${var.name}-lambda-basic-execution\" description = \"Policy for Lambda basic execution (CloudWatch logs)\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\" ] Resource = \"arn:aws:logs:${var.aws_region}:${data.aws_caller_identity.current.account_id}:*\" } ] })} resource \"aws_iam_role_policy_attachment\" \"lambda_s3_read\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_s3_read.arn} resource \"aws_iam_role_policy_attachment\" \"lambda_dynamodb\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_dynamodb.arn} resource \"aws_iam_role_policy_attachment\" \"lambda_basic_execution\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_basic_execution.arn} The final resource you’re going to provision is the Lambda function. Lambda functions are a form of cheap, ephemeral compute that are especially useful for demo guides like this where you might forget to clean up some dummy resources. They won’t cost you anything while they’re not doing anything! resource \"aws_lambda_function\" \"main\" { function_name = \"${var.name}-function\" filename = var.lambda_zip_file source_code_hash = filebase64sha256(var.lambda_zip_file) role = aws_iam_role.lambda_role.arn handler = var.lambda_handler runtime = var.lambda_runtime timeout = var.lambda_timeout memory_size = var.lambda_memory_size architectures = var.lambda_architectures environment { variables = { S3_BUCKET_NAME = aws_s3_bucket.static_assets.bucket DYNAMODB_TABLE_NAME = aws_dynamodb_table.asset_metadata.name } } depends_on = [ aws_iam_role_policy_attachment.lambda_s3_read, aws_iam_role_policy_attachment.lambda_dynamodb, aws_iam_role_policy_attachment.lambda_basic_execution ]} resource \"aws_lambda_function_url\" \"main\" { function_name = aws_lambda_function.main.function_name authorization_type = \"NONE\"} Now let’s add some variables that we want to specify for this project. You can think of these as the inputs that you supply to your generically defined IaC to get infrastructure customized to your needs. As a matter of best practice, we’re going to separate the required variables from the optional ones. variable \"name\" { description = \"Name used for all resources\" type = string} variable \"lambda_zip_file\" { description = \"Path to the Lambda function zip file\" type = string} variable \"aws_region\" { description = \"AWS region for all resources\" type = string default = \"us-east-1\"} variable \"lambda_runtime\" { description = \"Lambda function runtime\" type = string default = \"nodejs22.x\"} variable \"lambda_handler\" { description = \"Lambda function handler\" type = string default = \"index.handler\"} variable \"lambda_timeout\" { description = \"Lambda function timeout in seconds\" type = number default = 30} variable \"lambda_memory_size\" { description = \"Lambda function memory size in MB\" type = number default = 128} variable \"lambda_architectures\" { description = \"Lambda function architectures\" type = list(string) default = [\"arm64\"]} variable \"force_destroy\" { description = \"Force destroy S3 buckets (only set to true for testing or cleanup of demo environments)\" type = bool default = false} Also add an .auto.tfvars file to define values for these variables automatically. This isn’t strictly required as you’ll be prompted for the values of required variables interactively if you don’t supply them here, but it does make your life easier. Note You’ll want to make sure that name is set to something globally unique, as it’ll be used as part of an S3 bucket name, which might conflict with a bucket created by somebody else otherwise (a simple way to decrease your odds of a collision is to use something like the date in your bucket name). # Required: Name used for all resources (must be unique)name = \"best-cat-2025-09-24-2359\" # Required: Path to your Lambda function zip filelambda_zip_file = \"../dist/best-cat.zip\" You’ll also want to add some outputs so that you can easily interact with the infrastructure you create. output \"lambda_function_url\" { description = \"URL of the Lambda function\" value = aws_lambda_function_url.main.function_url} output \"lambda_function_name\" { description = \"Name of the Lambda function\" value = aws_lambda_function.main.function_name} output \"s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = aws_s3_bucket.static_assets.bucket} output \"s3_bucket_arn\" { description = \"ARN of the S3 bucket for static assets\" value = aws_s3_bucket.static_assets.arn} output \"dynamodb_table_name\" { description = \"Name of the DynamoDB table for asset metadata\" value = aws_dynamodb_table.asset_metadata.name} output \"dynamodb_table_arn\" { description = \"ARN of the DynamoDB table for asset metadata\" value = aws_dynamodb_table.asset_metadata.arn} output \"lambda_role_arn\" { description = \"ARN of the Lambda execution role\" value = aws_iam_role.lambda_role.arn} As a best practice, you’ll want to add a backend configuration so that state isn’t stored locally. This is important if you want to collaborate with others in infrastructure management. Note that you’ll likely want to change the name of the bucket you use here, as it also has to be globally unique to avoid conflicts with anyone else. terraform { backend \"s3\" { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} Note We’re using the special new ✨ lockfile-based state locking ✨ made available in OpenTofu 1.10! This is especially convenient for this guide, as it saves us from provisioning an additional DynamoDB table to handle state locking. Unfortunately, OpenTofu will not provision this bucket for us automatically, so we will have to provision that manually. aws s3api create-bucket --bucket 'terragrunt-to-terralith-tfstate-2025-09-24-2359' --region 'us-east-1'aws s3api put-bucket-versioning --bucket 'terragrunt-to-terralith-tfstate-2025-09-24-2359' --versioning-configuration 'Status=Enabled' Project Layout Check-inSection titled “Project Layout Check-in” At this stage, you should have a live directory that looks like this: Directorylive backend.tfdata.tfddb.tfiam.tflambda.tfoutputs.tfproviders.tfs3.tfvars-optional.tfvars-required.tfversions.tf Applying UpdatesSection titled “Applying Updates” We can now apply our live infrastructure! cd livetofu inittofu apply You’ll receive a prompt to approve the apply (type yes then enter to continue). Make sure to review the plan thoroughly, then approve it. Assuming everything went well, you’ll see a bunch of outputs at the end of the apply. One of them will be an output that looks like the following: lambda_function_url = \"https://somerandomcharacters.lambda-url.us-east-1.on.aws/\" Copy that link, and paste it into your browser to see a page like the following: Congratulations! You’ve got live infrastructure you built yourself running in AWS! We can see an error that the site doesn’t have any images, and a prompt to upload some cat pictures to get started. To get those pictures uploaded, we’ll want to grab the name of the S3 bucket, and use the AWS CLI to upload the assets. # (Assuming you're using bash or zsh and you're in the `live` directory). # Grab the bucket name into the `bucket_name` variable.bucket_name=\"$(tofu output -raw s3_bucket_name)\" # Navigate to the root of the git repository.cd \"$(git rev-parse --show-toplevel)\" # Navigate to the directory where you stored your cat pictures.cd dist/static # Use the AWS CLI to sync the assets to the bucket.aws s3 sync . \"s3://${bucket_name}/\" If you reload the website, you should be able to see the cat images you uploaded. Wrap UpSection titled “Wrap Up” You’ve successfully built and deployed a complete, serverless web application using OpenTofu! All of your infrastructure including: An S3 bucket A DynamoDB table An IAM role A Lambda function Are defined and managed within a single root module. This configuration, could be called a “Terralith” or “Megamodule,” but it’s probably not obvious that there’s anything wrong with this setup. This a common and perfectly acceptable starting point for many projects. It’s simple and direct, but as you continue to adjust and refactor this project, its monolithic nature will present challenges in reusability and safe environment management as you scale. In the next step, you’ll begin to address these challenges by refactoring your code into reusable modules. Edit page Previous Setup Next Step 2: Refactoring",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.329Z"
  },
  {
    "title": "Step 2: Refactoring",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-2-refactoring/",
    "content": "Step 2: Refactoring One of the most important skills you can learn when managing IaC is learning how to refactor IaC. The code you write directly relates to infrastructure that delivers value to your team and wider organization, so knowing how to safely reorganize your code so that it’s easier to reuse and reason about without incurring risk for the infrastructure you support is invaluable. Some of the most common reasons you might engage in this kind of refactoring includes: Rewriting bespoke IaC as consumption of a reusable module so that you can repurpose the common IaC in other environments/projects. Standardizing IaC patterns for consistent application of security/cost best practices. Abstracting away the implementation details of one or more resources as a module so that you can focus on the higher level abstraction of how that module integrates with the rest of your infrastructure. Creating a generic module with a well defined API for a component in your infrastructure so that you can easily swap out the module with another module that shares a compatible (or close enough to compatible) API. In this step, we’ll start going down the road of making our infrastructure components modular so that we are well prepared for the next step, when we introduce a secondary environment as a replica of the environment we provisioned in the last step. TutorialSection titled “Tutorial” The Gruntwork recommended best practice for creating reusable IaC is to create a dedicated catalog directory (or a dedicated catalog repository) outside the live directory (or live repository) where reusable IaC patterns like OpenTofu/Terraform modules are stored. To reorganize the resources that we’ve created so far into reusable modules, we’ll create a directory called catalog/modules where we can store our modules for reusability. We’ll create an OpenTofu module for each piece of high-level functionality that we are provisioning in our current environment (s3, lambda, iam and ddb). mkdir -p catalog/modules/{s3, lambda, iam, ddb} Now we can move over the files that were provisioning these independent resources into their own modules so we can establish APIs for them and start reusing some of this code. It’s a pretty standard convention to name the core file used in a module main.tf. Good modules do one thing, and if you can’t figure out what a module does by the name of the module, it’s probably indicative that you’re making an odd abstraction. mv live/ddb.tf catalog/modules/ddb/main.tfmv live/iam.tf catalog/modules/iam/main.tfmv live/data.tf catalog/modules/iam/data.tfmv live/lambda.tf catalog/modules/lambda/main.tfmv live/s3.tf catalog/modules/s3/main.tf The contents of some of these files need a little massaging, however, as the IaC didn’t have clear boundaries between the constituent components. Let’s fix that by providing an API for each of these modules in the form of variables for inputs and outputs…. for outputs. variable \"name\" { description = \"The name of the DynamoDB table\" type = string} output \"name\" { value = aws_dynamodb_table.asset_metadata.name} output \"arn\" { value = aws_dynamodb_table.asset_metadata.arn} variable \"name\" { description = \"Name used for all resources\" type = string} variable \"force_destroy\" { description = \"Force destroy S3 buckets (only set to true for testing or cleanup of demo environments)\" type = bool default = false} output \"name\" { value = aws_s3_bucket.static_assets.bucket} output \"arn\" { value = aws_s3_bucket.static_assets.arn} variable \"name\" { description = \"The name of the IAM role\" type = string} variable \"aws_region\" { description = \"The AWS region to deploy the resources to\" type = string} variable \"s3_bucket_arn\" { description = \"The ARN of the S3 bucket\" type = string} variable \"dynamodb_table_arn\" { description = \"The ARN of the DynamoDB table\" type = string} output \"name\" { value = aws_iam_role.lambda_role.name} output \"arn\" { value = aws_iam_role.lambda_role.arn} For the iam module, we’re also going to need to make adjustments to the main.tf file to account for previous tight coupling between resources. The updates here take advantage of those new s3_bucket_arn and dynamodb_table_arn variables for message passing between modules, exposed by the outputs of the ddb and s3 modules. resource \"aws_iam_role\" \"lambda_role\" { name = \"${var.name}-lambda-role\" assume_role_policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Action = \"sts:AssumeRole\" Effect = \"Allow\" Principal = { Service = \"lambda.amazonaws.com\" } } ] })} resource \"aws_iam_policy\" \"lambda_s3_read\" { name = \"${var.name}-lambda-s3-read\" description = \"Policy for Lambda to read from S3 bucket\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"s3:GetObject\", \"s3:ListBucket\" ] Resource = [ var.s3_bucket_arn, \"${var.s3_bucket_arn}/*\" ] } ] })} resource \"aws_iam_policy\" \"lambda_dynamodb\" { name = \"${var.name}-lambda-dynamodb\" description = \"Policy for Lambda to read/write to DynamoDB table\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"dynamodb:GetItem\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\", \"dynamodb:Query\", \"dynamodb:Scan\" ] Resource = var.dynamodb_table_arn } ] })} resource \"aws_iam_policy\" \"lambda_basic_execution\" { name = \"${var.name}-lambda-basic-execution\" description = \"Policy for Lambda basic execution (CloudWatch logs)\" policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\" ] Resource = \"arn:aws:logs:${var.aws_region}:${data.aws_caller_identity.current.account_id}:*\" } ] })} resource \"aws_iam_role_policy_attachment\" \"lambda_s3_read\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_s3_read.arn} resource \"aws_iam_role_policy_attachment\" \"lambda_dynamodb\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_dynamodb.arn} resource \"aws_iam_role_policy_attachment\" \"lambda_basic_execution\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_basic_execution.arn} variable \"lambda_runtime\" { description = \"Lambda function runtime\" type = string default = \"nodejs22.x\"} variable \"lambda_handler\" { description = \"Lambda function handler\" type = string default = \"index.handler\"} variable \"lambda_timeout\" { description = \"Lambda function timeout in seconds\" type = number default = 30} variable \"lambda_memory_size\" { description = \"Lambda function memory size in MB\" type = number default = 128} variable \"lambda_architectures\" { description = \"Lambda function architectures\" type = list(string) default = [\"arm64\"]} variable \"name\" { description = \"Name used for all resources\" type = string} variable \"aws_region\" { description = \"AWS region to deploy the resources to\" type = string} variable \"lambda_zip_file\" { description = \"Path to the Lambda function zip file\" type = string} variable \"lambda_role_arn\" { description = \"Lambda function role ARN\" type = string} variable \"s3_bucket_name\" { description = \"S3 bucket name\" type = string} variable \"dynamodb_table_name\" { description = \"DynamoDB table name\" type = string} output \"name\" { value = aws_lambda_function.main.function_name} output \"arn\" { value = aws_lambda_function.main.arn} output \"url\" { value = aws_lambda_function_url.main.function_url} Again, for the Lambda module we’re going to need to make updates to the main.tf file to account for the tight coupling between resources now that we’re wiring them together via variables and outputs. resource \"aws_lambda_function\" \"main\" { function_name = \"${var.name}-function\" filename = var.lambda_zip_file source_code_hash = filebase64sha256(var.lambda_zip_file) role = var.lambda_role_arn handler = var.lambda_handler runtime = var.lambda_runtime timeout = var.lambda_timeout memory_size = var.lambda_memory_size architectures = var.lambda_architectures environment { variables = { S3_BUCKET_NAME = var.s3_bucket_name DYNAMODB_TABLE_NAME = var.dynamodb_table_name } }} resource \"aws_lambda_function_url\" \"main\" { function_name = aws_lambda_function.main.function_name authorization_type = \"NONE\"} Let’s make sure that our modules have a copy of the versions.tf file that was in the root module (if you’re not comfortable with using the find command below, you can just copy the versions.tf file into each of the modules you’ve created so far manually). It’s a best practice to have reusable modules define their version constraints so that they can explicitly signal to module consumers when they use features in newer provider versions that might require a provider upgrade or are dodging a bug in a particular version of a provider that consumers should avoid. find catalog/modules -mindepth 1 -type d -exec cp live/versions.tf {}/versions.tf \\; To use these modules, we need to use OpenTofu module blocks to reference them in a new main.tf file placed in the live directory (the OpenTofu root module). What we’re doing here is simply instantiating each of the modules that we’ve created so far by referencing them using a relative path to the module in the source attribute, setting values for their required inputs (some of which are acquired as outputs from other modules). module \"s3\" { source = \"../catalog/modules/s3\" name = var.name force_destroy = var.force_destroy} module \"ddb\" { source = \"../catalog/modules/ddb\" name = var.name} module \"iam\" { source = \"../catalog/modules/iam\" name = var.name aws_region = var.aws_region s3_bucket_arn = module.s3.arn dynamodb_table_arn = module.ddb.arn} module \"lambda\" { source = \"../catalog/modules/lambda\" name = var.name aws_region = var.aws_region s3_bucket_name = module.s3.name dynamodb_table_name = module.ddb.name lambda_zip_file = var.lambda_zip_file lambda_role_arn = module.iam.arn} We also want to forward outputs from these modules into our root module so that we can access them from the tofu CLI. output \"lambda_function_url\" { description = \"URL of the Lambda function\" value = module.lambda.url} output \"lambda_function_name\" { description = \"Name of the Lambda function\" value = module.lambda.name} output \"s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.s3.name} output \"s3_bucket_arn\" { description = \"ARN of the S3 bucket for static assets\" value = module.s3.arn} output \"dynamodb_table_name\" { description = \"Name of the DynamoDB table for asset metadata\" value = module.ddb.name} output \"dynamodb_table_arn\" { description = \"ARN of the DynamoDB table for asset metadata\" value = module.ddb.arn} output \"lambda_role_arn\" { description = \"ARN of the Lambda execution role\" value = module.iam.arn} We can also reduce the amount of content in the optional variables file, now that each of the modules define the variables that matter to them. This keeps the API of each module clean, as each module exposes the variables that specifically control them. variable \"aws_region\" { description = \"AWS region for all resources\" type = string default = \"us-east-1\"} variable \"force_destroy\" { description = \"Force destroy S3 buckets (only set to true for testing or cleanup of demo environments)\" type = bool default = false} After all this refactoring, we’ll want to run a plan to make sure we can safely apply our changes. Note You’ll need to re-initialize now that you’re using modules here. $ tofu init$ tofu plan...Plan: 11 to add, 0 to change, 11 to destroy.... Oh no! After all our refactors, we’ve introduced changes that would completely destroy all of the infrastructure we’ve created so far! This is a common scenario that you need to become comfortable with as you learn how to refactor and adjust IaC for scalability and maintainability. You leveraged the built-in protections of plans to give you a dry-run of your infrastructure updates, and can reason about why OpenTofu is trying to do what it’s doing here to avoid catastrophe. We, as authors of the IaC, know that all we’ve done in this step is move some files into different directories, but as far as OpenTofu is concerned, we’ve deleted resources at addresses like the following: # aws_lambda_function.main will be destroyed # (because aws_lambda_function.main is not in configuration) And introduced resources at addresses the like the following: # module.lambda.aws_lambda_function.main will be created The reason for this is that OpenTofu doesn’t really have a way of knowing the difference between moving a file like that for the sake of reorganization and completely removing infrastructure in one place and adding it in another without some help from IaC authors. The way we communicate to OpenTofu that a resource at one address has simply moved to a new address is to introduce move blocks. For each resource that we want to move, we’ll want to introduce a move block with a from of the old address (what OpenTofu reports as being destroyed in our plan) and a to of the equivalent new address (what OpenTofu reports as being created in our plan). moved { from = aws_dynamodb_table.asset_metadata to = module.ddb.aws_dynamodb_table.asset_metadata} moved { from = aws_iam_policy.lambda_basic_execution to = module.iam.aws_iam_policy.lambda_basic_execution} moved { from = aws_iam_policy.lambda_dynamodb to = module.iam.aws_iam_policy.lambda_dynamodb} moved { from = aws_iam_policy.lambda_s3_read to = module.iam.aws_iam_policy.lambda_s3_read} moved { from = aws_iam_role.lambda_role to = module.iam.aws_iam_role.lambda_role} moved { from = aws_iam_role_policy_attachment.lambda_basic_execution to = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = aws_iam_role_policy_attachment.lambda_dynamodb to = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = aws_iam_role_policy_attachment.lambda_s3_read to = module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = aws_lambda_function.main to = module.lambda.aws_lambda_function.main} moved { from = aws_lambda_function_url.main to = module.lambda.aws_lambda_function_url.main} moved { from = aws_s3_bucket.static_assets to = module.s3.aws_s3_bucket.static_assets} It’s worth noting that we haven’t been working with any infrastructure that’s important to preserve in this demo so far. We can easily reproduce this infrastructure without much effort. It’s important to know how to perform refactors without having to recreate infrastructure, though, as we need to be able to avoid paying the penalty of outages or data loss — especially when working with production infrastructure. If, for example, the database or s3 bucket being managed here had real customer information, it would be extremely important to avoid recreating these resources. OpenTofu doesn’t always know that recreating a stateful resource can cause permanent data loss. If you want the benefits we mentioned earlier of refactored IaC, you’ll want to know how to carefully handle state manipulation in OpenTofu and understand what it’s trying to do. So, as a small tangent, let’s discuss what actually happened when we introduced these moved blocks. There are multiple ways to configure OpenTofu backend state configurations, but the way that we’ve configured it here is to have the state files stored in S3 as JSON files. What we did under the hood with our moved blocks was update the content of that JSON file in s3://[your-state-bucket]/tofu.tfstate so that each of the resources in your state file used updated values for their resource addresses. In the example of this move: moved { from = aws_dynamodb_table.asset_metadata to = module.ddb.aws_dynamodb_table.asset_metadata} We updated one of the JSON objects in the state file from one that had these values: # Some stuff \"mode\": \"managed\", \"type\": \"aws_dynamodb_table\", \"name\": \"asset_metadata\", \"provider\": \"provider[\\\"registry.opentofu.org/hashicorp/aws\\\"]\", # More stuff To one that had these values: # Some stuff \"module\": \"module.ddb\", \"mode\": \"managed\", \"type\": \"aws_dynamodb_table\", \"name\": \"asset_metadata\", \"provider\": \"provider[\\\"registry.opentofu.org/hashicorp/aws\\\"]\", # More stuff When OpenTofu wants to know the current state of aws_dynamodb_table.asset_metadata, it can look it up using the first value, and when it wants to lookup the state of module.ddb.aws_dynamodb_table.asset_metadata it uses the second value. By moving the value in state, we’re just telling OpenTofu that we’re calling the resource by a different name now, without actually changing anything in AWS. Project Layout Check-inSection titled “Project Layout Check-in” You should have a filesystem layout that look like the following for your IaC now: Directorycatalog Directorymodules Directoryddb main.tfoutputs.tfvars-required.tfversions.tfDirectoryiam data.tfmain.tfoutputs.tfvars-required.tfversions.tfDirectorylambda main.tfoutputs.tfvars-optional.tfvars-required.tfversions.tfDirectorys3 main.tfoutputs.tfvars-optional.tfvars-required.tfversions.tfDirectorylive backend.tfmain.tfmoved.tfoutputs.tfproviders.tfvars-optional.tfvars-required.tfversions.tf Applying UpdatesSection titled “Applying Updates” You can now run tofu apply with no changes (don’t worry, you’ll get a chance to confirm you want to proceed before you have to commit to anything). $ tofu apply...Plan: 0 to add, 0 to change, 0 to destroy.... Do you want to perform these actions?OpenTofu will perform the actions described above.Only 'yes' will be accepted to approve. Enter a value: Trade-OffsSection titled “Trade-Offs” Before moving on to the next step, where we’ll duplicate our entire infrastructure estate to introduce a new development environment, it’s important to pause here and evaluate the trade-offs of this refactor. Both the infrastructure in step 1 and step 2 provisioned the exact same infrastructure (remember that there were 0 to add, 0 to change, 0 to destroy.). In fact, with the exception of the next step where we introduce the new dev environment, every step will result in the exact same infrastructure being provisioned. Why then is this refactor valuable? What do we gain by refactoring our IaC like this? What do we trade away in exchange? ProsSection titled “Pros” Abstraction by encapsulation. Instead of one large set of variables that could be used by any resource, or one large set of resources that could interact in ways that are difficult to understand, there are modules that encapsulate subsets of infrastructure so that they have explicit interfaces via variables and outputs. More code reusability. Each of these modules can be reused in live infrastructure or in other catalog modules (which we’ll see in the next step). ConsSection titled “Cons” Increased complexity. Instead of one self-contained directory with files directly defining resources to be provisioned, there’s a layer of indirection via modules. As someone consuming the module, you have to either trust it has been authored well (and that it’s well documented, tested, etc.) or vet the module yourself. State Adjustment. State manipulation or resource recreation is required to migrate to this pattern. Every subsequent stage is going to continue incurring trade-offs. You (or someone experienced you trust) must to decide whether these trade-offs are appropriate for your organization and your infrastructure estate. Wrap UpSection titled “Wrap Up” This was a significant refactoring step. You’ve transformed your flat configuration into a set of distinct, reusable modules, each with a well-defined API of variables and outputs. The most critical lesson here was mastering the moved block. This powerful feature allowed you to completely reorganize your code’s structure without OpenTofu needing to destroy and recreate your existing infrastructure, a vital skill for managing real-world infrastructure. While this adds a layer of indirection, the trade-off is greater code reusability and clearer separation of concerns. With this new modular structure, you’re now perfectly positioned to create a second environment with ease. Edit page Previous Step 1: Starting the Terralith Next Step 3: Adding dev",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.386Z"
  },
  {
    "title": "Step 3: Adding dev",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-3-adding-dev/",
    "content": "Step 3: Adding dev In the last step, you engaged in the foundational work of refactoring your monolithic configuration into a set of reusable modules, still instantiated in a single root module. Now it’s time to leverage those newly develop skills to create new infrastructure. One of the main advantages gained in creating infrastructure using IaC is improved reproducibility. The naive approach to creating new infrastructure is to directly copy and paste IaC to duplicate it, but there’s frequently advantage in packaging the infrastructure you’re going to replicate as a new pattern in your catalog so that you have a single source of truth for your shared IaC patterns. In this step, you’ll take the infrastructure you’ve created so far, do one more refactor to encapsulate it as a single reusable module, then instantiate it a second time as a second dev environment. TutorialSection titled “Tutorial” Let’s introduce that new higher level module as a new module named best_cat. It will provision the s3, ddb, lambda and iam modules we added in the last step, and wire them together. This will give us a single entity that we can duplicate across environments. Note We’re basically taking all the stuff in live and shoving it into this new best_cat module. module \"s3\" { source = \"../s3\" name = var.name force_destroy = var.force_destroy} module \"ddb\" { source = \"../ddb\" name = var.name} module \"iam\" { source = \"../iam\" name = var.name aws_region = var.aws_region s3_bucket_arn = module.s3.arn dynamodb_table_arn = module.ddb.arn} module \"lambda\" { source = \"../lambda\" name = var.name aws_region = var.aws_region s3_bucket_name = module.s3.name dynamodb_table_name = module.ddb.name lambda_zip_file = var.lambda_zip_file lambda_role_arn = module.iam.arn} output \"lambda_function_url\" { description = \"URL of the Lambda function\" value = module.lambda.url} output \"lambda_function_name\" { description = \"Name of the Lambda function\" value = module.lambda.name} output \"s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.s3.name} output \"s3_bucket_arn\" { description = \"ARN of the S3 bucket for static assets\" value = module.s3.arn} output \"dynamodb_table_name\" { description = \"Name of the DynamoDB table for asset metadata\" value = module.ddb.name} output \"dynamodb_table_arn\" { description = \"ARN of the DynamoDB table for asset metadata\" value = module.ddb.arn} output \"lambda_role_arn\" { description = \"ARN of the Lambda execution role\" value = module.iam.arn} variable \"aws_region\" { description = \"AWS region for all resources\" type = string default = \"us-east-1\"} variable \"force_destroy\" { description = \"Force destroy S3 buckets (only set to true for testing or cleanup of demo environments)\" type = bool default = false} variable \"name\" { description = \"Name used for all resources\" type = string} variable \"lambda_zip_file\" { description = \"Path to the Lambda function zip file\" type = string} Similar to what we did before with the constituent modules, we can simply replace the content in live with a reference to our new best_cat module. module \"dev\" { source = \"../catalog/modules/best_cat\" name = \"${var.name}-dev\" aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} module \"prod\" { source = \"../catalog/modules/best_cat\" name = var.name aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} Once again, we get the scary tofu plan that tells us we would recreate all our infrastructure if we were to naively apply here: $ tofu plan...Plan: 11 to add, 0 to change, 11 to destroy.... Luckily, we already know how to handle this. We’re going to update our moved.tf file to declare all the moves that need to be performed to transition the old addresses of resources to their new addresses. moved { from = module.ddb.aws_dynamodb_table.asset_metadata to = module.prod.module.ddb.aws_dynamodb_table.asset_metadata} moved { from = module.iam.aws_iam_policy.lambda_basic_execution to = module.prod.module.iam.aws_iam_policy.lambda_basic_execution} moved { from = module.iam.aws_iam_policy.lambda_dynamodb to = module.prod.module.iam.aws_iam_policy.lambda_dynamodb} moved { from = module.iam.aws_iam_policy.lambda_s3_read to = module.prod.module.iam.aws_iam_policy.lambda_s3_read} moved { from = module.iam.aws_iam_role.lambda_role to = module.prod.module.iam.aws_iam_role.lambda_role} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = module.lambda.aws_lambda_function.main to = module.prod.module.lambda.aws_lambda_function.main} moved { from = module.lambda.aws_lambda_function_url.main to = module.prod.module.lambda.aws_lambda_function_url.main} moved { from = module.s3.aws_s3_bucket.static_assets to = module.prod.module.s3.aws_s3_bucket.static_assets} Our apply now successfully completes without doing anything! $ tofu apply...Apply complete! Resources: 0 added, 0 changed, 0 destroyed.... Now the stage is set to add the additional dev environment. We can do that by duplicating the prod module, and labeling the new module block dev (you’ll also want to add a little suffix to the end of the name input to avoid naming collisions). module \"dev\" { source = \"../catalog/modules/best_cat\" name = \"${var.name}-dev\" aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} module \"prod\" { source = \"../catalog/modules/best_cat\" name = var.name aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} We also need to expose some of the outputs of the new dev module, but if we just duplicated all the prod outputs, we’d end up with a massive wall of outputs that would be hard to parse. Luckily, we only need two outputs to be externally accessible per environment, so we can drop a bunch of outputs to streamline things. output \"dev_lambda_function_url\" { description = \"URL of the Lambda function\" value = module.dev.lambda_function_url} output \"dev_s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.dev.s3_bucket_name} output \"prod_lambda_function_url\" { description = \"URL of the Lambda function\" value = module.prod.lambda_function_url} output \"prod_s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.prod.s3_bucket_name} Project Layout Check-inSection titled “Project Layout Check-in” We should have a project layout that looks like this now: Directorycatalog Directorymodules Directorybest_catmain.tfoutputs.tfvars-optional.tfvars-required.tfDirectoryddb main.tfoutputs.tfvars-required.tfversions.tfDirectoryiam data.tfmain.tfoutputs.tfvars-required.tfversions.tfDirectorylambda main.tfoutputs.tfvars-optional.tfvars-required.tfversions.tfDirectorys3 main.tfoutputs.tfvars-optional.tfvars-required.tfversions.tfDirectorylive backend.tfmain.tfmoved.tfoutputs.tfproviders.tfvars-optional.tfvars-required.tfversions.tf Applying UpdatesSection titled “Applying Updates” Now we can deploy our changes. Note We need to re-initialize here as we’ve added a new module. tofu inittofu apply We now have our new, fresh dev environment! Note This environment is very fresh. We don’t have the static assets uploaded, and we’re working with a fresh database. This guide isn’t going to into the ways in which Terragrunt could integrate into your build system to do this for you automatically, but know that it is part of Terragrunt’s feature set to handle this kind of thing. There are hints at the end of this guide to point those capabilities out and encourage your own exploration. Trade-offsSection titled “Trade-offs” ProsSection titled “Pros” We have officially reached the stage where we’re hitting risk increase due to our Terralith! This is the configuration of IaC that a lot of infrastructure estates grow to naturally as they tack on more resources and add environments. It’s a tipping point in maintainability that is best caught early, and addressed. We gained the ability to easily provision new infrastructure via reusable modules and could simply copy and paste (then season to taste) some configuration in our live/main.tf file. We also had a single source of truth for representing all the infrastructure that we were provisioning, in both the reusable module, and our live OpenTofu root module. We traded that for additional risk incurred, as every apply or destroy now has the potential to modify or destroy multiple environments, and you have to carefully avoid misconfiguration by reading plans (and trusting that they’re accurate) to avoid accidentally damaging the wrong environment. Furthermore, you also have to be very careful that you only modify the resources that you intend to modify within a given environment when you make updates to it (are you accidentally destroying your database when attempting a tagging update for your Lambda function?). The reason for this is that all your resources are in the same state file. OpenTofu has to make one atomic change to that single state file with every update, so all the resources in state are at risk when any change is made. For your information, there are tools out there, like OPA that enable automated reasoning about plan risk, but those tools are typically adopted by more advanced infrastructure teams, and there is typically a significant amount of overhead in authoring and maintaining the policies that assess plan risk (and driving behavior off those assessments). There are hints at the end of this guide to point those capabilities out and encourage your own exploration on this topic. Generally, the approach that teams take to structurally reduce this risk is to start to break down the Terralith into separate root modules, each with their own state. This gives teams confidence that they can only modify dev when they set their current working directory to the dev root module, and prod when their current working directory is the prod root module. When thinking through access control, this can also be convenient, as you can segment the access control that you use for one root module from another. Teams frequently configure their setups so that they need to explicitly use different credentials via role assumption, etc. when running commands in root modules related to different environments (e.g. dev vs prod ) to avoid accidental updates in the wrong environment. ConsSection titled “Cons” The downside to that approach, as we’ll see in the next step, is that it does increase the management burden of orchestrating and maintaining your IaC, and additional tooling like Terragrunt is a good way to handle that additional orchestration burden. Wrap UpSection titled “Wrap Up” You’ve successfully spun up a second, isolated development environment by reusing your new best_cat module. However, this is also the point where the Terralith design pattern starts to incur some serious drawbacks. At this stage, all your infrastructure for both your environments (dev and prod) now lives in a single state file. This introduces significant risk. A small mistake intended for dev could potentially damage or destroy your prod environment because OpenTofu sees it all as one atomic unit to manage, and you’re responsible for reasoning about the generated plan to see if you should proceed with an apply. The next step is the most critical step in maturing your IaC estate (as far as this guide is concerned) as you break this monolith apart to limit the blast radius of your updates. Edit page Previous Step 2: Refactoring Next Step 4: Breaking the Terralith",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.429Z"
  },
  {
    "title": "Step 4: Breaking the Terralith",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-4-breaking-the-terralith/",
    "content": "Step 4: Breaking the Terralith In the previous step, you successfully duplicated your prod environment as a new dev environment by replicating your prod module declaration as a new dev module instance in your live/main.tf file. While this demonstrated the power of reusable modules, it also introduced significant risk: the Terralith. You’ve tightly coupled management of both your dev and prod environments in a single state file, and you introduce risk to one whenever you attempt to make changes to another. In this step, you’ll solve this problem by breaking apart your Terralith apart. You’ll refactor your live root module into two distinct dev and prod root modules. Each will have its own state file, completely eliminating the risk of accidental cross-environment changes. TutorialSection titled “Tutorial” Breaking down your Terralith so that you have multiple root modules is fairly simple now that you understand state manipulation a bit better. First, let’s create a top-level directory for prod in live. mkdir prod Next, let’s move everything into the prod directory (If you’re not comfortable with using the find command here, you can just drag the content into the prod directory). find . -mindepth 1 -maxdepth 1 -not -name 'prod' -exec mv {} prod/ ; To complete our new multi-environment setup, let’s duplicate that prod directory to a new dev directory. cp -R prod dev We need to edit the contents of the dev and prod directories to make some key adjustments. First, we’ll want to make sure that the backend.tf files are updated to use new keys so that the two root modules don’t conflict with each other. terraform { backend \"s3\" { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"dev/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} terraform { backend \"s3\" { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"prod/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} We’ll also want to update the references to the shared module, update the .auto.tfvars file and edit the outputs to handle all the changes necessary for this project. module \"main\" { source = \"../../catalog/modules/best_cat\" name = var.name aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} module \"main\" { source = \"../../catalog/modules/best_cat\" name = var.name aws_region = var.aws_region lambda_zip_file = var.lambda_zip_file force_destroy = var.force_destroy} Note These two files are now exactly the same. This will be important to keep in mind later. Given that we’ve renamed the the module, we’ll also need to add moved blocks to handle the state moves that need to take place here. If you’re not sure what we’re doing here, consider reviewing earlier steps. moved { from = module.dev.module.ddb.aws_dynamodb_table.asset_metadata to = module.main.module.ddb.aws_dynamodb_table.asset_metadata} moved { from = module.dev.module.iam.aws_iam_policy.lambda_basic_execution to = module.main.module.iam.aws_iam_policy.lambda_basic_execution} moved { from = module.dev.module.iam.aws_iam_policy.lambda_dynamodb to = module.main.module.iam.aws_iam_policy.lambda_dynamodb} moved { from = module.dev.module.iam.aws_iam_policy.lambda_s3_read to = module.main.module.iam.aws_iam_policy.lambda_s3_read} moved { from = module.dev.module.iam.aws_iam_role.lambda_role to = module.main.module.iam.aws_iam_role.lambda_role} moved { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = module.dev.module.lambda.aws_lambda_function.main to = module.main.module.lambda.aws_lambda_function.main} moved { from = module.dev.module.lambda.aws_lambda_function_url.main to = module.main.module.lambda.aws_lambda_function_url.main} moved { from = module.dev.module.s3.aws_s3_bucket.static_assets to = module.main.module.s3.aws_s3_bucket.static_assets} moved { from = module.prod.module.ddb.aws_dynamodb_table.asset_metadata to = module.main.module.ddb.aws_dynamodb_table.asset_metadata} moved { from = module.prod.module.iam.aws_iam_policy.lambda_basic_execution to = module.main.module.iam.aws_iam_policy.lambda_basic_execution} moved { from = module.prod.module.iam.aws_iam_policy.lambda_dynamodb to = module.main.module.iam.aws_iam_policy.lambda_dynamodb} moved { from = module.prod.module.iam.aws_iam_policy.lambda_s3_read to = module.main.module.iam.aws_iam_policy.lambda_s3_read} moved { from = module.prod.module.iam.aws_iam_role.lambda_role to = module.main.module.iam.aws_iam_role.lambda_role} moved { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = module.main.module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = module.prod.module.lambda.aws_lambda_function.main to = module.main.module.lambda.aws_lambda_function.main} moved { from = module.prod.module.lambda.aws_lambda_function_url.main to = module.main.module.lambda.aws_lambda_function_url.main} moved { from = module.prod.module.s3.aws_s3_bucket.static_assets to = module.main.module.s3.aws_s3_bucket.static_assets} Next, we’ll update the outputs, just like we did for the main.tf files. output \"lambda_function_url\" { description = \"URL of the Lambda function\" value = module.main.lambda_function_url} output \"s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.main.s3_bucket_name} output \"lambda_function_url\" { description = \"URL of the Lambda function\" value = module.main.lambda_function_url} output \"s3_bucket_name\" { description = \"Name of the S3 bucket for static assets\" value = module.main.s3_bucket_name} Note These two files are also exactly the same. This will be important to keep in mind later. Finally, we need to update the .auto.tfvars files to reflect the difference in inputs passed to variables in these two root modules. # Required: Name used for all resources (must be unique)name = \"best-cat-2025-09-24-2359\" # Required: Path to your Lambda function zip filelambda_zip_file = \"../../dist/best-cat.zip\" # Required: Name used for all resources (must be unique)name = \"best-cat-2025-09-24-2359-dev\" # Required: Path to your Lambda function zip filelambda_zip_file = \"../../dist/best-cat.zip\" Note These two files differ in that they have different name values. They’re not identical, but they’re very similar. It’s time for some more state manipulation! We currently have a single state file in S3 at s3://[your-state-bucket]/tofu.tfstate. Our plan for splitting the state is to basically duplicate state for both the dev and prod root modules, then remove resources that we don’t need from state in each of the root modules. In addition to having the state in S3, we also have a local copy of state in each root module. Running the tofu init -migrate-state command with the .terraform directory populated by copy of state from the previous configuration of the project will copy state to the new location in each new root module. $ tofu init -migrate-state Initializing the backend...Backend configuration changed! OpenTofu has detected that the configuration specified for the backendhas changed. OpenTofu will now check for existing state in the backends. Do you want to copy existing state to the new backend?Pre-existing state was found while migrating the previous \"s3\" backend to thenewly configured \"s3\" backend. No existing state was found in the newlyconfigured \"s3\" backend. Do you want to copy this state to the new \"s3\"backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: yes Successfully configured the backend \"s3\"! OpenTofu will automaticallyuse this backend unless the backend configuration changes. $ tofu init -migrate-state Initializing the backend...Backend configuration changed! OpenTofu has detected that the configuration specified for the backendhas changed. OpenTofu will now check for existing state in the backends. Do you want to copy existing state to the new backend?Pre-existing state was found while migrating the previous \"s3\" backend to thenewly configured \"s3\" backend. No existing state was found in the newlyconfigured \"s3\" backend. Do you want to copy this state to the new \"s3\"backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: yes Successfully configured the backend \"s3\"! OpenTofu will automaticallyuse this backend unless the backend configuration changes. We now have the state in s3://[your-state-bucket]/tofu.tfstate copied to both: s3://[your-state-bucket]/dev/tofu.tfstate s3://[your-state-bucket]/prod/tofu.tfstate We need to remove the resources from state that aren’t relevant in the new root modules, now so that we don’t deploy prod resources in the dev root module and vice versa. removed { from = module.prod.module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.prod.module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.prod.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.prod.module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.prod.module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} removed { from = module.dev.module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.dev.module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.dev.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.dev.module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.dev.module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} Project Layout Check-inSection titled “Project Layout Check-in” At this stage, we should have a live directory that looks like the following (the catalog directory shouldn’t have changed at all): Directorylive Directorydev backend.tfmain.tfmoved.tfoutputs.tfproviders.tfremoved.tfvars-optional.tfvars-required.tfversions.tfDirectoryprod backend.tfmain.tfmoved.tfoutputs.tfproviders.tfremoved.tfvars-optional.tfvars-required.tfversions.tf Applying UpdatesSection titled “Applying Updates” We should now see that we’re simply going to forget the removed resources instead of destroying them. $ tofu plan...Plan: 0 to add, 1 to change, 0 to destroy, 11 to forget.... Let’s apply both dev and prod to finalize the moves and removals. $ tofu apply...Apply complete! Resources: 0 added, 1 changed, 0 destroyed, 11 forgotten.... $ tofu apply...Apply complete! Resources: 0 added, 1 changed, 0 destroyed, 11 forgotten.... Trade-offsSection titled “Trade-offs” ProsSection titled “Pros” We did it! We successfully broke apart our Terralith using OpenTofu alone. Some organizations get to this stage in their IaC journey, and are perfectly happy with managing their infrastructure like this. You can limit the blast radius of your dev and prod environments this way, and it’s fairly straightforward to adjust your current working directory to the dev root module when making modifications to the dev environment, and adjusting your working directory to the prod root module when making modifications to the prod environment. This is actually the pattern that Gruntwork was initially helping customers achieve early on to make their infrastructure safer, and more manageable by teams. ConsSection titled “Cons” There are, however, some downsides to how we’re managing infrastructure here. There’s some annoying boilerplate that’s inconvenient to create and maintain. The following files are identical in each environment, but need to be present just to get OpenTofu to provision the same module: main.tf outputs.tf providers.tf vars-optional.tf vars-required.tf We also have almost the same file in each of these, and their values aren’t really that interesting. backend.tf .auto.tfvars We also don’t have a convenient way to run multiple root modules at once. What if we want to update both dev and prod at once? What if we want to break down the environments further? As you might have guessed, the next step is to introduce Terragrunt to address some of these downsides, and unlock even more capabilities for managing infrastructure at scale. Wrap UpSection titled “Wrap Up” This is a pivotal moment in this guide. You have successfully started to break down the Terralith! By migrating your state and refactoring your configuration, you have split your single, high-risk state file into two separate ones: one for dev and one for prod. The primary benefit is safety. You’ve drastically reduced the blast radius, as running tofu apply in the dev directory can now only affect development resources and running tofu apply in the prod directory can only affect production resources. However, this safety has come at the cost of duplication. Your dev and prod directories contain a lot of identical, boilerplate .tf files, and it isn’t very scalable. What if you have twice as many environments? What if you have ten times as many? How are you going to handle making all those updates? Helping customers solve these problems and more at scale is what Terragrunt was designed for, which we’ll introduce next to streamline your workflow. Edit page Previous Step 3: Adding dev Next Step 5: Adding Terragrunt",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.479Z"
  },
  {
    "title": "Step 5: Adding Terragrunt",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-5-adding-terragrunt/",
    "content": "Step 5: Adding Terragrunt In the last step, you took a massive leap forward in safety by breaking your Terralith into separate dev and prod root modules. The trade-off, however, is that you’ve created a significant amount of boilerplate and duplication. Your dev and prod directories are filled with nearly identical .tf files (if not completely identical), and managing them involves a lot of careful copy-pasting. You also can’t conveniently manage multiple root modules at once. This isn’t scalable and is prone to error. This is the problem Terragrunt was created to solve. It acts as an orchestrator for OpenTofu/Terraform, helping you write DRY (Don’t Repeat Yourself) infrastructure code that scales. In this step, you’ll introduce Terragrunt to drastically reduce that boilerplate. You will: Replace the duplicated .tf and .auto.tfvars files in each environment with a single, concise terragrunt.hcl file. Use Terragrunt’s terraform, inputs, and generate blocks to define the module source, pass variables, and create configuration files on the fly. Centralize common configurations (like your S3 backend configuration) in a single root.hcl file using the include block, ensuring your setup is easy to maintain. By the end of this step, your live directory will be dramatically leaner, paving the way for easier management and scaling. TutorialSection titled “Tutorial” Now that we’ve structured our project to segment environments into their own root modules (and their own state files), it’s pretty simple to convert our root modules to Terragrunt units. In Terragrunt terminology, a unit is a single instance of infrastructure managed by Terragrunt. They’re easy to manage, and they come with a lot of tooling to support common IaC needs, like code generation, authentication, error handling, and more. The process of converting an OpenTofu root module to a Terragrunt unit simply involves adding an empty terragrunt.hcl file to each root module (that’s all the find command below does). This allows Terragrunt to recognize the contents of the directory as a Terragrunt unit, and orchestrate infrastructure updates within it. find . -mindepth 1 -maxdepth 1 -type dir -exec touch {}/terragrunt.hcl ; Now, we can use Terragrunt to orchestrate runs across both of these units. $ terragrunt run --all plan15:07:02.593 INFO The runner at . will be processed in the following order for command plan:Group 1- Unit ./dev- Unit ./prod... We can also selectively run the plan for the dev environment by changing the working directory to dev, or using the --queue-include-dir flag. $ terragrunt plan $ terragrunt run --all --queue-include-dir dev plan15:09:17.090 INFO The runner at . will be processed in the following order for command plan:Group 1- Unit ./dev ... Terragrunt is frequently adopted gradually in this manner. If you have an infrastructure problem you want addressed, you can gradually introduce more and more Terragrunt tooling to address those problems. We can also simplify things significantly now that we’re using Terragrunt. Terragrunt is designed to work well in this pattern where the majority of logic is abstracted away to a shared module. We can eliminate the need for some boilerplate now that we have access to the terraform block in terragrunt.hcl files (It’s named terraform for legacy reasons. It’s 100% compatible with OpenTofu). terraform { source = \"../../catalog/modules//best_cat\"} terraform { source = \"../../catalog/modules//best_cat\"} Note Those // are there on purpose. They’re how go-getter, the library that Terragrunt uses (just like OpenTofu), indicates that it’s working with a directory within a module source. This allows relative references like ../s3 to work within the best_cat module. With those changes, we can now remove the unnecessary boilerplate related to invoking the shared module. rm -f ./*/main.tf ./*/outputs.tf ./*/vars-*.tf ./*/versions.tf We can also leverage the inputs attribute in the terragrunt.hcl file to set inputs instead of relying on the separate .auto.tfvars file. terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} Note the use of get_repo_root(). This is a simple convenience function you can use to get the path to the root of your Git repository. You can use almost all of the same HCL functions you can use in OpenTofu, with some additional functions supplied by Terragrunt for tasks that are more useful in the context of Terragrunt (you can see the full list in the official Terragrunt HCL functions reference here). rm -f ./*/.auto.tfvars ./*/.auto.tfvars.example We can also get Terragrunt to generate that backend.tf file for us on-demand using the remote_state block. remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"dev/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"prod/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} rm -f ./*/backend.tf In fact, we can have Terragrunt generate any arbitrary file we need on-demand, including boilerplate files like we had in the providers.tf file. remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"dev/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} generate \"providers\" { path = \"providers.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"prod/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} generate \"providers\" { path = \"providers.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} rm -f ./*/providers.tf What basically all Terragrunt users do at this stage is refactor out that core shared configuration (backend and provider configurations in this case), into a shared root.hcl file that all terragrunt.hcl files include. This allows for greater reuse of configuration that’s common to all Terragrunt units. remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-to-terralith-tfstate-2025-09-24-2359\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} generate \"providers\" { path = \"providers.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\"}EOF} Note the use of path_relative_to_include() in the key. This tells Terragrunt to use the path relative to the include of the root.hcl file. This can be a little confusing for new users, so just to make it very explicit: The live/root.hcl file is going to be included by the live/dev/terragrunt.hcl file. As such, the path of the including unit (live/dev) relative to the path of the directory for the included file (live) is dev. We therefore expect ${path_relative_to_include()} to resolve to dev in the live/dev unit, and prod in the live/prod unit (which is coincidentally how we setup our state keys before). Now we can add the include block that actually performs this include in each of the unit configuration files, which is just three lines. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\" # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"../../catalog/modules//best_cat\"} inputs = { name = \"best-cat-2025-09-24-2359\" lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\" # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true} Note the addition of find_in_parent_folders() in the added include block. As you might expect, it returns the path to the root.hcl file found in the parent folders of live/prod (which is live/root.hcl). We just need to do a little more state manipulation using moved blocks, which we should be very familiar with at this stage. When we removed the indirection of the main module in the main.tf file, we also changed the addresses of resources in state. Let’s take care of that by updating the moved.tf file. moved { from = module.main.module.ddb.aws_dynamodb_table.asset_metadata to = module.ddb.aws_dynamodb_table.asset_metadata} moved { from = module.main.module.iam.aws_iam_policy.lambda_basic_execution to = module.iam.aws_iam_policy.lambda_basic_execution} moved { from = module.main.module.iam.aws_iam_policy.lambda_dynamodb to = module.iam.aws_iam_policy.lambda_dynamodb} moved { from = module.main.module.iam.aws_iam_policy.lambda_s3_read to = module.iam.aws_iam_policy.lambda_s3_read} moved { from = module.main.module.iam.aws_iam_role.lambda_role to = module.iam.aws_iam_role.lambda_role} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = module.main.module.lambda.aws_lambda_function.main to = module.lambda.aws_lambda_function.main} moved { from = module.main.module.lambda.aws_lambda_function_url.main to = module.lambda.aws_lambda_function_url.main} moved { from = module.main.module.s3.aws_s3_bucket.static_assets to = module.s3.aws_s3_bucket.static_assets} moved { from = module.main.module.ddb.aws_dynamodb_table.asset_metadata to = module.ddb.aws_dynamodb_table.asset_metadata} moved { from = module.main.module.iam.aws_iam_policy.lambda_basic_execution to = module.iam.aws_iam_policy.lambda_basic_execution} moved { from = module.main.module.iam.aws_iam_policy.lambda_dynamodb to = module.iam.aws_iam_policy.lambda_dynamodb} moved { from = module.main.module.iam.aws_iam_policy.lambda_s3_read to = module.iam.aws_iam_policy.lambda_s3_read} moved { from = module.main.module.iam.aws_iam_role.lambda_role to = module.iam.aws_iam_role.lambda_role} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.main.module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = module.iam.aws_iam_role_policy_attachment.lambda_s3_read} moved { from = module.main.module.lambda.aws_lambda_function.main to = module.lambda.aws_lambda_function.main} moved { from = module.main.module.lambda.aws_lambda_function_url.main to = module.lambda.aws_lambda_function_url.main} moved { from = module.main.module.s3.aws_s3_bucket.static_assets to = module.s3.aws_s3_bucket.static_assets} Note This is an important movement to take note of. Even though we introduced the new abstraction of the Terragrunt unit, we actually reduced indirection in OpenTofu. As we’ll see again later in this guide, creating granular Terragrunt units results in simpler OpenTofu modules, reducing the complexity of each unit of infrastructure. We can also remove the removed.tf files now that we’ve already “forgotten” them. rm -f ./*/removed.tf Project Layout Check-inSection titled “Project Layout Check-in” We should now have a file layout like the following in the live directory: Directorylive Directorydev moved.tfterragrunt.hclDirectoryprod moved.tfterragrunt.hclroot.hcl Applying UpdatesSection titled “Applying Updates” We’re ready to run a plan across both units to see if things are working correctly after all our refactors! terragrunt run --all plan When we’re ready, we can apply our changes as well. terragrunt run --all apply Trade-offsSection titled “Trade-offs” ProsSection titled “Pros” Significantly Reduced Duplication: We’ve eliminated the need to have the following files in every environment (along with their contents): main.tf providers.tf versions.tf outputs.tf Centralized Configuration: You know have a central location for storing common configurations like backend and provider configurations in your root.hcl file. Scalable IaC Growth: Adding new environments and more is scalable now. You simply add a new Terragrunt unit, and you get isolated infrastructure that can be managed independently of the rest of your infrastructure estate. Orchestration: You can now manage all your environments from the root of the live directory using commands like terragrunt run --all apply, which was not possible before without custom scripting or other additional tooling. ConsSection titled “Cons” Additional Tooling: You and your team now depend on Terragrunt for critical workflows. You need to make sure you have the tool is installed and supported everywhere you want to manage infrastructure, and that your team is educated on how it works. Added Abstraction: Although the OpenTofu code that you manage in each unit is now simpler, you now have to reason about Terragrunt configurations and commands when considering how they’ll be used. Wrap UpSection titled “Wrap Up” With the introduction of Terragrunt, you’ve remediated the duplication and boilerplate created in the last step. You replaced numerous .tf and .tfvars files in each environment with a single, concise terragrunt.hcl file. In this step, you learned how to use the terraform block to specify a module source to generate a root module on demand, the inputs block to pass variables to that root module, and the generate block to inject additional files on the fly. Finally, you used the powerful include block to create a central root.hcl, ensuring your configuration is DRY (Don’t Repeat Yourself). Your live infrastructure code is now dramatically leaner and easier to manage across many environments. Edit page Previous Step 4: Breaking the Terralith Next Step 6: Breaking the Terralith Further",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.527Z"
  },
  {
    "title": "Step 6: Breaking the Terralith Further",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-6-breaking-the-terralith-further/",
    "content": "Step 6: Breaking the Terralith Further You’ve successfully added Terragrunt to your project, eliminating significant boilerplate from each of your dev and prod environments. While your environments are now isolated from each other, the resources within each environment (your S3 bucket, DynamoDB table, IAM role, and Lambda function) are still managed together in a single state file. This is essentially a smaller-scale Terralith within each environment. This tight coupling poses its own risks. Do you really want a routine update to your Lambda function’s application code to require a plan that also evaluates your production database? Stateful resources like databases and storage buckets change infrequently and require maximum stability, while stateless application code changes constantly. Coupling them in the same state file means a mistake in one could still impact the other, increasing the blast radius of any single change. In this step, you will break the Terralith down even further. You will transform each environment from a single large unit into a collection of smaller, independent units, one for each core component (S3, DDB, IAM, and Lambda). This granular approach provides far more safety and flexibility, and is common in Terragrunt projects. To connect these newly independent components, you’ll learn one of Terragrunt’s most powerful features: the dependency block, which allows units to share outputs, such as passing the ARN of your S3 bucket to your IAM policy, and control the order of updates in your infrastructure units. TutorialSection titled “Tutorial” We’re going to follow a very similar process to what we did when breaking apart the Terralith into two environments. First, we’ll create a directory for each of the new units we want to create for all the constituent modules of the best_cat megamodule. In each of our environments (dev and prod). mkdir -p {dev, prod}/{s3, ddb, iam, lambda} Next, we’ll create the terragrunt.hcl files in each of these directories. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//ddb\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//ddb\"} inputs = { name = \"best-cat-2025-09-24-2359\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//s3\"} inputs = { name = \"best-cat-2025-09-24-2359-dev\" # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//s3\"} inputs = { name = \"best-cat-2025-09-24-2359\" # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true} In units where we need to integrate with other units (like the iam unit), we’ll need to add a dependency block to tell Terragrunt how it can fetch outputs from relevant dependencies for use as inputs. Terragrunt has to integrate different units like this, as they don’t have the same state file, so OpenTofu needs an external tool, like Terragrunt to pull outputs out of state from one unit and pass in inputs to another unit. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//iam\"} dependency \"s3\" { config_path = \"../s3\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:s3:::mock-bucket-name\" }} dependency \"ddb\" { config_path = \"../ddb\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:dynamodb:us-east-1:123456789012:table/mock-table-name\" }} inputs = { name = \"best-cat-2025-09-24-2359-dev\" aws_region = \"us-east-1\" s3_bucket_arn = dependency.s3.outputs.arn dynamodb_table_arn = dependency.ddb.outputs.arn} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//iam\"} dependency \"s3\" { config_path = \"../s3\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:s3:::mock-bucket-name\" }} dependency \"ddb\" { config_path = \"../ddb\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:dynamodb:us-east-1:123456789012:table/mock-table-name\" }} inputs = { name = \"best-cat-2025-09-24-2359\" aws_region = \"us-east-1\" s3_bucket_arn = dependency.s3.outputs.arn dynamodb_table_arn = dependency.ddb.outputs.arn} Note Unfortunately, OpenTofu doesn’t have a way of specifying certain inputs as “unknown” as of yet, but we can work around this limitation by taking advantage of Terragrunt’s ability to mock outputs from dependencies using mock_outputs. This allows us to plan successfully without worrying about getting errors that required variables aren’t passed in. Note that some providers like the AWS provider require these inputs to be well formed (in this case, they have to be valid AWS ARNs). In these scenarios, it can be important to provide valid looking ARNs as a consequence to satisfy provider validations. If you just passed mock-bucket-arn as the value of the input s3_bucket_arn, the AWS provider might throw an error during plans, as it expects the value to look more like arn:aws:s3:::mock-bucket-name, and it assumes that the user made an error. We’ve also set the mock_outputs_allowed_terraform_commands attribute. By default, Terragrunt will use mocked outputs whenever a dependency returns no outputs. This is typically only the case for plans, but we can be explicit about when Terragrunt is allowed to mock outputs to avoid any accidental applies with mocked values. Other commands that might benefit from mocking are commands like destroy and validate. I don’t anticipate needing them mocked here, so I’ve only allowed mocking for commands where I know we’re going to need them mocked during this guide (you’ll see why state can get mocked outputs in a bit). Finally, note that we’ve also set the mock_outputs_merge_strategy_with_state attribute. By default, Terragrunt treats mocking as something binary: Either outputs are mocked, or they’re not. This is because you typically don’t have a need to partially mock some outputs and not others. In our use-case, where we’re migrating over state we will need to do this, as we’ll be pushing existing state to units, but their outputs are also changing. We’ll see what that looks like later. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//lambda\"} dependency \"s3\" { config_path = \"../s3\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-bucket-name\" }} dependency \"ddb\" { config_path = \"../ddb\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-table-name\" }} dependency \"iam\" { config_path = \"../iam\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:iam::123456789012:role/mock-role-name\" }} inputs = { name = \"best-cat-2025-09-24-2359-dev\" aws_region = \"us-east-1\" s3_bucket_name = dependency.s3.outputs.name dynamodb_table_name = dependency.ddb.outputs.name lambda_role_arn = dependency.iam.outputs.arn lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//lambda\"} dependency \"s3\" { config_path = \"../s3\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-bucket-name\" }} dependency \"ddb\" { config_path = \"../ddb\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-table-name\" }} dependency \"iam\" { config_path = \"../iam\" mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:iam::123456789012:role/mock-role-name\" }} inputs = { name = \"best-cat-2025-09-24-2359\" aws_region = \"us-east-1\" s3_bucket_name = dependency.s3.outputs.name dynamodb_table_name = dependency.ddb.outputs.name lambda_role_arn = dependency.iam.outputs.arn lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} Project Layout Check-inSection titled “Project Layout Check-in” We should now have a file tree that looks like the following (we’ll be getting rid of the two top-level terragrunt.hcl and moved.tf files in each environment soon): Directorylive Directorydev Directoryddb terragrunt.hclDirectoryiam terragrunt.hclDirectorylambda terragrunt.hclDirectorys3 terragrunt.hclterragrunt.hcl (This is being removed soon)moved.tf (This is being removed soon)Directoryprod Directoryddb terragrunt.hclDirectoryiam terragrunt.hclDirectorylambda terragrunt.hclDirectorys3 terragrunt.hclterragrunt.hcl (This is being removed soon)moved.tf (This is being removed soon)root.hcl Migrating State to Individual UnitsSection titled “Migrating State to Individual Units” It’s time to engage in our favorite solution for IaC refactoring, state manipulation! We’re going to use the tools we’ve learned so far, and pull state from those two top-level units, then push them into the constituent units we’ve broken the megamodule down into. We expect to need to both move resource addresses in state, and forget particular resources to avoid accidentally destroying anything. terragrunt state pull > /tmp/tofu.tfstatecd ddb && terragrunt state push /tmp/tofu.tfstatecd ../iam && terragrunt state push /tmp/tofu.tfstatecd ../lambda && terragrunt state push /tmp/tofu.tfstatecd ../s3 && terragrunt state push /tmp/tofu.tfstate terragrunt state pull > /tmp/tofu.tfstatecd ddb && terragrunt state push /tmp/tofu.tfstatecd ../iam && terragrunt state push /tmp/tofu.tfstatecd ../lambda && terragrunt state push /tmp/tofu.tfstatecd ../s3 && terragrunt state push /tmp/tofu.tfstate We can now clean up the extraneous files mentioned earlier at the root of the environments. rm -f {dev, prod}/{terragrunt.hcl, moved.tf} Go ahead and run the following to see very similar plan output to what we’ve seen in the past when we needed to make state moves & removes. terragrunt run --all plan # Lots of destroys! The following moves and removes will handle the state transitions necessary here. moved { from = module.ddb.aws_dynamodb_table.asset_metadata to = aws_dynamodb_table.asset_metadata} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} moved { from = module.iam.aws_iam_policy.lambda_basic_execution to = aws_iam_policy.lambda_basic_execution} moved { from = module.iam.aws_iam_policy.lambda_dynamodb to = aws_iam_policy.lambda_dynamodb} moved { from = module.iam.aws_iam_policy.lambda_s3_read to = aws_iam_policy.lambda_s3_read} moved { from = module.iam.aws_iam_role.lambda_role to = aws_iam_role.lambda_role} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = aws_iam_role_policy_attachment.lambda_s3_read} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} moved { from = module.lambda.aws_lambda_function.main to = aws_lambda_function.main} moved { from = module.lambda.aws_lambda_function_url.main to = aws_lambda_function_url.main} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} moved { from = module.s3.aws_s3_bucket.static_assets to = aws_s3_bucket.static_assets} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} moved { from = module.ddb.aws_dynamodb_table.asset_metadata to = aws_dynamodb_table.asset_metadata} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} moved { from = module.iam.aws_iam_policy.lambda_basic_execution to = aws_iam_policy.lambda_basic_execution} moved { from = module.iam.aws_iam_policy.lambda_dynamodb to = aws_iam_policy.lambda_dynamodb} moved { from = module.iam.aws_iam_policy.lambda_s3_read to = aws_iam_policy.lambda_s3_read} moved { from = module.iam.aws_iam_role.lambda_role to = aws_iam_role.lambda_role} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution to = aws_iam_role_policy_attachment.lambda_basic_execution} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb to = aws_iam_role_policy_attachment.lambda_dynamodb} moved { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read to = aws_iam_role_policy_attachment.lambda_s3_read} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} moved { from = module.lambda.aws_lambda_function.main to = aws_lambda_function.main} moved { from = module.lambda.aws_lambda_function_url.main to = aws_lambda_function_url.main} removed { from = module.s3.aws_s3_bucket.static_assets lifecycle { destroy = false }} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} moved { from = module.s3.aws_s3_bucket.static_assets to = aws_s3_bucket.static_assets} removed { from = module.ddb.aws_dynamodb_table.asset_metadata lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role.lambda_role lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_policy.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_s3_read lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_dynamodb lifecycle { destroy = false }} removed { from = module.iam.aws_iam_role_policy_attachment.lambda_basic_execution lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function.main lifecycle { destroy = false }} removed { from = module.lambda.aws_lambda_function_url.main lifecycle { destroy = false }} That was a ton of work! The effort of making these state moves might encourage you to do some early planning to avoid the need to do these kinds of state moves down the line as you plan your infrastructure estate. Folks sometimes feel like they don’t really want or need to adopt Terragrunt before they reach a point where scaling up IaC further becomes painful. Deciding to avoid learning Terragrunt before this point is a form of tech debt accrual. Doing the work up-front to follow the patterns that Terragrunt enables (like segmenting state at granular levels) helps to mitigate the severity of refactor work down the line. If we had architected our IaC ahead of time to use small, focused units, we never would have had to do the work of these state moves. Hopefully, going through these state moves in this guide gives you confidence that you can do it if you need to, however. As long as you move carefully, and know what you’re doing, you can break down even the largest Terraliths with time! Applying UpdatesSection titled “Applying Updates” Now, let’s repeat our plan to confirm that we won’t destroy anything important. If you do see any destroys, you probably have something misconfigured in one of your moved.tf or removed.tf files. Review them carefully. terragrunt run --all plan # No destroys!# You might see some creates, but that's a side-effect of how# OpenTofu tracks state internally. You are safe to ignore them. Thankfully, now that we’ve segmented state we can carefully run across the dev units before running in prod, with zero risk that we’re going to accidentally break anything there. We can actually perform our updates even more carefully by updating one unit at a time, but that’s not really necessary for our use-case here. Consider when it might make sense to do that for your own real infrastructure, however. If you are doing state manipulation like this on stateful production resources like databases or blob stores for example, it’s a good idea to move slower to avoid data loss or outages. terragrunt run --all apply # Migration complete! terragrunt run --all apply # Migration complete! Trade-offsSection titled “Trade-offs” You’ve now reached the most granular and arguably the safest way to structure a Terragrunt project (while remaining practical about avoiding over-segmenting resources). By breaking down each environment into component-specific units, you’ve moved from a “one state file per environment” model to a “one state file per component, per environment” model. This is a common and highly recommended pattern for mature Infrastructure as Code (IaC) management, but it comes with its own set of trade-offs. Pros Safety and Granular Blast Radius: This is the single biggest advantage. A change to a stateless resource that changes frequently, like the Lambda function, now has zero chance of impacting a stateful resource that changes rarely, like the DynamoDB table or S3 bucket. Reduced Lock Contention: State locks are now per-component, meaning an apply on the Lambda function won’t block a simultaneous apply on the IAM role, enabling more concurrent infrastructure work by platform teams. Faster Feedback Loops: When you run terragrunt plan inside a specific component directory (e.g., live/dev/lambda), OpenTofu only needs to refresh the state for that single component. This is significantly faster than refreshing the state for the entire environment, which is a huge productivity win on large projects. Cons Increased Configuration Complexity: The number of directories and terragrunt.hcl files has multiplied. While each file is simple, managing the overall structure requires more discipline. The cognitive load shifts from understanding a single large module to understanding how many small, interconnected modules form a complete system. Explicit Dependency Management: You now must explicitly define the relationships between your components using dependency blocks. This is powerful but also creates another layer of configuration to maintain. Forgetting a dependency or referencing it incorrectly will cause failures. Mocking Outputs: As demonstrated in the tutorial, you can’t plan a component that depends on another component that doesn’t exist yet. This necessitates using mock_outputs if you want to perform a run --all plan against a stack with unapplied dependencies, which is a powerful workaround but adds another concept that engineers must learn and manage correctly. Wrap UpSection titled “Wrap Up” You’ve now taken modularity to the next level. Instead of one state file per environment, you now have one state file per component (S3, DDB, IAM, Lambda) within each environment. This provides the ultimate level of granular control and safety. You can now update your application’s Lambda function with zero risk of accidentally modifying your stateful database or storage bucket. The core lesson here was learning how to use the dependency block, Terragrunt’s mechanism for wiring together independent units by passing outputs from one unit as inputs to another. You also learned to use mock_outputs to solve the problem that arises when planning interdependent infrastructure that doesn’t exist yet. However, this safety came with a trade-off: a proliferation of terragrunt.hcl files across your codebase. In the next step, you will eliminate this final piece of boilerplate by using Terragrunt Stacks, which allow you to generate entire collections of units on-demand from a single terragrunt.stack.hcl file. Edit page Previous Step 5: Adding Terragrunt Next Step 7: Taking advantage of Terragrunt Stacks",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.591Z"
  },
  {
    "title": "Step 7: Taking advantage of Terragrunt Stacks",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-7-taking-advantage-of-terragrunt-stacks/",
    "content": "Step 7: Taking advantage of Terragrunt Stacks Up until relatively recently in the history of Terragrunt, the proliferation of terragrunt.hcl files was the trade-off platform engineers had to accept for state segmentation. Luckily, with the advent of Terragrunt Stacks, that’s not the case anymore. Collections of units, like the ones you created in the last step, can be generated on-demand using terragrunt.stack.hcl files. This saves you from having to duplicate terragrunt.hcl files across your codebase. In this step, you will migrate to this modern pattern. You’ll start by persisting your unit definitions in your catalog. Then, you’ll replace the numerous terragrunt.hcl files in your live directory with a single terragrunt.stack.hcl file in each environment. To handle the slight configuration differences between dev and prod, you’ll use Terragrunt’s values attributes, which allow you to parameterize your reusable unit definitions. TutorialSection titled “Tutorial” Let’s start migrating to Terragrunt Stacks by persisting unit definitions in the catalog. Tip We copy over .terraform.lock.hcl files in addition to terragrunt.hcl files here. Persisting .terraform.lock.hcl files is a best practice to ensure reproducibility of units, and to maximize the performance of OpenTofu (as OpenTofu relies on the availability of this file when determining whether it can safely reuse content in the provider cache directory). mkdir -p catalog/units/{ddb, iam, lambda, s3}cp live/dev/ddb/{terragrunt.hcl, .terraform.lock.hcl} catalog/units/ddb/cp live/dev/iam/{terragrunt.hcl, .terraform.lock.hcl} catalog/units/iam/cp live/dev/lambda/{terragrunt.hcl, .terraform.lock.hcl} catalog/units/lambda/cp live/dev/s3/{terragrunt.hcl, .terraform.lock.hcl} catalog/units/s3/ You might remember that the unit configurations differed very slightly between dev and prod. Luckily, Terragrunt has special tooling to handle that via the usage of values variables. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//ddb\"} inputs = { name = values.name} By specifying values.name there, we’re allowing values to be used in our unit configurations from terragrunt.stack.hcl files. You’ll see how these values are set later in this step. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//iam\"} dependency \"s3\" { config_path = values.s3_path mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:s3:::mock-bucket-name\" }} dependency \"ddb\" { config_path = values.ddb_path mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:dynamodb:us-east-1:123456789012:table/mock-table-name\" }} inputs = { name = values.name aws_region = values.aws_region s3_bucket_arn = dependency.s3.outputs.arn dynamodb_table_arn = dependency.ddb.outputs.arn} Note We’re not just using the values variable for the inputs that differ between stacks, like the name and aws_region. We can use them for any value we want to substitute in our terragrunt.hcl files, including the relative paths to other units (which we might want to be dynamic if we’re going to refactor or add additional dependencies, etc.) include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//lambda\"} dependency \"s3\" { config_path = values.s3_path mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-bucket-name\" }} dependency \"ddb\" { config_path = values.ddb_path mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { name = \"mock-table-name\" }} dependency \"iam\" { config_path = values.iam_path mock_outputs_allowed_terraform_commands = [\"plan\", \"state\"] mock_outputs_merge_strategy_with_state = \"shallow\" mock_outputs = { arn = \"arn:aws:iam::123456789012:role/mock-role-name\" }} inputs = { name = values.name aws_region = values.aws_region s3_bucket_name = dependency.s3.outputs.name dynamodb_table_name = dependency.ddb.outputs.name lambda_role_arn = dependency.iam.outputs.arn lambda_zip_file = \"${get_repo_root()}/dist/best-cat.zip\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"${find_in_parent_folders(\"catalog/modules\")}//s3\"} inputs = { name = values.name # Optional: Force destroy S3 buckets even when they have objects in them. force_destroy = try(values.force_destroy, false)} Now we can replace the terragrunt.hcl files in live with a single terragrunt.stack.hcl file in each environment to generate them on-demand using unit blocks. By default, units generated by Terragrunt are generated into .terragrunt-stack directories. We opt out of that by setting no_dot_terragrunt_stack to true. locals { name = \"best-cat-2025-09-24-2359-dev\" aws_region = \"us-east-1\" units_path = find_in_parent_folders(\"catalog/units\")} unit \"ddb\" { source = \"${local.units_path}/ddb\" path = \"ddb\" no_dot_terragrunt_stack = true values = { name = local.name }} unit \"s3\" { source = \"${local.units_path}/s3\" path = \"s3\" no_dot_terragrunt_stack = true values = { name = local.name # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true }} unit \"iam\" { source = \"${local.units_path}/iam\" path = \"iam\" no_dot_terragrunt_stack = true values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" }} unit \"lambda\" { source = \"${local.units_path}/lambda\" path = \"lambda\" no_dot_terragrunt_stack = true values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" iam_path = \"../iam\" }} We’ll also add this .gitignore file to avoid recommitting the generated files in our repository, as they’ll be regenerated whenever we need them. We’ll see how we can remove the need for this in a future step. *!.gitignore!terragrunt.stack.hcl Now that we can generate these unit configurations on demand, we can remove the copies that we created manually! rm -rf .terraform.lock.hcl ddb iam lambda s3terragrunt run --all plan All that’s left now is to repeat the same thing for prod. locals { name = \"best-cat-2025-09-24-2359\" aws_region = \"us-east-1\" units_path = find_in_parent_folders(\"catalog/units\")} unit \"ddb\" { source = \"${local.units_path}/ddb\" path = \"ddb\" no_dot_terragrunt_stack = true values = { name = local.name }} unit \"s3\" { source = \"${local.units_path}/s3\" path = \"s3\" no_dot_terragrunt_stack = true values = { name = local.name # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true }} unit \"iam\" { source = \"${local.units_path}/iam\" path = \"iam\" no_dot_terragrunt_stack = true values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" }} unit \"lambda\" { source = \"${local.units_path}/lambda\" path = \"lambda\" no_dot_terragrunt_stack = true values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" iam_path = \"../iam\" }} *!.gitignore!terragrunt.stack.hcl rm -rf .terraform.lock.hcl ddb iam lambda s3terragrunt run --all plan Project Layout Check-inSection titled “Project Layout Check-in” If you clean out the .gitignore’ed files and take a look at the file tree, you should see that your live file count has shrunk down again! rm -rf ./***/ddb ./***/iam ./***/lambda ./***/s3 Directorylive Directorydev terragrunt.stack.hclDirectoryprod terragrunt.stack.hclroot.hcl Trade-offsSection titled “Trade-offs” You’ve now adopted one of Terragrunt’s most advanced features, Terragrunt Stacks, to achieve an exceptionally clean and DRY (Don’t Repeat Yourself) infrastructure codebase. By generating your component configurations on the fly from a central catalog, you’ve eliminated the last major source of boilerplate in your Terragrunt configurations. However, this abstraction comes with its own set of trade-offs. ProsSection titled “Pros” Maximum Reusability and Deduplication: This is the most significant benefit of using Terragrunt Stacks. Instead of having multiple terragrunt.hcl files scattered across each environment’s subdirectories, you now have a single, reusable unit definition for each component in your catalog. Adding a new environment is as simple as creating a new terragrunt.stack.hcl and defining its unique inputs. Simplified live Directory: Your live directory is now incredibly lean and easy to navigate. Each environment is represented by a single terragrunt.stack.hcl file, which serves as a clear manifest of all the components that make up that environment. This is a similar layout to that which was achieved in step 5, but we’ve gained the ability to retain state segmentation and operate granularly. Centralized Configuration Catalog: If you need to update the configuration for a component across all environments (e.g., add a new dependency or change a mock_output), you only need to edit the corresponding file in catalog/units. This drastically reduces the chance of configuration drift and makes maintenance much easier. ConsSection titled “Cons” Increased Abstraction: The biggest trade-off is the added layer of indirection. Engineers no longer have all the configuration for their Terragrunt units committed to their repository. If they want to read through their configurations, they need to generate the stack, or read the contents stored in their catalog. Steeper Learning Curve: The concepts of terragrunt.stack.hcl, unit blocks, and the values attribute are powerful but are also more advanced Terragrunt features. Onboarding new team members may require more time to explain this higher level of abstraction compared to the more explicit file-based approach from the previous step. Wrap UpSection titled “Wrap Up” You’ve conquered the final major source of Terragrunt boilerplate! In this step, you adopted one of Terragrunt’s most powerful features: Terragrunt Stacks. By centralizing your generic unit configurations into the catalog, you were able to replace the numerous terragrunt.hcl files in each environment with a single, clean terragrunt.stack.hcl file. The core concept you mastered was using the values object to parameterize these reusable units, allowing you to define a component’s configuration once and deploy it many times with environment-specific values. However, there’s one piece of technical debt left from our migration: the state paths in your S3 backend don’t yet align with Terragrunt’s default conventions, requiring those annoying .gitignore files. In the final step, you’ll perform one last set of state migrations to finalize your layout, fully mastering this guide. Edit page Previous Step 6: Breaking the Terralith Further Next Step 8: Refactoring state with Terragrunt Stacks",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.632Z"
  },
  {
    "title": "Step 8: Refactoring state with Terragrunt Stacks",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/step-8-refactoring-state-with-terragrunt-stacks/",
    "content": "Step 8: Refactoring state with Terragrunt Stacks You’ve just completed a major refactor using Terragrunt Stacks. However, there’s one final piece of technical debt remaining to complete this guide. To make the transition in the previous step smoother, we used the no_dot_terragrunt_stack attribute, which generated the unit configurations directly into directories like dev/s3 and prod/lambda. While this worked perfectly fine for our migration, and is a recommended first step to adopting Terragrunt Stacks, it’s not the standard configuration you would arrive at if you wrote the configurations by hand. By default, Terragrunt generates unit configurations into a hidden .terragrunt-stack directory within each environment. This keeps your generated code is neatly tucked away and easily ignored by version control. Our current setup requires .gitignore files in each stack directory to prevent committing this generated code. In this final step, you will perform one last state migration to align your project with Terragrunt’s best practices. You will remove the no_dot_terragrunt_stack attribute and move your state to match the default, conventional directory structure. TutorialSection titled “Tutorial” To review, this is what our S3 layout looks like for our state (ignoring the state that we’ve left behind during our refactors): $ aws s3 ls --recursive s3://[your-state-bucket] | awk '{print $4}' | rg -v '^tofu.tfstate$' | rg -v '^dev/tofu.tfstate$' | rg -v '^prod/tofu.tfstate$'dev/ddb/tofu.tfstatedev/iam/tofu.tfstatedev/lambda/tofu.tfstatedev/s3/tofu.tfstateprod/ddb/tofu.tfstateprod/iam/tofu.tfstateprod/lambda/tofu.tfstateprod/s3/tofu.tfstate What we’d like our state keys to look like is the following, which is how it would look if we provisioned our stack without usage of no_dot_terragrunt_stack from the beginning: dev/.terragrunt-stack/ddb/tofu.tfstatedev/.terragrunt-stack/iam/tofu.tfstatedev/.terragrunt-stack/lambda/tofu.tfstatedev/.terragrunt-stack/s3/tofu.tfstateprod/.terragrunt-stack/ddb/tofu.tfstateprod/.terragrunt-stack/iam/tofu.tfstateprod/.terragrunt-stack/lambda/tofu.tfstateprod/.terragrunt-stack/s3/tofu.tfstate Given that there’s a close relationship between filesystem layout and backend state keys, we can achieve this by having our units generated into the default .terragrunt-stack directories instead of being generated directly adjacent to terragrunt.stack.hcl files. What we’ll want to do is perform state migration while having both unit layouts generated locally. If you remember earlier steps, the way to that this is to use the state pull and state push commands. First, let’s make sure we have our stack generated as-is without removing the no_dot_terragrunt_stack attribute. terragrunt stack generate16:36:50.794 INFO Generating stack from ./dev/terragrunt.stack.hcl16:36:50.797 INFO Generating stack from ./prod/terragrunt.stack.hcl16:36:50.798 INFO Processing unit s3 from ./dev/terragrunt.stack.hcl16:36:50.798 INFO Processing unit ddb from ./dev/terragrunt.stack.hcl16:36:50.798 INFO Processing unit lambda from ./dev/terragrunt.stack.hcl16:36:50.798 INFO Processing unit iam from ./dev/terragrunt.stack.hcl16:36:50.798 INFO Processing unit lambda from ./prod/terragrunt.stack.hcl16:36:50.798 INFO Processing unit iam from ./prod/terragrunt.stack.hcl16:36:50.798 INFO Processing unit ddb from ./prod/terragrunt.stack.hcl16:36:50.798 INFO Processing unit s3 from ./prod/terragrunt.stack.hcl Now let’s edit our terragrunt.stack.hcl files to remove the no_dot_terragrunt_stack attribute. This will generate units into the desired final directory structure. locals { name = \"best-cat-2025-09-24-2359-dev\" aws_region = \"us-east-1\" units_path = find_in_parent_folders(\"catalog/units\")} unit \"ddb\" { source = \"${local.units_path}/ddb\" path = \"ddb\" values = { name = local.name }} unit \"s3\" { source = \"${local.units_path}/s3\" path = \"s3\" values = { name = local.name # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true }} unit \"iam\" { source = \"${local.units_path}/iam\" path = \"iam\" values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" }} unit \"lambda\" { source = \"${local.units_path}/lambda\" path = \"lambda\" values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" iam_path = \"../iam\" }} locals { name = \"best-cat-2025-09-24-2359\" aws_region = \"us-east-1\" units_path = find_in_parent_folders(\"catalog/units\")} unit \"ddb\" { source = \"${local.units_path}/ddb\" path = \"ddb\" values = { name = local.name }} unit \"s3\" { source = \"${local.units_path}/s3\" path = \"s3\" values = { name = local.name # Optional: Force destroy S3 buckets even when they have objects in them. # You're generally advised not to do this with important infrastructure, # however this makes testing and cleanup easier for this guide. force_destroy = true }} unit \"iam\" { source = \"${local.units_path}/iam\" path = \"iam\" values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" }} unit \"lambda\" { source = \"${local.units_path}/lambda\" path = \"lambda\" values = { name = local.name aws_region = local.aws_region s3_path = \"../s3\" ddb_path = \"../ddb\" iam_path = \"../iam\" }} Now let’s generate again to get that generated .terragrunt-stack directory. terragrunt stack generate Project Layout Check-inSection titled “Project Layout Check-in” Should see a layout like the following now, with both a stack generated within .terragrunt-stack and one generated outside of it: Directorylive Directorydev Directory.terragrunt-stackDirectoryddb.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryiam.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorylambda.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorys3.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryddb .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryiam .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorylambda .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorys3 .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclterragrunt.stack.hclDirectoryprod Directory.terragrunt-stackDirectoryddb.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryiam.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorylambda.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorys3.terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryddb .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectoryiam .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorylambda .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclDirectorys3 .terraform.lock.hcl.terragrunt-stack-manifestterragrunt.hclterragrunt.values.hclterragrunt.stack.hcl Applying UpdatesSection titled “Applying Updates” To migrate state from the old unit paths to the new paths, we can run the following: # Migrate dev statecd dev/ddbterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/ddbterragrunt state push /tmp/tofu.tfstatecd ../../s3terragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/s3terragrunt state push /tmp/tofu.tfstatecd ../../iamterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/iamterragrunt state push /tmp/tofu.tfstatecd ../../lambdaterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/lambdaterragrunt state push /tmp/tofu.tfstate # Migrate prod statecd ../../../prod/ddbterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/ddbterragrunt state push /tmp/tofu.tfstatecd ../../s3terragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/s3terragrunt state push /tmp/tofu.tfstatecd ../../iamterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/iamterragrunt state push /tmp/tofu.tfstatecd ../../lambdaterragrunt state pull > /tmp/tofu.tfstatecd ../.terragrunt-stack/lambdaterragrunt state push /tmp/tofu.tfstate We can now remove the .gitignore files, and prove to ourselves that state has migrated successfully! rm -rf ./***/.gitignore ./***/ddb ./***/iam ./***/lambda ./***/s3terragrunt run --all plan # No changes! Trade-offsSection titled “Trade-offs” This final refactor brings your project into alignment with Terragrunt’s standard conventions, but there are some minor trade-offs to consider. ProsSection titled “Pros” Cleaner Working Directory: The most significant advantage is the cleanliness of your live directory. All generated code now resides in a hidden .terragrunt-stack directory, leaving your environment folders (e.g., live/dev) containing only your manually-managed terragrunt.stack.hcl file. Simplified Version Control: You can now remove the environment-specific .gitignore files. A single, global entry to ignore .terragrunt-stack and .terragrunt-cache is all that’s needed, making your version control rules simpler and more reliable. ConsSection titled “Cons” State Migration: The primary cost is the one-time effort of performing the state migration. While powerful, any direct state manipulation requires careful execution to avoid errors. This refactor is an investment of time and attention to detail. Tooling Requirements: If you currently use a CI/CD tool that supports Terragrunt, it has to have built-in awareness of how Terragrunt Stack generation, like Gruntwork Pipelines. CI/CD tools that have been around for a long while might not prioritize handling stack generation, and lack support as a consequence. Placing all generated units in a .gitignore file, CI/CD tools might not be able to track when units change, and make selective changes to IaC. Wrap UpSection titled “Wrap Up” This final step was about aligning with Terragrunt’s standard conventions. By removing the no_dot_terragrunt_stack attribute, you enabled Terragrunt’s default behavior of generating code into a hidden .terragrunt-stack directory. This required one last, careful state migration. You used terragrunt state pull to download state from old unit keys and terragrunt state push to the new, conventional backend keys that matched the updated directory structure from stack generation. Your project is now not only easy to manage but also immediately familiar to any engineer experienced with Terragrunt, featuring a state backend structure aligned with your filesystem. Edit page Previous Step 7: Taking advantage of Terragrunt Stacks Next Wrap Up",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.686Z"
  },
  {
    "title": "Wrap Up",
    "url": "https://terragrunt.gruntwork.io/docs/guides/terralith-to-terragrunt/wrap-up/",
    "content": "Wrap Up Congratulations on making it through the Terralith to Terragrunt guide! You’ve successfully navigated the entire journey from a cumbersome Terralith to a clean, scalable, and maintainable IaC setup managed by Terragrunt. This process mirrors the real-world challenges many teams face as their infrastructure grows, and the skills you’ve developed are invaluable for managing IaC effectively at any scale. Along the way, you’ve practiced and mastered critical techniques that will serve you well on your path to IaC mastery. You now have hands-on experience with: Advanced State Manipulation: Safely refactoring your infrastructure’s state without destroying and recreating critical resources. You’ve used OpenTofu’s moved and removed blocks and state pull and state push commands to migrate state between different file system layouts and backend configurations. Modular Infrastructure Design: Breaking down a monolithic configuration into smaller, reusable modules with clear, encapsulated APIs. You learned to structure your project with a catalog of components that can be consumed by your live infrastructure. Boilerplate Reduction with Terragrunt: Eliminating repetitive code by using terragrunt.hcl to generate provider and backend configurations, define module sources, and pass inputs dynamically using functions like get_repo_root() and path_relative_to_include(). Dependency Management: Orchestrating complex deployments across multiple, isolated state files. You learned to use Terragrunt’s dependency blocks to pass outputs from one component as inputs to another, and how to use mock_outputs to enable successful planning even before dependencies have been applied. Dynamic Stack Generation: Transitioning from manually managing individual terragrunt.hcl files to defining entire environments on-demand with a single terragrunt.stack.hcl file, making your infrastructure definitions radically simpler and easier to maintain. If you’d like to continue practicing on your own, consider how you might continue experimenting with this configuration. Nested Stacks: What if you wanted to manage both the dev and prod environments from a single, top-level file using the Environment-based Stacks pattern? Think about how you would refactor your current setup to achieve this. Would you need to perform more state manipulation? If so, which techniques from this guide would you leverage to accomplish it safely? Tip Review the state migration steps throughout this guide! Integrating Terragrunt Into Your Build System: In this guide, the artifact that’s deployed to Lambda is manually packaged by you before provisioning any infrastructure. Terragrunt has special tooling in hooks to automate manual tasks like this for you on-demand as you interact with your infrastructure. How would you integrate that into the configurations you’ve built so far? Tip You might find an example of this in our example catalog! Testing Our IaC: If you were to use this infrastructure in production, you might want the underlying patterns (and perhaps the live infrastructure) tested automatically, rather than just applying the IaC and manually verifying that it is working correctly. How would you go about doing that? Tip You might find a helpful tool to accomplish that in Terratest! Automating Plan Assessments: Throughout this guide, you engaged in careful manual evaluation of plans, and it was hinted early on that tools like OPA provide a way to automatically assess the risk of plans. How would you integrate them into Terragrunt? Tip You might find it useful to review the documentation on hooks! This was a simple example, but if you’re struggling with a Terralith, your use-case is almost definitely more complex! Gruntwork offers support for assisting customers transition out of Terraliths into maintainable, best practices IaC codebases. Send an email to sales@gruntwork.io for Terragrunt Enterprise Support for more information. Edit page Previous Step 8: Refactoring state with Terragrunt Stacks",
    "section": "guides",
    "lastUpdated": "2025-10-23T16:40:57.722Z"
  },
  {
    "title": "Units",
    "url": "https://terragrunt.gruntwork.io/docs/features/units/",
    "content": "Units MotivationSection titled “Motivation” Consider the following file structure in a typical OpenTofu/Terraform project, which defines three environments (prod, qa, stage) with the same infrastructure in each one (an app, a MySQL database, and a VPC): Directorylive Directoryprod Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectoryqa Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectorystage Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf The contents of each environment could be more or less identical, except perhaps for a few settings (e.g. the prod environment may run bigger or more servers). As the size of the infrastructure grows, having to maintain all of this duplicated code between environments becomes more error prone. You can reduce the amount of copy paste using OpenTofu/Terraform modules, but even the code to instantiate a module and set up input variables, output variables, providers, and remote state can still create a lot of maintenance overhead. How can you keep your OpenTofu/Terraform code DRY so that you can maximize code reuse and minimize maintenance overhead? Moreover, how can you ensure that you are reproducing as close to the same infrastructure as possible across environments, so that you can be confident that what you test in qa will work when you deploy to prod? Terragrunt unitsSection titled “Terragrunt units” A unit in Terragrunt is a directory containing a terragrunt.hcl file. This hermetic unit of infrastructure is the smallest deployable entity in Terragrunt. It’s also the most important feature Terragrunt has. Units are designed to be contained, and can be operated on independently of other units. Infrastructure changes to units are also meant to be atomic. The interface you have with a unit is a single terragrunt.hcl file, and any change you make to it should result in one reproducible change to a limited subset of your infrastructure. Units are designed to work with immutable OpenTofu/Terraform modules. The OpenTofu/Terraform code referenced by a unit should be versioned, and that version of the module should be immutable. This ensures that the infrastructure you deploy is consistent across environments, and that you are confident you can reproduce the same pattern of infrastructure as many times as you need. Remote OpenTofu/Terraform modulesSection titled “Remote OpenTofu/Terraform modules” Terragrunt has the ability to download remote OpenTofu/Terraform configurations. The idea is that you define the OpenTofu/Terraform code for your infrastructure just once, in a single repo, called, for example, modules: Directorymodules Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf This repo contains typical OpenTofu/Terraform code, with one difference: anything in your code that should be different between environments should be exposed as an input variable. For example, the app module might expose the following variables: variable \"instance_count\" { description = \"How many servers to run\"}variable \"instance_type\" { description = \"What kind of servers to run (e.g. t3.large)\"} These variables allow you to run smaller/fewer servers in qa and stage to save money and larger/more servers in prod to ensure availability and scalability. They also define the variability of this infrastructure pattern. When instantiating the app module as a Terragrunt unit, you can be fairly confident that the only variance you are likely to see between environments is in the values of these variables. In a separate repo, called, for example, live, you define the code for all of your environments, which now consists of just one terragrunt.hcl file per unit (e.g. app/terragrunt.hcl, mysql/terragrunt.hcl, etc). This gives you the following file layout: Directorylive Directoryprod Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Notice how there are no OpenTofu/Terraform configurations (.tf files) in any of the folders. Instead, each terragrunt.hcl file specifies a terraform { …​ } block that specifies from where to download the OpenTofu/Terraform code, as well as the environment-specific values for the input variables in that OpenTofu/Terraform code. For example, stage/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.3 in stage source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"} inputs = { instance_count = 3 instance_type = \"t4g.micro\"} (Note: the double slash (//) in the source parameter is intentional and required. It’s part of OpenTofu/Terraform’s Git syntax for module sources. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) And prod/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.1 in prod source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.1\"} inputs = { instance_count = 10 instance_type = \"m8g.large\"} You can now deploy the modules in your live repo. For example, to deploy the app module in stage, you would do the following: cd live/stage/appterragrunt apply When Terragrunt finds the terraform block with a source parameter in live/stage/app/terragrunt.hcl file, it will: Download the configurations specified via the source parameter into the --download-dir folder (by default .terragrunt-cache in the working directory, which we recommend adding to .gitignore). This downloading is done by using the same go-getter library OpenTofu/Terraform uses, so the source parameter supports the same syntax as the module source parameter, including local file paths, Git URLs, and Git URLs with ref parameters (useful for checking out a specific tag, commit, or branch of Git repo). Terragrunt will download all the code in the repo (i.e. the part before the double-slash //) so that relative paths work correctly between modules in that repo. Copy all files from the current working directory into the temporary folder. Execute whatever OpenTofu/Terraform command you specified in that temporary folder (assuming you are performing a run). Set any variables defined in the inputs = { …​ } block as environment variables (prefixed with TF_VAR_) before running your OpenTofu/Terraform code. Notice how the inputs block in stage/app/terragrunt.hcl deploys fewer and smaller instances than prod. Check out the terragrunt-infrastructure-modules-example and terragrunt-infrastructure-live-example repos for fully-working sample code that demonstrates our recommended folder structure for successful infrastructure management. Immutable modules and atomic deploymentsSection titled “Immutable modules and atomic deployments” With this approach, copy/paste between environments is minimized. The terragrunt.hcl files contain solely the source URL of the module to deploy and the inputs to set for that module in the current environment. To create a new unit, you copy an old one and update just the environment-specific inputs in the terragrunt.hcl files, which is about as close to the “essential complexity” of the problem as you can get. Just as importantly, since the OpenTofu/Terraform module code is now defined in a single repo, you can version it (e.g., using Git tags and referencing them using the ref parameter in the source URL, as in the stage/app/terragrunt.hcl and prod/app/terragrunt.hcl examples above), and promote a single, immutable version through each environment (e.g., qa → stage → prod). This is especially powerful when thinking about how the pattern is deployed. Because all of the configuration for a unit is defined using a versioned URL and a set of inputs, it’s easy to reliably promote an infrastructure change across environments as one atomic change. It’s also easy to roll back to a previous version of the infrastructure by changing the ref parameter in the source URL. This idea is inspired by Kief Morris’ blog post Using Pipelines to Manage Environments with Infrastructure as Code. Working locallySection titled “Working locally” If you’re testing changes to a local copy of the modules repo, you can use the --source command-line option or the TG_SOURCE environment variable to override the source parameter. This is useful to point Terragrunt at a local checkout of your code so you can do rapid, iterative, make-a-change-and-rerun development: cd live/stage/appterragrunt apply --source ../../../modules//app (Note: the double slash (//) here too is intentional and required. Terragrunt downloads all the code in the folder before the double-slash into the temporary folder so that relative paths between modules work correctly. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) Working with lock filesSection titled “Working with lock files” Terraform 0.14 introduced lock files. These should mostly “just work” with Terragrunt version v0.27.0 and above: that is, the lock file (.terraform.lock.hcl) will be generated next to your terragrunt.hcl, and you should check it into version control. See the Lock File Handling docs for more details. Terragrunt cachingSection titled “Terragrunt caching” The first time you set the source parameter to a remote URL, Terragrunt will download the code from that URL into a tmp folder. It will NOT download it again afterwards unless you change that URL. That’s because downloading code—and more importantly, reinitializing remote state, redownloading provider plugins, and redownloading modules—can take a long time. To avoid adding 10-90 seconds of overhead to every Terragrunt command, Terragrunt assumes all remote URLs are immutable, and only downloads them once. Therefore, when working locally, you should use the --source parameter and point it at a local file path as described in the previous section. Terragrunt will copy the local files every time you run it, which is nearly instantaneous, and doesn’t require reinitializing everything, so you’ll be able to iterate quickly. If you need to force Terragrunt to redownload something from a remote URL, run Terragrunt with the --source-update flag, and it’ll delete the tmp folder, download the files from scratch, and reinitialize everything. This can take a while, so avoid it and use --source when you can! Working with relative file pathsSection titled “Working with relative file paths” One of the gotchas with downloading OpenTofu/Terraform configurations is that when you run terragrunt apply in folder foo, OpenTofu/Terraform will actually run in some temporary folder such as .terragrunt-cache/foo. That means you have to be especially careful with relative file paths, as they will be relative to that temporary folder and not the folder where you ran Terragrunt! In particular: Command line: When using file paths on the command line, such as passing an extra -var-file argument, you should use absolute paths: # Use absolute file paths on the CLI!terragrunt apply -var-file /foo/bar/extra.tfvars# Or use the PWD environment variable to construct# an absolute path before passing it to Terragrunt# $ terragrunt apply -var-file \"$PWD/extra.tfvars\" Terragrunt configuration: When using file paths directly in your Terragrunt configuration (terragrunt.hcl), such as in an extra_arguments block, you can’t use hard-coded absolute file paths, or it won’t work on your teammates’ computers. Therefore, you should utilize the Terragrunt built-in function get_terragrunt_dir() to use a relative file path: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] # With the get_terragrunt_dir() function, you can use relative paths! arguments = [ \"-var-file=${get_terragrunt_dir()}/../common.tfvars\", \"-var-file=example.tfvars\" ] }} See the get_terragrunt_dir() documentation for more details. Using Terragrunt with private Git reposSection titled “Using Terragrunt with private Git repos” The easiest way to use Terragrunt with private Git repos is to use SSH authentication. Configure your Git account so you can use it with SSH (see the guide for GitHub here) and use the SSH URL for your repo: terraform { source = \"git@github.com:foo/modules.git//path/to/module?ref=v0.0.1\"} Look up the Git repo for your repository to find the proper format. Note: In automated pipelines, you may need to run the following command for your Git repository prior to calling terragrunt to ensure that the ssh host is registered locally, e.g.: ssh -T -oStrictHostKeyChecking=accept-new git@github.com || true Generate blocksSection titled “Generate blocks” In an ideal world, all that units do would be to run versioned, immutable OpenTofu/Terraform modules with environment-specific inputs. In the real world, however, certain scenarios arise when you have to inject additional configurations to the immutable OpenTofu/Terraform modules you use. This is where generate blocks prove useful. When you define a generate block, Terragrunt will do the following before any run: Fetch any module referenced in a source URL in the terraform block into the .terragrunt-cache folder (if there is none, it will run in the current working directory). Generate the file specified in the generate block into the directory where Terragrunt will run OpenTofu/Terraform. Run the OpenTofu/Terraform command. The most common example of this is to dynamically generate a provider.tf file that includes provider configurations. Most OpenTofu/Terraform modules are authored in such a way that defining a provider is an exercise left to the consumer of the module. This is a good practice, as it allows the consumer to define the provider configuration in a way that suits their needs, and it may not make sense for a nested module to define a provider configuration that is not used by the consumer. Consider a setup where you want to always assume a specific role when calling out to a given OpenTofu/Terraform module. Not all modules expose the right variables for configuring the aws provider so that you can assume the role through OpenTofu/Terraform. In this situation, you can use Terragrunt generate blocks to generate a tf file called provider.tf that includes the provider configuration. Add an env.hcl file for each of the environments in the file layout for the live repo: Directorylive Directoryprod env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Each env.hcl file (the one at the environment level, e.g prod/env.hcl) should define a generate block to generate the AWS provider configuration to assume the role for that environment. For example, if you wanted to assume the role arn:aws:iam::0123456789:role/terragrunt in all the units for the prod account, you would put the following in prod/env.hcl: generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { assume_role { role_arn = \"arn:aws:iam::0123456789:role/terragrunt\" }}EOF} This instructs Terragrunt to create the file provider.tf in the working directory where Terragrunt calls tofu/terraform before it runs any of the OpenTofu/Terraform commands (e.g plan, apply, validate, etc). This allows you to inject this provider configuration for any unit that includes the env.hcl file. To include this in the child configurations (e.g app/terragrunt.hcl), you would update all the units to include this configuration using the include block: include \"env\" { path = find_in_parent_folders(\"env.hcl\")} The include block tells Terragrunt to use the exact same Terragrunt configuration from the env.hcl file specified via the path parameter. It behaves exactly as if you had copy/pasted the OpenTofu/Terraform configuration from the included file generate configuration into the child, but this approach is much easier to maintain! Further ReadingSection titled “Further Reading” Note that if you’re considering this solution because you’re struggling with dynamic provider authentication in AWS, you may be interested in the dedicated documentation on working with multiple AWS accounts. Edit page Previous Wrap Up Next Stacks",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:57.784Z"
  },
  {
    "title": "Stacks",
    "url": "https://terragrunt.gruntwork.io/docs/features/stacks/",
    "content": "Stacks What are Stacks?Section titled “What are Stacks?” A stack in Terragrunt is a collection of related units that can be managed together. Stacks provide a way to: Deploy multiple infrastructure components with a single command Manage dependencies between units automatically Control the blast radius of changes Organize infrastructure into logical groups Terragrunt supports two approaches to defining stacks: Implicit Stacks: Created by organizing units in a directory structure. Explicit Stacks: Defined using terragrunt.stack.hcl files. Implicit Stacks: Directory-Based OrganizationSection titled “Implicit Stacks: Directory-Based Organization” The simplest way to create a stack is to organize your units in a directory structure. When you have multiple units in a directory, Terragrunt automatically treats that directory as a stack. Converting Terraform Modules to UnitsSection titled “Converting Terraform Modules to Units” Let’s say your infrastructure is defined across multiple OpenTofu/Terraform root modules: Directoryroot Directorybackend-app main.tfDirectoryfrontend-app main.tfDirectorymysql main.tfDirectoryvalkey main.tfDirectoryvpc main.tf To convert these to Terragrunt units, simply add a terragrunt.hcl file to each directory: Directoryroot Directorybackend-app main.tfterragrunt.hclDirectoryfrontend-app main.tfterragrunt.hclDirectorymysql main.tfterragrunt.hclDirectoryvalkey main.tfterragrunt.hclDirectoryvpc main.tfterragrunt.hcl Now you have an implicit stack! The root directory contains all your units and can be managed as a single stack. Working with Implicit StacksSection titled “Working with Implicit Stacks” Use the --all flag to run an OpenTofu/Terraform command on all units in the implicit stack in the current working directory: # Deploy all units discovered in the current working directoryterragrunt run --all apply # Plan changes across all units discovered in the current working directoryterragrunt run --all plan # Destroy all units discovered in the current working directoryterragrunt run --all destroy # View outputs from all units discovered in the current working directoryterragrunt run --all output You can also use the --graph flag to run an OpenTofu/Terraform command on all units in the DAG of the unit in the current working directory. # Run an OpenTofu/Terraform command on all units in the DAG of the unit in the current working directoryterragrunt run --graph apply Advantages of Implicit StacksSection titled “Advantages of Implicit Stacks” Simple: Just organize units in directory trees. Familiar: Works like recommended best practices for OpenTofu/Terraform module organization. Flexible: Easy to add/remove units by creating/deleting directories. Version Control Friendly: Each unit is a separate directory with its own history. Backwards Compatible: This has been the default way to work with Terragrunt for over eight years, and the majority of existing Terragrunt configurations use this approach. Limitations of Implicit StacksSection titled “Limitations of Implicit Stacks” Manual Management: Each unit must be manually created and configured. No Reusability: Patterns can’t be easily shared across environments. Repetitive: Similar configurations must be duplicated or referenced from includes. Explicit Stacks: Blueprint-Based GenerationSection titled “Explicit Stacks: Blueprint-Based Generation” For a more modern approach, you can define stacks using terragrunt.stack.hcl files. These are blueprints that programmatically generate units. What is a terragrunt.stack.hcl file?Section titled “What is a terragrunt.stack.hcl file?” A terragrunt.stack.hcl file is a blueprint that defines how to generate Terragrunt configuration programmatically. It tells Terragrunt: What units to create. Where to get their configurations from. Where to place them in the directory structure. What values to pass to each unit. The Two Types of BlocksSection titled “The Two Types of Blocks” unit blocks - Define Individual Infrastructure ComponentsSection titled “unit blocks - Define Individual Infrastructure Components” Purpose: Define a single, deployable piece of infrastructure. Use case: When you want to create a single piece of isolated infrastructure (e.g. a specific VPC, database, or application). Result: Generates a single terragrunt.hcl file in the specified path. stack blocks - Define Reusable Infrastructure PatternsSection titled “stack blocks - Define Reusable Infrastructure Patterns” Purpose: Define a collection of related units that can be reused. Use case: When you have a common, multi-unit pattern (like “dev environment” or “three-tier web application”) that you want to deploy multiple times. Result: Generates another terragrunt.stack.hcl file that can contain more units or stacks. Comparison: unit vs stack blocksSection titled “Comparison: unit vs stack blocks” Aspectunit blockstack blockPurposeDefine a single infrastructure component.Define a reusable collection of components.When to useFor specific, one-off infrastructure pieces.For patterns of infrastructure pieces that you want provisioned together.Generated outputA directory with a single terragrunt.hcl file.A directory with a terragrunt.stack.hcl file. Example: Simple Stack with UnitsSection titled “Example: Simple Stack with Units” unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { engine = \"postgres\" version = \"13\" vpc_path = \"../vpc\" }} Running terragrunt stack generate creates: terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.hclterragrunt.values.hclDirectorydatabase terragrunt.hclterragrunt.values.hcl Example: Nested Stack with Reusable PatternsSection titled “Example: Nested Stack with Reusable Patterns” stack \"dev\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"dev\" values = { environment = \"development\" cidr = \"10.0.0.0/16\" }} stack \"prod\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"prod\" values = { environment = \"production\" cidr = \"10.1.0.0/16\" }} The referenced stack might contain: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = values.environment cidr = values.cidr }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { environment = values.environment vpc_path = \"../vpc\" }} Working with Explicit StacksSection titled “Working with Explicit Stacks” # Generate units from the `terragrunt.stack.hcl` file in the current working directory.terragrunt stack generate # Deploy all generated units defined using the `terragrunt.stack.hcl` file in the current working directory (and any units generated by stacks in this file).terragrunt stack run apply # Or combine generation and deployment.terragrunt stack run apply # Automatically generates first Advantages of Explicit StacksSection titled “Advantages of Explicit Stacks” Reusability: Define patterns once, use them across environments. Consistency: Ensure all environments follow the same structure. Version Control: Version collections of infrastructure patterns alongside the units of infrastructure that make them up. Automation: Generate complex infrastructure from simple blueprints. Flexibility: Easy to create variations with different values. Limitations of Explicit StacksSection titled “Limitations of Explicit Stacks” Complexity: Requires understanding another configuration file. Generation Overhead: Units must be generated before use. Debugging: Generated files can be harder to debug if you accidentally generate files that are not what you intended. Choosing Between Implicit and Explicit StacksSection titled “Choosing Between Implicit and Explicit Stacks” Use Implicit Stacks When:Section titled “Use Implicit Stacks When:” You have a small number of units (1-10). Each unit is unique and not repeated across environments. You don’t mind a high file count. You’re just getting started with Terragrunt. You need maximum flexibility and transparency. Use Explicit Stacks When:Section titled “Use Explicit Stacks When:” You have multiple environments (dev, staging, prod). You want to reuse infrastructure patterns. You have many similar units that differ only in values. You want to version your infrastructure patterns. You’re building infrastructure catalogs or templates. The Complete WorkflowSection titled “The Complete Workflow” For Implicit Stacks:Section titled “For Implicit Stacks:” Organize: Create directories for each unit with terragrunt.hcl files. Configure: Set up inputs, dependencies, etc. in each unit. Deploy: Use terragrunt run --all apply to deploy all units. For Explicit Stacks:Section titled “For Explicit Stacks:” Catalog: Create a catalog of infrastructure patterns (using terragrunt.hcl files, terragrunt.stack.hcl files, etc.) in a git repository. Author: Write a terragrunt.stack.hcl file with unit and/or stack blocks. Generate: Run terragrunt stack generate to create the actual units*. Deploy: Run terragrunt stack run apply to deploy all units**. * Multiple commands (like stack run or run --all) automatically generate units from terragrunt.stack.hcl files for you. ** You can also just use run --all apply to deploy all units in the stack like you can with implicit stacks. Common PatternsSection titled “Common Patterns” Note These are simplified examples that show common high-level patterns.For more detailed examples, see the Gruntwork Terragrunt Infrastructure Catalog Stack Examples. Environment-based StacksSection titled “Environment-based Stacks” Create reusable environment patterns that can be instantiated for dev, staging, and production: stack \"dev\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"dev\" values = { environment = \"development\" cidr = \"10.0.0.0/16\" }} stack \"staging\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"staging\" values = { environment = \"staging\" cidr = \"10.1.0.0/16\" }} stack \"prod\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"prod\" values = { environment = \"production\" cidr = \"10.2.0.0/16\" }} Application StacksSection titled “Application Stacks” Define complete application patterns with all required components: stack \"web-app\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/web-application?ref=v0.0.1\" path = \"web-app\" values = { app_name = \"my-web-app\" domain = \"example.com\" environment = \"production\" }} Benefits of Using StacksSection titled “Benefits of Using Stacks” Code ReusabilitySection titled “Code Reusability” Define infrastructure patterns once and reuse them across environments. Reduce duplication and maintenance overhead. Ensure consistency across deployments. Simplified ManagementSection titled “Simplified Management” Generate multiple units from a single configuration file. Deploy entire environments with a single command. Manage dependencies automatically. Version ControlSection titled “Version Control” Version your infrastructure patterns alongside your application code. Track changes to infrastructure patterns. Roll back infrastructure changes when needed. Team CollaborationSection titled “Team Collaboration” Share infrastructure patterns across teams. Standardize deployment approaches. Reduce onboarding time for new team members. Passing outputs between unitsSection titled “Passing outputs between units” Consider the following file structure: Directoryroot Directorybackend-app terragrunt.hclDirectorymysql terragrunt.hclDirectoryvalkey terragrunt.hclDirectoryvpc terragrunt.hcl Suppose that you wanted to pass in the VPC ID of the VPC that is created from the vpc unit in the folder structure above to the mysql unit as an input variable. Or that you wanted to pass in the subnet IDs of the private subnet that is allocated as part of the vpc unit. You can use the dependency block to extract those outputs and use them as inputs to the mysql unit. For example, suppose the vpc unit outputs the ID under the output named vpc_id. To access that output, you would specify in mysql/terragrunt.hcl: dependency \"vpc\" { config_path = \"../vpc\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} When you apply this unit, the output will be read from the vpc unit and passed in as an input to the mysql unit right before calling tofu apply/terraform apply. You can also specify multiple dependency blocks to access the outputs of multiple units. For example, in the above folder structure, you might want to reference the domain output of the valkey and mysql units for use as inputs in the backend-app unit. To access those outputs, you would specify the following in backend-app/terragrunt.hcl: dependency \"mysql\" { config_path = \"../mysql\"} dependency \"valkey\" { config_path = \"../valkey\"} inputs = { mysql_url = dependency.mysql.outputs.domain valkey_url = dependency.valkey.outputs.domain} Note that each dependency block results in a relevant status in the Terragrunt DAG. This means that when you run run --all apply on a config that has dependency blocks, Terragrunt will not attempt to deploy the config until all the units referenced in dependency blocks have been applied. So for the above example, the order for the run --all apply command would be: Deploy the VPC Deploy MySQL and valkey in parallel Deploy the backend-app If any of the units failed to deploy, then Terragrunt will not attempt to deploy the units that depend on them. Note: Not all blocks can access outputs passed by dependency blocks. See the section on Configuration parsing order for more information. Unapplied dependency and mock outputsSection titled “Unapplied dependency and mock outputs” Terragrunt will return an error if the unit referenced in a dependency block has not been applied yet. This is because you cannot actually fetch outputs out of an unapplied unit, even if there are no resources being created in the unit. This is most problematic when running commands that do not modify state (e.g run --all plan and run --all validate) on a completely new setup where no infrastructure has been deployed. You won’t be able to plan or validate a unit if you can’t determine the inputs. If the unit depends on the outputs of another unit that hasn’t been applied yet, you won’t be able to compute the inputs unless the dependencies are all applied. Of course, in real life usage, you typically need the ability to run run --all validate or run --all plan on a completely new set of infrastructure. To address this, you can provide mock outputs to use when a unit hasn’t been applied yet. This is configured using the mock_outputs attribute on the dependency block and it corresponds to a map that will be injected in place of the actual dependency outputs if the target config hasn’t been applied yet. Using a mock output is typically the best solution here, as you typically don’t actually care that an accurate value is used for a given value at this stage, just that it will plan successfully. When you actually apply the unit, that’s when you want to be sure that a real value is used. For example, in the previous scenario with a mysql unit and vpc unit, suppose you wanted to mock a value for the vpc_id during a run --all validate for the mysql unit. You can specify that in mysql/terragrunt.hcl: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"mock-vpc-id\" }} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} You can now run validate on this config before the vpc unit is applied because Terragrunt will use the map {vpc_id = \"mock-vpc-id\"} as the outputs attribute on the dependency instead of erroring out. What if you wanted to restrict this behavior to only the validate command? For example, you might not want to use the defaults for a plan operation because the plan doesn’t give you any indication of what is actually going to be created. You can use the mock_outputs_allowed_terraform_commands attribute to indicate that the mock_outputs should only be used when running those OpenTofu/Terraform commands. So to restrict the mock_outputs to only when validate is being run, you can modify the above terragrunt.hcl file to: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" } mock_outputs_allowed_terraform_commands = [\"validate\"]} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} Note that indicating validate means that the mock_outputs will be used either with validate or with run --all validate. You can also use skip_outputs on the dependency block to specify the dependency without pulling in the outputs: dependency \"vpc\" { config_path = \"../vpc\" skip_outputs = true} When skip_outputs is used with mock_outputs, mocked outputs will be returned without attempting to load outputs from OpenTofu/Terraform. This can be useful when you disable backend initialization (remote_state.disable_init) in CI for example. dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" } skip_outputs = true} You can also use mock_outputs_merge_strategy_with_state on the dependency block to merge mocked outputs and real outputs: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" new_output = \"temporary-dummy-value\" } mock_outputs_merge_strategy_with_state = \"shallow\"} If real outputs only contain vpc_id, dependency.outputs will contain a real value for vpc_id and a mocked value for new_output. Stack outputsSection titled “Stack outputs” When defining a stack using a terragrunt.stack.hcl file, you also have the ability to interact with the aggregated outputs of all the units in the stack. To do this, use the stack output command (not the stack run output command). $ terragrunt stack outputbackend_app = { domain = \"backend-app.example.com\"}frontend_app = { domain = \"frontend-app.example.com\"}mysql = { endpoint = \"terraform-20250504140737772400000001.abcdefghijkl.us-east-1.rds.amazonaws.com\"}valkey = { endpoint = \"serverless-valkey-01.amazonaws.com\"}vpc = { vpc_id = \"vpc-1234567890\"} This will return a single aggregated value for all the outputs of all the units in the stack. Dependencies between unitsSection titled “Dependencies between units” You can also specify dependencies without accessing any of the outputs of units. Consider the following file structure: Directoryroot Directorybackend-app terragrunt.hclDirectoryfrontend-app terragrunt.hclDirectorymysql terragrunt.hclDirectoryvalkey terragrunt.hclDirectoryvpc terragrunt.hcl Let’s assume you have the following dependencies between OpenTofu/Terraform units: backend-app depends on mysql, valkey, and vpc frontend-app depends on backend-app and vpc mysql depends on vpc valkey depends on vpc vpc has no dependencies You can express these dependencies in your terragrunt.hcl config files using a dependencies block. For example, in backend-app/terragrunt.hcl you would specify: dependencies { paths = [\"../vpc\", \"../mysql\", \"../valkey\"]} Similarly, in frontend-app/terragrunt.hcl, you would specify: dependencies { paths = [\"../vpc\", \"../backend-app\"]} Once you’ve specified these dependencies in each terragrunt.hcl file, Terragrunt will be able to perform updates respecting the DAG of dependencies. For the example at the start of this section, the order of runs for the run --all apply command would be: Deploy the VPC Deploy MySQL and valkey in parallel Deploy the backend-app Deploy the frontend-app Any error encountered in an individual unit during a run --all command will prevent Terragrunt from proceeding with the deployment of any dependent units. To check all of your dependencies and validate the code in them, you can use the run --all validate command. Note During destroy runs, Terragrunt will try to find all dependent units and show a confirmation prompt with a list of detected dependencies.This is because Terragrunt knows that once resources in a dependency are destroyed, any commands run on dependent units may fail.For example, if destroy was called on the Valkey unit, you’d be asked for confirmation, as the backend-app depends on Valkey. You can suppress the prompt by using the --non-interactive flag. Visualizing the DAGSection titled “Visualizing the DAG” To visualize the dependency graph you can use the dag graph command (similar to the terraform graph command). The graph is output in DOT format. The typical program used to render this file format is GraphViz, but many web services are available that can do this as well. terragrunt dag graph | dot -Tsvg > graph.svg The example above generates the following graph: Note that this graph shows the dependency relationship in the direction of the arrow, with the tip pointing to the dependency (e.g. frontend-app depends on backend-app). For most commands, Terragrunt will run in the opposite direction, however (e.g. backend-app would be applied before frontend-app). The exception to this rule is during the destroy (and plan -destroy) command, where Terragrunt will run in the direction of the arrow (e.g. frontend-app would be destroyed before backend-app). Testing multiple units locallySection titled “Testing multiple units locally” If you are using Terragrunt to download remote OpenTofu/Terraform modules and all of your units have the source parameter set to a Git URL, but you want to test with a local checkout of the code, you can use the --source parameter to override that value: terragrunt run --all plan --source /source/modules If you set the --source parameter, the run --all command will assume that parameter is pointing to a folder on your local file system that has a local checkout of all of your OpenTofu/Terraform modules. For each unit that is being processed via a run --all command, Terragrunt will: Read in the source parameter in that unit’s terragrunt.hcl file. Parse out the path (the portion after the double-slash). Append the path to the --source parameter to create the final local path for that unit. For example, consider the following terragrunt.hcl file: terraform { source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"} Running the following: terragrunt run --all apply --source /source/infrastructure-modules Will result in a unit with the configuration for the source above being resolved to /source/infrastructure-modules//networking/vpc. Limiting run parallelismSection titled “Limiting run parallelism” By default, Terragrunt will not impose a limit on the number of units it executes when it traverses the dependency graph, meaning that if it finds 5 units without dependencies, it’ll run OpenTofu/Terraform 5 times in parallel, once in each unit. Sometimes, this can create a problem if there are a lot of units in the dependency graph, like hitting a rate limit on a cloud provider. To limit the maximum number of unit executions at any given time use the --parallelism [number] flag terragrunt run --all apply --parallelism 4 Saving OpenTofu/Terraform plan outputSection titled “Saving OpenTofu/Terraform plan output” A powerful feature of OpenTofu/Terraform is the ability to save the result of a plan as a binary or JSON file using the -out flag. Terragrunt provides special tooling in run --all execution to ensure that the saved plan for a run --all against a stack has a corresponding entry for each unit in the stack in a directory structure that mirrors the stack structure. To save plan against a stack, use the --out-dir flag (or TG_OUT_DIR environment variable) as demonstrated below: $ terragrunt run --all plan --out-dir /tmp/tfplan Directoryapp1 tfplan.tfplanDirectoryapp2 tfplan.tfplanDirectoryapp3 tfplan.tfplanDirectoryproject-2 Directoryproject-2-app1 tfplan.tfplan $ terragrunt run --all --out-dir /tmp/tfplan apply For planning a destroy operation, use the following commands: terragrunt run --all --out-dir /tmp/tfplan plan -destroyterragrunt run --all --out-dir /tmp/tfplan apply To save plan in json format use --json-out-dir flag (or TG_JSON_OUT_DIR environment variable): terragrunt run --all --json-out-dir /tmp/json plan Directoryapp1 tfplan.jsonDirectoryapp2 tfplan.jsonDirectoryapp3 tfplan.jsonDirectoryproject-2 Directoryproject-2-app1 tfplan.json terragrunt run --all --out-dir /tmp/all --json-out-dir /tmp/all plan Directoryapp1 tfplan.jsontfplan.tfplanDirectoryapp2 tfplan.jsontfplan.tfplanDirectoryapp3 tfplan.jsontfplan.tfplanDirectoryproject-2 Directoryproject-2-app1 tfplan.jsontfplan.tfplan To recap: The plan for each unit in a stack is saved in the same hierarchy as the unit structure. The file name for plan binaries are tfplan.tfplan and tfplan.json for plan JSON. JSON plan files can’t be used with terragrunt run --all apply command, only binary plan files can be used. Output directories can be combined which will lead to saving both binary and JSON plans. Nested StacksSection titled “Nested Stacks” Note that you can also have nested stacks. For example, consider the following file structure: Directoryroot Directoryus-east-1 Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectoryus-west-2 Directoryapp terragrunt.hclDirectorydb terragrunt.hcl In this example, there’s the root stack, that contains all the infrastructure you’ve defined so far, and there’s also the us-east-1 and us-west-2 stacks, that contain the infrastructure for the app and db units in those regions. You can run run --all commands at any depth of the stack to run the units in that stack and all of its children. For example, to run all the units in the us-east-1 stack, you can run: cd root/us-east-1terragrunt run --all apply Terragrunt will only include the units in the us-east-1 stack and its children in the queue of units to run (unless external dependencies are pulled in, as discussed in the run —all command). Generally speaking, this is the primary tool Terragrunt users use to control the blast radius of their changes. For the most part, it is the current working directory that determines the blast radius of a run --all command. In addition to using your working directory to control what’s included in a run queue, you can also use flags like —include-dir and —exclude-dir to explicitly control what’s included in a run queue within a stack, or outside of it. There are more flags that control the behavior of the run command, which you can find in the run docs. Using Local State with StacksSection titled “Using Local State with Stacks” When using Terragrunt Stacks, you might want to use local state files instead of remote state for development, testing, or specific use cases. However, this presents a challenge because the generated .terragrunt-stack directory can be safely deleted and regenerated using terragrunt stack clean && terragrunt stack generate, which would normally cause local state files to be lost. To solve this problem, you can configure your stack to store state files outside of the .terragrunt-stack directory, in a persistent location that survives stack regeneration. ConfigurationSection titled “Configuration” Here’s how to configure local state that persists across stack regeneration: 1. Create a root.hcl file with local backend configuration: remote_state { backend = \"local\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { path = \"${get_parent_terragrunt_dir()}/.terragrunt-local-state/${path_relative_to_include()}/tofu.tfstate\" }} 2. Create your stack definition: unit \"vpc\" { source = \"${find_in_parent_folders(\"units/vpc\")}\" path = \"vpc\"} unit \"database\" { source = \"${find_in_parent_folders(\"units/database\")}\" path = \"database\"} unit \"app\" { source = \"${find_in_parent_folders(\"units/app\")}\" path = \"app\"} 3. Configure your units to include the root configuration: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \".\"} 4. Add a .gitignore file to exclude state files from version control: .terragrunt-local-state Important: Local state files should never be committed to version control as they may contain sensitive information and can cause conflicts when multiple developers work on the same infrastructure. How It WorksSection titled “How It Works” The key insight is using path_relative_to_include() in the state path configuration. This function returns the relative path from each unit to the root.hcl file, creating unique state file paths like: .terragrunt-local-state/live/.terragrunt-stack/vpc/tofu.tfstate.terragrunt-local-state/live/.terragrunt-stack/database/tofu.tfstate.terragrunt-local-state/live/.terragrunt-stack/app/tofu.tfstate Since these state files are stored in .terragrunt-local-state/ (outside of .terragrunt-stack/), they persist when you run: terragrunt stack clean && terragrunt stack generate Directory StructureSection titled “Directory Structure” After running the stack, your directory structure will look like this: Directory. root.hcl.gitignore (Excludes .terragrunt-local-state)Directory.terragrunt-local-state/ (Persistent state files - ignored by git) Directorylive/ Directory.terragrunt-stack/ Directoryvpc/ tofu.tfstateDirectorydatabase/ tofu.tfstateDirectoryapp/ tofu.tfstateDirectorylive/ terragrunt.stack.hclDirectory.terragrunt-stack/ (Generated stack - can be deleted) Directoryvpc/ terragrunt.hclmain.tfDirectorydatabase/ terragrunt.hclmain.tfDirectoryapp/ terragrunt.hclmain.tfDirectoryunits/ (Reusable unit definitions) Directoryvpc/…Directorydatabase/…Directoryapp/… BenefitsSection titled “Benefits” This approach provides several advantages: State Persistence: State files survive stack regeneration Isolation: Each unit gets its own state file Consistency: Directory structure mirrors the stack layout Flexibility: You can switch between local and remote state easily by changing the backend configuration Example WorkflowSection titled “Example Workflow” # Initial setupterragrunt stack generateterragrunt stack run apply # Later, regenerate the stack without losing stateterragrunt stack cleanterragrunt stack generate # Verify existing resources are recognizedterragrunt stack run plan # Should show \"No changes\" This pattern is particularly useful for development environments, testing scenarios, or when you need to maintain local state for compliance or security reasons while still benefiting from Terragrunt’s stack management capabilities. Next StepsSection titled “Next Steps” Now that you understand both implicit and explicit stacks, you can: Learn about the detailed syntax for unit and stack blocks Explore the stack commands for generating and managing stacks Understand how to pass values between units Tip Pro Tip: Start with implicit stacks to get familiar with the concept, then gradually introduce explicit stacks for reusable patterns as your infrastructure grows. Learning moreSection titled “Learning more” If you’d like more advanced examples on stacks, check out the terragrunt-infrastructure-catalog-example repository. These have full-featured examples of stacks that deploy real, stateful infrastructure in an AWS account. Edit page Previous Units Next Includes",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:57.859Z"
  },
  {
    "title": "Includes",
    "url": "https://terragrunt.gruntwork.io/docs/features/includes/",
    "content": "Includes MotivationSection titled “Motivation” As covered in Units and State Backend, it quickly becomes important to define base Terragrunt configuration files that are included in units. This is to ensure that all units have a consistent configuration, and to avoid repeating the same configuration across multiple units. For example, you might have a root Terragrunt configuration that defines the remote state and provider configurations for all your units: remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { assume_role { role_arn = \"arn:aws:iam::0123456789:role/terragrunt\" }}EOF} You can then include this in each of your unit terragrunt.hcl files using the include block for each infrastructure module you need to deploy: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} This pattern is useful for global configuration blocks that need to be included in all of your modules, but what if you have Terragrunt configurations that are only relevant to subsets of your stack? For example, consider the following terragrunt file structure, which defines three environments (prod, qa, and stage) with the same infrastructure in each one (an app, a MySQL database, and a VPC): Directorylive root.hclDirectoryprod Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl More often than not, each of the services will look similar across the different environments, only requiring small tweaks. For example, the app/terragrunt.hcl files may be identical across all three environments except for an adjustment to the instance_type parameter for each environment. These identical settings don’t belong in the root terragrunt.hcl configuration because they are only relevant to the app configurations, and not mysql or vpc. To solve this, you can use multiple include blocks. Using multiple includesSection titled “Using multiple includes” Suppose your qa/app/terragrunt.hcl configuration looks like the following: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"github.com/<org>/modules.git//app?ref=v0.1.0\"} dependency \"vpc\" { config_path = \"../vpc\"} dependency \"mysql\" { config_path = \"../mysql\"} inputs = { env = \"qa\" basename = \"example-app\" vpc_id = dependency.vpc.outputs.vpc_id subnet_ids = dependency.vpc.outputs.subnet_ids mysql_endpoint = dependency.mysql.outputs.endpoint} In this example, the only thing that is different between the environments is the env input variable. This means that except for one line, everything in the config is duplicated across prod, qa, and stage. To DRY this up, we will introduce a new folder called _env which will contain the common configurations across the three environments (we prefix with _ to indicate that this folder doesn’t contain deployable configurations, and so that it is lexically sorted first in the directory listing): Directorylive root.hclDirectory_env app.hclmysql.hclvpc.hclDirectoryprod Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl In our example, the contents of _env/app.hcl would look like the following: terraform { source = \"github.com/<org>/modules.git//app?ref=v0.1.0\"} dependency \"vpc\" { config_path = \"../vpc\"} dependency \"mysql\" { config_path = \"../mysql\"} inputs = { basename = \"example-app\" vpc_id = dependency.vpc.outputs.vpc_id subnet_ids = dependency.vpc.outputs.subnet_ids mysql_endpoint = dependency.mysql.outputs.endpoint} Note that everything is defined except for the env input variable. We now modify qa/app/terragrunt.hcl to include this alongside the root configuration by using multiple include blocks, significantly reducing our per environment configuration: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"env\" { path = \"${get_terragrunt_dir()}/../../_env/app.hcl\"} inputs = { env = \"qa\"} Using exposed includesSection titled “Using exposed includes” In the previous section, we covered using include to DRY common component configurations. While powerful, include has a limitation where the included configuration is statically merged into the child configuration. In our example, note that the _env/app.hcl file hardcodes the app module version to v0.1.0 (relevant section pasted below for convenience): terraform { source = \"github.com/<org>/modules.git//app?ref=v0.1.0\"} # ... other blocks omitted for brevity ... What if we want to deploy a different version for each environment? One way you can do this is by redefining the terraform block in the unit. For instance, to deploy v0.2.0 in the qa environment, you can perform the following: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"env\" { path = \"${get_terragrunt_dir()}/../../_env/app.hcl\"} # Override the terraform.source attribute to v0.2.0terraform { source = \"github.com/<org>/modules.git//app?ref=v0.2.0\"} inputs = { env = \"qa\"} While this works, we now have duplicated the source URL. To avoid repeating the source URL, we can use exposed includes to reference data defined in the parent configurations. To do this, modify the parent configuration to export the source URL as a local variable instead of defining it into the terraform block: locals { source_base_url = \"github.com/<org>/modules.git//app\"} # ... other blocks and attributes omitted for brevity ... We then set the expose attribute to true on the include block in the child configuration so that we can reference the data defined in the included configuration. Using that, we can construct the terraform source URL without having to repeat the module source: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"env\" { path = \"${get_terragrunt_dir()}/../../_env/app.hcl\" expose = true} # Construct the terraform.source attribute using the source_base_url and custom version v0.2.0terraform { source = \"${include.env.locals.source_base_url}?ref=v0.2.0\"} inputs = { env = \"qa\"} Using read_terragrunt_configSection titled “Using read_terragrunt_config” In the previous two sections, we covered using include to merge Terragrunt configurations through static merges with unit configuration. What if you want included configurations to be dynamic in the context of the unit where they are being used? In our example, the unit configuration defines the env input in its configuration (pasted below for convenience): # ... other blocks omitted for brevity ... inputs = { env = \"qa\"} What if some inputs depend on this env input? For example, what if we want to append the env to the name input before passing to OpenTofu/Terraform? One way to do this is to define the override parameters in the child config instead of the parent: # ... other blocks omitted for brevity ... include \"env\" { path = \"${get_terragrunt_dir()}/../../_env/app.hcl\" expose = true} inputs = { env = \"qa\" basename = \"${include.env.locals.basename}-qa\"} While this works, you could lose all the DRY advantages of the include block if you have many configurations that depend on the env input. Instead, you can use read_terragrunt_config to load additional context when including configurations by taking advantage of the folder structure, and define the env-based logic in the included configuration. To show this, let’s introduce a new env.hcl configuration in each environment: Directorylive root.hclDirectory_env app.hclmysql.hclvpc.hclDirectoryprod env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl The env.hcl configuration will look like the following: locals { env = \"qa\" # this will be prod in the prod folder, and stage in the stage folder.} We can then read the env.hcl file in the included _env/app.hcl file and use the env local: locals { # Load the relevant env.hcl file based on where the including unit is. # This works because find_in_parent_folders always runs in the context of the unit. env_vars = read_terragrunt_config(find_in_parent_folders(\"env.hcl\")) env_name = local.env_vars.locals.env source_base_url = \"github.com/<org>/modules.git//app\"} dependency \"vpc\" { config_path = \"../vpc\"} dependency \"mysql\" { config_path = \"../mysql\"} inputs = { env = local.env_name basename = \"example-app-${local.env_name}\" vpc_id = dependency.vpc.outputs.vpc_id subnet_ids = dependency.vpc.outputs.subnet_ids mysql_endpoint = dependency.mysql.outputs.endpoint} With this configuration, the env_vars local is set based on the location of the unit. For example, when Terragrunt is run in the context of the prod/app unit, prod/env.hcl is read, while qa/env.hcl is read when Terragrunt is run in the qa/app unit. Now we can clean up the child config to eliminate the env input variable, since that is loaded in the env.hcl context: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"env\" { path = \"${get_terragrunt_dir()}/../../_env/app.hcl\" expose = true} # Construct the terraform.source attribute using the source_base_url and custom version v0.2.0terraform { source = \"${include.env.locals.source_base_url}?ref=v0.2.0\"} Considerations for CI/CD PipelinesSection titled “Considerations for CI/CD Pipelines” For infrastructure CI/CD pipelines, it is common to only want to run the workflow on the modules that were updated. For example, if you only changed the unit configuration for the RDS database in the dev account, then you only want to run plan and apply on that module, not other components or other accounts. If you did not take advantage of include or read_terragrunt_config, then implementing this pipeline is straightforward: you can use git diff to collect all the files that changed, and for those terragrunt.hcl files that were updated, you can run terragrunt plan or terragrunt apply in that unit. However, if you use include or read_terragrunt_config, then a single file change may need to be reflected on multiple files that were not touched at all in the commit. In our previous example, when a configuration is updated in the _env/app.hcl file, we need to apply the change to all the modules that include that common environment configuration. The most comprehensive approach to managing this is to use the —queue-include-units-reading flag. This flag will automatically add all units that read the file to the queue of units to be run. This includes both units that include the file, and units that read the file using something like read_terragrunt_config (make to read the documentation on this so that you know the limitations of this flag). In the previous example, your CI/CD pipeline can run: terragrunt run --all plan --queue-include-units-reading _env/app.hcl This will: Recursively find all Terragrunt units in the current directory tree. Filter out any units that don’t include _env/app.hcl so that they won’t be run. Run plan on any modules remaining (which will be the set of units in the current tree that include _env/app.hcl). Thereby allowing you to only run those modules that need to be updated by the code change. Alternatively, you can implement a promotion workflow if you have multiple environments that depend on the _env/app.hcl configuration. In the above example, suppose you wanted to progressively roll out the changes through the environments, qa, stage, and prod in order. In this case, you can use --working-dir to scope down the updates from the common file: # Roll out the change to the qa environment firstterragrunt run --all plan --queue-include-units-reading _env/app.hcl --working-dir qaterragrunt run --all apply --queue-include-units-reading _env/app.hcl --working-dir qa# If the apply succeeds to qa, move on to the stage environmentterragrunt run --all plan --queue-include-units-reading _env/app.hcl --working-dir stageterragrunt run --all apply --queue-include-units-reading _env/app.hcl --working-dir stage# And finally, prod.terragrunt run --all plan --queue-include-units-reading _env/app.hcl --working-dir prodterragrunt run --all apply --queue-include-units-reading _env/app.hcl --working-dir prod This allows you to have flexibility in how changes are rolled out. For example, you can add extra validation stages in-between the roll-out to each environment, or add in manual approval between the stages. NOTE: If you identify an issue with rolling out the change in a downstream environment, and want to abort, you will need to make sure that that environment uses the older version of the common configuration. This is because the common configuration is now partially rolled out, where some environments need to use the new updated common configuration, while other environments need the old one. The best way to handle this situation is to create a new copy of the common configuration at the old version and have the environments that depend on the older version point to that version. Edit page Previous Stacks Next State Backend",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:57.911Z"
  },
  {
    "title": "State Backend",
    "url": "https://terragrunt.gruntwork.io/docs/features/state-backend/",
    "content": "State Backend MotivationSection titled “Motivation” OpenTofu/Terraform supports remote state storage via various backends that you normally configure in your .tf files as follows: terraform { backend \"s3\" { bucket = \"my-tofu-state\" key = \"frontend-app/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} Unfortunately, the backend configuration does not currently support expressions, variables, or functions. This makes it hard to keep your code DRY if you have multiple OpenTofu/Terraform modules. For example, consider the following folder structure, which uses different OpenTofu/Terraform modules to deploy a backend app, frontend app, MySQL database, and a VPC: Directorybackend-app main.tfDirectoryfrontend-app main.tfDirectorymysql main.tfDirectoryvpc main.tf To use remote state with each of these modules, you would have to copy/paste the identical backend configuration into each of the main.tf files. The only thing that would differ between the configurations would be the key parameter: e.g., the key for mysql/main.tf might be mysql/terraform.tfstate and the key for frontend-app/main.tf might be frontend-app/terraform.tfstate. In addition, the resources used for remote state will be provisioned somewhere else, and that somewhere else needs to be managed. Most users end up using “click-ops” to provision the S3 bucket and DynamoDB table used for AWS remote state (clicking around in the AWS console until they have what they need). This is error-prone, difficult to reproduce, and makes it hard to do the right thing consistently (e.g., enabling versioning, encryption, and access logging). Luckily, Terragrunt has built-in tooling to make it easy to manage remote state. Generating remote state settings with TerragruntSection titled “Generating remote state settings with Terragrunt” To fill in the settings via Terragrunt, create a root.hcl file in the root folder, plus one terragrunt.hcl file in each of the OpenTofu/Terraform modules: root.hclDirectorybackend-app main.tfterragrunt.hclDirectoryfrontend-app main.tfterragrunt.hclDirectorymysql main.tfterragrunt.hclDirectoryvpc main.tfterragrunt.hcl In your root.hcl file, you can define your entire remote state configuration just once in a generate block, to generate a backend.tf file that includes the backend configuration: generate \"backend\" { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFterraform { backend \"s3\" { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }}EOF} This instructs Terragrunt to create the file backend.tf in the working directory (where Terragrunt calls tofu/terraform) before it runs any OpenTofu/Terraform commands, including init. This allows you to inject this backend configuration in all the units that include the root file and have terragrunt properly initialize the backend configuration with interpolated values. To inherit this configuration in each unit, such as mysql/terragrunt.hcl, you can tell Terragrunt to automatically include all the settings from the root root.hcl file as follows: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} The include block tells Terragrunt to use an identical Terragrunt configuration from the root.hcl file specified via the path parameter. It behaves exactly as if you had copy/pasted the OpenTofu/Terraform configuration from the included file generate configuration into mysql/terragrunt.hcl, but this approach is much easier to maintain! The next time you run terragrunt, it will automatically configure all the settings for the backend, if they aren’t configured already, by calling tofu/terraform init. The terragrunt.hcl files above use two Terragrunt built-in functions: find_in_parent_folders(): This function returns the absolute path to the first file it finds in the parent folders above the current unit named something. In the example above, the call to find_in_parent_folders(\"root.hcl\") in mysql/terragrunt.hcl will return /your-root-folder/root.hcl. This way, you don’t have to hard code the path parameter in every unit. path_relative_to_include(): This function returns the relative path between the unit and the path specified in its include block. We typically use this in a root root.hcl file so that each unit stores its OpenTofu/Terraform state at a different key. For example, the mysql unit will have its key parameter resolve to mysql/tofu.tfstate and the frontend-app module will have its key parameter resolve to frontend-app/tofu.tfstate. Read Functions docs for more info. Create remote state resources automaticallySection titled “Create remote state resources automatically” The generate block is useful for allowing you to set up the remote state backend configuration automatically, but this introduces a bootstrapping problem: how do you create and manage the underlying storage resources for the remote state? For example, when using the s3 backend, OpenTofu/Terraform expects the S3 bucket to already exist for it to upload the state objects. Ideally, you can manage the S3 bucket using OpenTofu/Terraform, but what about the state object for the module managing the S3 bucket? How do you create the S3 bucket, before you run tofu/terraform, if you need to run tofu/terraform to create the bucket? To handle this, Terragrunt supports a different block for managing the backend configuration: the remote_state block. NOTE remote_state is an alternative way of managing the OpenTofu/Terraform backend compared to generate. You cannot use both methods at the same time to manage the remote state configuration. When implementing remote_state, be sure to remove the corresponding generate block for managing the backend. The following backends are currently supported by remote_state: s3 backend gcs backend For all other backends, the remote_state block operates in the same manner as generate. However, we may add support for additional backends to remote_state blocks, which may disrupt your environment. If you do not want support for automated management of remote state resources, we recommend sticking to generate blocks to configure the backend. When you run terragrunt with a remote_state configuration, it will automatically create the following resources if they don’t already exist: S3 bucket: If you are using the S3 backend for remote state storage and the bucket you specify in remote_state.config doesn’t already exist, Terragrunt will create it automatically, with versioning, server-side encryption, and access logging enabled. In addition, you can let terragrunt tag the bucket with custom tags that you specify in remote_state.config.s3_bucket_tags. DynamoDB table: If you are using the S3 backend for remote state storage and/or you specify a dynamodb_table (a DynamoDB table used for locking) in remote_state.config, Terragrunt will create them automatically if they don’t already exist. They will be created with server-side encryption enabled, and the DynamoDB table will use the primary key LockID. You may configure a custom endpoint for the AWS DynamoDB API using remote_state.config.dynamodb_endpoint. In addition, you can let terragrunt tag the DynamoDB table with custom tags that you specify in remote_state.config.dynamodb_table_tags. GCS bucket: If you are using the GCS backend for remote state storage and the bucket you specify in remote_state.config doesn’t already exist, Terragrunt will create it automatically, with versioning enabled. For this to work correctly you must also specify project and location keys in remote_state.config, so Terragrunt knows where to create the bucket. You will also need to supply valid credentials using either remote_state.config.credentials or by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. If you want to skip creating the bucket entirely, simply set skip_bucket_creation to true and Terragrunt will assume the bucket has already been created. If you don’t specify bucket in remote_state then terragrunt will assume that you will pass bucket through -backend-config in extra_arguments. We also strongly recommend you enable Cloud Audit Logs to audit and track API operations performed against the state bucket. In addition, you can let Terragrunt label the bucket with custom labels that you specify in remote_state.config.gcs_bucket_labels. Note: If you specify a profile key in remote_state.config, Terragrunt will automatically use this AWS profile when creating the S3 bucket or DynamoDB table. Note: You can disable automatic remote state initialization by setting remote_state.disable_init, this will skip the automatic creation of remote state resources and will execute terraform init passing the backend=false option. This can be handy when running commands such as run --all validate as part of a CI process where you do not want to initialize remote state. The following example demonstrates using an environment variable to configure this option: remote_state { # ... disable_init = tobool(get_env(\"TG_DISABLE_INIT\", \"false\"))} Here is an example of using the remote_state block to configure the S3 backend: remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"my-terraform-state\" key = \"${path_relative_to_include()}/terraform.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} Like the approach with generate blocks, this will generate a backend.tf file that contains the remote state configuration. However, in addition to that, terragrunt will also now manage the S3 bucket and DynamoDB table for you. This means that if the S3 bucket my-terraform-state and DynamoDB table my-lock-table does not exist in your account, Terragrunt will automatically create these resources before calling terraform and configure them based on the specified configuration parameters. Additionally, for the S3 backend only, Terragrunt will automatically update the S3 resource to match the configuration specified in the remote_state bucket. For example, if you require versioning in the remote_state block, but the underlying state bucket doesn’t have versioning enabled, Terragrunt will automatically turn on versioning on the bucket to match the configuration. If you do not want terragrunt to automatically apply changes, you can configure the following: remote_state { # ... other args omitted for brevity ... config = { # ... other config omitted for brevity ... disable_bucket_update = true }} Check out the terragrunt-infrastructure-modules-example and terragrunt-infrastructure-live-example repos for fully-working sample code that demonstrates how to use Terragrunt to manage remote state. S3-specific remote state settingsSection titled “S3-specific remote state settings” For the s3 backend, the following config options can be used for S3-compatible object stores, as necessary: Note: The skip_bucket_accesslogging is now DEPRECATED. It is replaced by accesslogging_bucket_name. Please read below for more details on when to use the new config option. remote_state { # ... config = { skip_bucket_versioning = true # use only if the object store does not support versioning skip_bucket_ssencryption = true # use only if non-encrypted OpenTofu/Terraform State is required and/or the object store does not support server-side encryption skip_bucket_root_access = true # use only if the AWS account root user should not have access to the remote state bucket for some reason skip_bucket_enforced_tls = true # use only if you need to access the S3 bucket without TLS being enforced skip_credentials_validation = true # skip validation of AWS credentials, useful when is used S3 compatible object store different from AWS enable_lock_table_ssencryption = true # use only if non-encrypted DynamoDB Lock Table for the OpenTofu/Terraform State is required and/or the NoSQL database service does not support server-side encryption accesslogging_bucket_name = <string> # use only if you need server access logging to be enabled for your terraform state S3 bucket. Provide a <string> value representing the name of the target bucket to be used for logs output. accesslogging_target_prefix = <string> # use only if you want to set a specific prefix for your terraform state S3 bucket access logs when Server Access Logging is enabled. Provide a <string> value representing the TargetPrefix to be used for the logs output objects. If set to empty <string>, then TargetPrefix will be set to empty <string>. If attribute is not provided at all, then TargetPrefix will be set to default value `TFStateLogs/`. shared_credentials_file = \"/path/to/credentials/file\" skip_metadata_api_check = true force_path_style = true }} If you experience an error for any of these configurations, confirm you are using OpenTofu or Terraform v0.12.2 or greater. Further, the config options s3_bucket_tags, dynamodb_table_tags, accesslogging_bucket_tags, skip_bucket_versioning, skip_bucket_ssencryption, skip_bucket_root_access, skip_bucket_enforced_tls, skip_bucket_public_access_blocking, accesslogging_bucket_name, accesslogging_target_prefix, and enable_lock_table_ssencryption are only valid for backend s3. They are used by terragrunt and are not passed on to OpenTofu/Terraform. See section Create remote state resources automatically GCS-specific remote state settingsSection titled “GCS-specific remote state settings” For the gcs backend, the following config options can be used for GCS-compatible object stores, as necessary: remote_state { # ... skip_bucket_versioning = true # use only if the object store does not support versioning enable_bucket_policy_only = false # use only if uniform bucket-level access is needed (https://cloud.google.com/storage/docs/uniform-bucket-level-access) encryption_key = \"GOOGLE_ENCRYPTION_KEY\"} If you experience an error for any of these configurations, confirm you are using Terraform v0.12.0 or greater. Further, the config options gcs_bucket_labels, skip_bucket_versioning and enable_bucket_policy_only are only valid for the backend gcs. They are used by Terragrunt and are not passed on to OpenTofu/Terraform. See “Create remote state resources automatically” for more details. Further readingSection titled “Further reading” Managing your remote state like this is really valuable when you organize your units into a stack. Reading about those concepts will help you understand how to organize your infrastructure such that different units stored in isolated state can interact with each other. Edit page Previous Includes Next Run Queue",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:57.958Z"
  },
  {
    "title": "Run Queue",
    "url": "https://terragrunt.gruntwork.io/docs/features/run-queue/",
    "content": "Run Queue Terragrunt’s “Run Queue” is the mechanism it uses to manage the run order and concurrency when running OpenTofu/Terraform commands across multiple Terragrunt units. This is particularly relevant when using the run --all or run --graph commands. How it Works: The Dependency Graph (DAG)Section titled “How it Works: The Dependency Graph (DAG)” At its core, the Run Queue relies on a Directed Acyclic Graph (DAG) built from the dependencies defined between your Terragrunt units. These dependencies are typically established using dependency or dependencies blocks in your terragrunt.hcl files. Terragrunt analyzes these dependencies to determine the correct order of operations: Discovery: Terragrunt discovers configurations that might be relevant to a run based on the current working directory. Constructing the Queue: Based on the command being run, Terragrunt creates an ordered queue. For commands like plan or apply, dependencies are run before the units that depend on them. For commands like destroy, dependent units are run before their dependencies. Runs: Terragrunt dequeues the units in the queue and runs them, respecting the queue order. By default, it runs units concurrently up to a certain limit (controlled by the --parallelism flag), but it will always wait for a unit’s dependencies (or dependents for destroys) to complete successfully before running that unit. Example DAGSection titled “Example DAG” Consider a setup where: Unit “dependent” depends on unit “dependency”. Unit “dependency” depends on unit “ancestor-dependency”. Unit “independent” has no dependencies nor dependents. Directoryroot Directorysubtree Directorydependent terragrunt.hclDirectorydependency terragrunt.hclDirectoryancestor-dependency terragrunt.hclDirectoryindependent terragrunt.hcl Assuming a current working directory of the root directory, Terragrunt would run units in the following order: run --all plan Order: Terragrunt would run independent and ancestor-dependency concurrently. Once ancestor-dependency finishes, dependency would run. Once dependency finishes, dependent would run. run --all destroy Order: Terragrunt would run dependent and independent concurrently. Once dependent finishes, dependency would run. Once dependency finishes, ancestor-dependency would run. Controlling the QueueSection titled “Controlling the Queue” Several flags allow you to customize how Terragrunt builds and executes the run queue. By default, Terragrunt will include all units that are in the current working directory. Include by defaultSection titled “Include by default” By default, when using the --all flag, Terragrunt will include all units that are in the current working directory, and any external dependencies. Certain flags trigger “Exclude by default” behavior, meaning that Terragrunt will no longer automatically include all units in the current working directory, and will instead rely on discovering configurations based on the provided queue control flags. Those flags will be discussed in the next section. Filtering UnitsSection titled “Filtering Units” You can control which units are included or excluded from the queue using several approaches: New: Unified Filtering The experimental --filter flag provides a unified approach to filtering units and stacks using a sophisticated query syntax. This will eventually replace the queue control flags listed below. Queue Control Flags: --queue-include-dir: Specify glob patterns for directories to include. Can be used multiple times. e.g. terragrunt run --all plan --queue-include-dir \"subtree/*\" Include units within the subtree directory (along with their dependencies), i.e., subtree/dependent, subtree/dependency and ancestor-dependency. Note Using the --queue-include-dir automatically triggers “Exclude by default” behavior, as mentioned above. Note ancestor-dependency is still included by default here because it’s a dependency of dependency.Using --queue-strict-include would prevent that. --queue-exclude-dir: Specify glob patterns for directories to exclude. Can be specified multiple times. e.g. terragrunt run --all plan --queue-exclude-dir \"independent\" Exclude the independent unit. ancestor-dependency, subtree/dependency, and subtree/dependent would still be processed according to their dependencies. Note Dependencies of excluded units will still be included unless they are also explicitly excluded.In this example, excluding subtree/dependency would not automatically exclude ancestor-dependency. --queue-excludes-file: Provide a file containing a list of directories to exclude. e.g. terragrunt run --all plan --queue-excludes-file \".tg-excludes\" independentsubtree/dependency Exclude independent and subtree/dependency from the run, only running subtree/dependent and ancestor-dependency. Tip The default value for this flag is .terragrunt-excludes.Populate a file named this in your project root to exclude units from being run by default, without using the --queue-excludes-file flag. --queue-strict-include: Only include units matching --queue-include-dir. e.g. terragrunt run --all plan --queue-include-dir \"subtree/dependency\" --queue-strict-include Only include the subtree/dependency unit. Its dependency, ancestor-dependency, will be excluded because it does not match the include and strict mode is enabled. --queue-include-external: Include external dependencies (those outside the current working directory) by default. e.g. terragrunt run --all plan --working-dir subtree --queue-include-external Include ancestor-dependency in addition to the subtree/dependent and subtree/dependency units. Note The --queue-include-external flag is simply a convenience flag to avoid the interactive prompt to request inclusion of external dependencies.By default, Terragrunt will wait for user input to determine whether or not external dependencies should be included. --queue-exclude-external: Exclude external dependencies. e.g. terragrunt run --all plan --working-dir subtree --queue-exclude-external Exclude ancestor-dependency from the run. Only run subtree/dependent and subtree/dependency. Note This flag is simply a convenience flag to avoid the interactive prompt to request exclusion of external dependencies.By default, Terragrunt will wait for user input to determine whether or not external dependencies should be excluded. --queue-include-units-reading: Include units that read a specific file (via includes or HCL functions like mark_as_read). e.g. terragrunt run --all plan --queue-include-units-reading \"subtree/common.hcl\" subtree/dependent/terragrunt.hcl subtree/dependency/terragrunt.hcl subtree/common.hcl dependency \"dep\" { config_path = \"../dependency\" skip_outputs = true} include \"common\" { path = find_in_parent_folders(\"common.hcl\")} dependency \"dep\" { config_path = \"../../ancestor-dependency\" skip_outputs = true} include \"common\" { path = find_in_parent_folders(\"common.hcl\")} # Intentionally empty Include subtree/dependent and subtree/dependency (the units that read subtree/common.hcl) in the run. Tip Sometimes, it can be impossible for Terragrunt to know that certain files are read by a unit (e.g. if the file is read in the OpenTofu/Terraform module, not in Terragrunt configuration).In cases like this, you can use the mark_as_read HCL function to explicitly tell Terragrunt that a unit reads a file. Modifying Order and Error HandlingSection titled “Modifying Order and Error Handling” --queue-construct-as (--as): Build the run queue as if a particular command was run. Useful for performing dry-runs of run using discovery commands, like find and list. e.g. terragrunt list --queue-construct-as destroy This lists the units in the order they’d be processed for run --all destroy: $ terragrunt list --as destroy -lType Pathunit independentunit subtree/dependentunit subtree/dependencyunit ancestor-dependency $ terragrunt list --as plan -lType Pathunit ancestor-dependencyunit independentunit subtree/dependencyunit subtree/dependent --queue-ignore-dag-order: Execute units concurrently without respecting the dependency order. e.g. terragrunt run --all plan --queue-ignore-dag-order Run plan on ancestor-dependency, subtree/dependency, subtree/dependent, and independent all concurrently, without waiting for their defined dependencies. For instance, subtree/dependent’s plan would not wait for subtree/dependency’s plan to complete). Caution This flag is useful for faster runs in stateless commands like validate or plan, but is dangerous for commands that modify state like apply or destroy.You might encounter failed applies if unit dependencies are not applied before dependents, and conversely, failed destroys if unit dependents are not destroyed before dependencies. --queue-ignore-errors: Continue processing the queue even if some units fail. e.g. terragrunt run --all plan --queue-ignore-errors If ancestor-dependency’s plan fails, Terragrunt will still attempt to run plan for subtree/dependency, then subtree/dependent, and also for independent. Caution This flag is useful for identifying all errors at once, but can lead to inconsistent state if used with apply or destroy.You might encounter failed applies if unit dependencies are not applied successfully before dependents, and conversely, failed destroys if unit dependents are not destroyed successfully before dependencies. Important ConsiderationsSection titled “Important Considerations” Caution When using run --all plan with units that have dependencies (e.g. via dependency or dependencies blocks), the command will fail if those dependencies have never been deployed. This is because Terragrunt cannot resolve dependency outputs without existing state.To work around this issue, use mock outputs in dependency blocks. Caution Do not set TF_PLUGIN_CACHE_DIR when using run --all (unless using OpenTofu >= 1.10).This can cause concurrent access issues with the provider cache. Instead, use Terragrunt’s built-in Provider Cache Server. Caution When using run --all with apply or destroy, Terragrunt automatically adds the -auto-approve flag due to limitations with shared stdin making individual approvals impossible. Use --no-auto-approve to override this, but be aware you might need alternative approval workflows. Edit page Previous State Backend Next Catalog",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:57.993Z"
  },
  {
    "title": "Catalog",
    "url": "https://terragrunt.gruntwork.io/docs/features/catalog/",
    "content": "Catalog Launch the user interface for searching and managing your module catalog. terragrunt catalog <repo-url> [--no-include-root] [--root-file-name] If <repo-url> is provided, the repository will be cloned into a temporary directory, otherwise: The repository list are searched in the config file terragrunt.hcl. if terragrunt.hcl does not exist in the current directory, the config are searched in the parent directories. If the repository list is not found in the configuration file, the modules are looked for in the current directory. An example of how to define the optional default template and the list of repositories for the catalog command in the terragrunt.hcl configuration file: catalog { default_template = \"git@github.com/acme/example.git//path/to/template\" # Optional default template to use for scaffolding urls = [ \"relative/path/to/repo\", # will be converted to the absolute path, relative to the path of the configuration file. \"/absolute/path/to/repo\", \"github.com/gruntwork-io/terraform-aws-lambda\", # url to remote repository \"http://github.com/gruntwork-io/terraform-aws-lambda\", # same as above ]} This will recursively search for OpenTofu/Terraform modules in the root of the repo and the modules directory and show a table with all the modules. You can then: Search and filter the table: / and start typing. Select a module in the table: use the arrow keys to go up and down and next/previous page. See the docs for a selected module: ENTER. Use terragrunt scaffold to render a terragrunt.hcl for using the module: S. Custom templates for scaffoldingSection titled “Custom templates for scaffolding” Terragrunt has a basic template built-in for rendering terragrunt.hcl files, but you can provide your own templates to customize how code is generated! Scaffolding is done via boilerplate, and Terragrunt allows you to specify custom boilerplate templates via two mechanisms while using catalog: You can define a custom Boilerplate template in a .boilerplate sub-directory of any OpenTofu/Terraform module. You can specify a custom Boilerplate template in the catalog configuration using the default_template option. Scaffolding FlagsSection titled “Scaffolding Flags” The following catalog flags control behavior of the underlying scaffold command when the S key is pressed in a catalog entry: --no-include-root - Do not include the root configuration file in any generated terragrunt.hcl during scaffolding. --root-file-name - The name of the root configuration file to include in any generated terragrunt.hcl during scaffolding. This value also controls the name of the root configuration file to search for when trying to determine Catalog urls. Edit page Previous Run Queue Next Scaffold",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.027Z"
  },
  {
    "title": "Scaffold",
    "url": "https://terragrunt.gruntwork.io/docs/features/scaffold/",
    "content": "Scaffold Terragrunt scaffolding can generate files for you automatically using boilerplate templates. Currently, one boilerplate template is supported out-of-the-box, which you can use to generate a best-practices terragrunt.hcl that configures an OpenTofu/Terraform module for deployment: terragrunt scaffold <MODULE_URL> [TEMPLATE_URL] [--var] [--var-file] [--no-include-root] [--root-file-name] [--no-dependency-prompt] Description: MODULE_URL - This parameter specifies the URL to an OpenTofu/Terraform module. It can be a local file path, git URL, registry URL, or any other module source URL. TEMPLATE_URL - This optional parameter specifies the URL to a custom boilerplate template to generate HCL files. It can be a local file path, git URL, registry URL, or any other module source URL. If not specified, Terragrunt will: Look for a .boilerplate folder in the module at MODULE_URL, and if found, use the boilerplate template in that folder. Failing to find that, Terragrunt will use a default boilerplate template that is built-in, which creates a best-practices terragrunt.hcl for deploying a single OpenTofu/Terraform module. For example, here’s how you can generate a terragrunt.hcl file to configure an example MySQL OpenTofu/Terraform module for deployment: terragrunt scaffold github.com/gruntwork-io/terragrunt-infrastructure-modules-example//modules/mysql This will create a terragrunt.hcl in your current working directory, with roughly the following contents: # This is a Terragrunt module generated by boilerplate.terraform { source = \"git::https://github.com/gruntwork-io/terragrunt-infrastructure-modules-example.git//modules/mysql?ref=v0.8.1\"} inputs = { # -------------------------------------------------------------------------------------------------------------------- # Required input variables # -------------------------------------------------------------------------------------------------------------------- # Type: string # Description: The AWS region to deploy to (e.g. us-east-1) aws_region = \"\" # TODO: fill in value # Type: string # Description: The name of the DB name = \"\" # TODO: fill in value # Type: string # Description: The instance class of the DB (e.g. db.t2.micro) instance_class = \"\" # TODO: fill in value # (... full list of inputs omitted for brevity ...)} Important notes: The source URL is configured for you automatically, with the ref pointing to the latest “release” tag of the module (found by scanning git tags). The inputs section is generated for you automatically, and will list all required and optional variables from the module, with their types, descriptions, and defaults, so you can easily fill them in to configure the module as you like. Custom templates for scaffoldingSection titled “Custom templates for scaffolding” Terragrunt has a basic template built-in for rendering terragrunt.hcl files, but you can provide your own templates to customize what code is generated! Scaffolding is done via boilerplate, and Terragrunt allows you to specify custom boilerplate templates via three mechanisms - listed in order of priority: You can specify a custom boilerplate template to use as the second argument of the scaffold command. You can define a custom boilerplate template in a .boilerplate subfolder of your module. You can define a default custom boilerplate template in the catalog config. If you define input variables in your boilerplate template, Terragrunt will prompt users for the values. Those values can also be passed in via --var and --var-file arguments. There are also a set of variables that Terragrunt will automatically expose to your boilerplate templates for rendering: sourceUrl - URL to module requiredVariables - list of required variables in the module being scaffolded (see below) optionalVariables - list of optional variables in the module being scaffolded (see below) The elements in the requiredVariables and optionalVariables lists are structs with the following fields: Name - variable name Description - variable description Type - variable type (string, number, bool, list, map, object) Type Constants DefaultValue - variable default value DefaultValuePlaceholder - default value placeholder (e.g. \"\" for a string or 0 for a number) Optional variables which can be passed to scaffold command: Ref - git tag or branch name for module to be used EnableRootInclude - add in default terragrunt.hcl inclusion for the root module, by default true RootFileName - name of the root configuration file, by default terragrunt.hcl * SourceUrlType - if set to git-ssh module url will be converted to Git/SSH format SourceGitSshUser - git user for Git/SSH format, by default git * NOTE: RootFileName is set to terragrunt.hcl by default to ensure backwards compatibility, but the pattern of using a terragrunt.hcl file at the root of Terragrunt projects has since been deprecated. When the root-terragrunt-hcl strict control is enabled, the default configuration file will change to root.hcl, which is considered a better practice. For more details, see Migrating from root terragrunt.hcl. Convenience flagsSection titled “Convenience flags” --no-include-root - Disable inclusion of the root module in the generated terragrunt.hcl file (equivalent to using --var=EnableRootInclude=false, and will be overridden if the corresponding var value is set). --root-file-name - Set the name of the root configuration file to include in the generated terragrunt.hcl file (equivalent to using --var=RootFileName=<name>, and will be overridden if the corresponding var value is set). --no-dependency-prompt - Disable dependency confirmation, but keep the interactive mode enabled (skip asking for confirmation about including dependencies defined in the boilerplate template). * NOTE: RootFileName is set to terragrunt.hcl by default to ensure backwards compatibility, but the pattern of using a terragrunt.hcl file at the root of Terragrunt projects has since been deprecated. See the note above on the root-terragrunt-hcl strict control for more information. ExamplesSection titled “Examples” Scaffold new project but use specific module version: terragrunt scaffold github.com/gruntwork-io/terragrunt.git//test/fixtures/inputs --var=Ref=v0.68.4 Scaffold new project but use Git/SSH URLs: terragrunt scaffold github.com/gruntwork-io/terragrunt.git//test/fixtures/inputs --var=SourceUrlType=git-ssh ```hcl# terragrunt.hclterraform { source = \"git::ssh://git@github.com/gruntwork-io/terragrunt.git//test/fixtures/inputs?ref=v0.68.4\"} Scaffold new project using a template from a Git repository: terragrunt scaffold github.com/gruntwork-io/terragrunt.git//test/fixtures/scaffold/module-with-template# The template from the .boilerplate directory will be used to generate terragrunt.hcl NOTE: Scaffolding infrastructure from an external repository might introduce security or stability risks. Always review code from trusted external sources before running it. Scaffold new project using an external template: terragrunt scaffold github.com/gruntwork-io/terragrunt.git//test/fixtures/inputs git@github.com:gruntwork-io/terragrunt.git//test/fixtures/scaffold/external-template# The files external-template.txt and terragrunt.hcl will be created from that external template Edit page Previous Catalog Next Extra Arguments",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.085Z"
  },
  {
    "title": "Extra Arguments",
    "url": "https://terragrunt.gruntwork.io/docs/features/extra-arguments/",
    "content": "Extra Arguments MotivationSection titled “Motivation” Sometimes you need to pass extra CLI arguments every time you run certain tofu/terraform commands. For example, you may want to set the lock-timeout setting to 20 minutes for all commands that may modify remote state so that OpenTofu/Terraform will keep trying to acquire a lock for up to 20 minutes if someone else already has the lock rather than immediately exiting with an error. You can configure Terragrunt to pass specific CLI arguments for specific commands using an extra_arguments block in your terragrunt.hcl file: terraform { # Force OpenTofu/Terraform to keep trying to acquire a lock for # up to 20 minutes if someone else already has the lock extra_arguments \"retry_lock\" { commands = [ \"init\", \"apply\", \"refresh\", \"import\", \"plan\", \"taint\", \"untaint\" ] arguments = [ \"-lock-timeout=20m\" ] env_vars = { TF_VAR_var_from_environment = \"value\" } }} Each extra_arguments block includes an arbitrary label (in the example above, retry_lock), a list of commands to which the extra arguments should be added, and a list of arguments, required_var_files or optional_var_files to add. You can also pass custom environment variables using the env_vars attribute, which stores environment variables in key value pairs. With the configuration above, when you run terragrunt apply, Terragrunt will call OpenTofu/Terraform as follows: $ terragrunt apply# tofu apply -lock-timeout=20m You can even use built-in functions such as get_terraform_commands_that_need_locking to conveniently populate the list of OpenTofu/Terraform commands that need locking: terraform { # Force OpenTofu/Terraform to keep trying to acquire a lock for up to 20 minutes if someone else already has the lock extra_arguments \"retry_lock\" { commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] }} Multiple extra_arguments blocksSection titled “Multiple extra_arguments blocks” You can specify one or more extra_arguments blocks. The arguments in each block will be applied any time you call terragrunt with one of the commands in the commands list. If more than one extra_arguments block matches a command, the arguments will be added in the order of appearance in the configuration. For example, in addition to lock settings, you may also want to pass custom -var-file arguments to several commands: terraform { # Force OpenTofu/Terraform to keep trying to acquire a lock for # up to 20 minutes if someone else already has the lock extra_arguments \"retry_lock\" { commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] } # Pass custom var files to OpenTofu/Terraform extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] arguments = [ \"-var\", \"foo=bar\", \"-var\", \"region=us-west-1\" ] }} With the configuration above, when you run terragrunt apply, Terragrunt will call OpenTofu/Terraform as follows: $ terragrunt apply# tofu apply -lock-timeout=20m -var foo=bar -var region=us-west-1 extra_arguments for initSection titled “extra_arguments for init” Extra arguments for the init command have some additional behavior and constraints. In addition to being appended to the tofu init/terraform init command that is run when you explicitly run terragrunt init, extra_arguments for init will also be appended to the init commands that are automatically run during other commands (see Auto-Init). You must not specify the -from-module option (aka. the SOURCE argument for terraform < 0.10.0) or the DIR argument in the extra_arguments for init. This option and argument will be provided automatically by terragrunt. Here’s an example of configuring extra_arguments for init in an environment in which OpenTofu/Terraform plugins are manually installed, rather than relying on OpenTofu/Terraform to automatically download them. terraform { # ... extra_arguments \"init_args\" { commands = [ \"init\" ] arguments = [ \"-plugin-dir=/my/tofu/plugin/dir\", ] }} Note that you’re encouraged to use the Provider Caching feature instead of manually installing plugins in most cases. Required and optional var-filesSection titled “Required and optional var-files” One common usage of extra_arguments is to include tfvars files. Instead of using arguments, it is simpler to use either required_var_files or optional_var_files. Both options require only to provide the list of files to include. The only difference is that required_var_files will add the extra argument -var-file=<your file> for each file specified and exit with an error if they don’t exist. optional_var_files, on the other hand, will skip over files that don’t exist. This allows many conditional configurations based on environment variables as you can see in the following example: root.hclprod.tfvarsus-west-2.tfvarsDirectorybackend-app main.tfdev.tfvarsterragrunt.hclDirectoryfrontend-app main.tfus-east-1.tfvarsterragrunt.hcl include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { extra_arguments \"conditional_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] required_var_files = [ \"${get_parent_terragrunt_dir()}/tofu.tfvars\" ] optional_var_files = [ \"${get_parent_terragrunt_dir(\"root\")}/${get_env(\"TF_VAR_env\", \"dev\")}.tfvars\", \"${get_parent_terragrunt_dir(\"root\")}/${get_env(\"TF_VAR_region\", \"us-east-1\")}.tfvars\", \"${get_terragrunt_dir()}/${get_env(\"TF_VAR_env\", \"dev\")}.tfvars\", \"${get_terragrunt_dir()}/${get_env(\"TF_VAR_region\", \"us-east-1\")}.tfvars\" ] }} See the get_terragrunt_dir() and get_parent_terragrunt_dir() documentation for more details. With the configuration above, when you run terragrunt run --all apply, Terragrunt will call OpenTofu/Terraform as follows: $ terragrunt run --all apply[backend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/backend-app/dev.tfvars[frontend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/frontend-app/us-east-1.tfvars $ TF_VAR_env=prod terragrunt run --all apply[backend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/prod.tfvars[frontend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/prod.tfvars -var-file=/my/tf/frontend-app/us-east-1.tfvars $ TF_VAR_env=prod TF_VAR_region=us-west-2 terragrunt run --all apply[backend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/prod.tfvars -var-file=/my/tf/us-west-2.tfvars[frontend-app] tofu apply -var-file=/my/tf/tofu.tfvars -var-file=/my/tf/prod.tfvars -var-file=/my/tf/us-west-2.tfvars Handling whitespaceSection titled “Handling whitespace” The list of arguments cannot include whitespace, so if you need to pass command line arguments that include spaces (e.g. -var bucket=example.bucket.name), then each of the arguments will need to be a separate item in the arguments list: terraform { extra_arguments \"bucket\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] arguments = [ \"-var\", \"bucket=example.bucket.name\", ] }} With the configuration above, when you run terragrunt apply, Terragrunt will call OpenTofu/Terraform as follows: $ terragrunt apply# tofu apply -var bucket=example.bucket.name Edit page Previous Scaffold Next Authentication",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.120Z"
  },
  {
    "title": "Authentication",
    "url": "https://terragrunt.gruntwork.io/docs/features/authentication/",
    "content": "Authentication MotivationSection titled “Motivation” AWS is by far the most popular OpenTofu/Terraform provider, and most Terragrunt users are using it to manage AWS infrastructure, at least in part. As a consequence, Terragrunt has a number of features that make it easier to work with AWS, especially when you have to manage multiple AWS accounts. The most secure way to manage AWS infrastructure is to segment infrastructure between multiple AWS accounts. Segmenting infrastructure in this way can ensure that developers are not granted permissions they don’t need on infrastructure they don’t manage. It’s also a best practice from a safety perspective, as it helps to prevent accidental changes to sensitive resources like production infrastructure. When working with multiple AWS accounts, a best practice is to temporarily assume roles within those AWS accounts to perform actions using mechanisms like IAM Identity Center or OIDC. When using these technologies, users don’t need any static users or credentials. All access is temporary, and permissions are determined by the role they assume. These technologies allow you to securely (and temporarily) acquire secrets for least privilege access to a target AWS account, and perform actions that can only impact that AWS account, limiting blast radius. There are a few ways to assume IAM roles when using AWS CLI tools, such as OpenTofu/Terraform: One option is to create a named profile, each with a different role_arn parameter. You then tell OpenTofu/Terraform which profile to use via the AWS_PROFILE environment variable. The downside to using profiles is that they can vary between users. One user might have a profile named dev that assumes a role in the dev account, while another user might have a profile named development that assumes the same role. This can lead to confusion and errors when sharing code between users. It also results in a requirement that all users have profiles set up on their local machines. Finally, this also presents a problem in CI/CD pipelines, where you typically avoid storing AWS credentials on disk so they are less likely to leak. Another option is to use the AWS CLI. As a standard operating procedure, users are required to assume a role before invoking OpenTofu/Terraform by running something like aws sts assume-role --role-arn <ROLE>, use the output of that command to set the appropriate environment variables, and the tool is run with those temporary credentials stored as environment variables. The downside to this approach is that it requires that users know this process and remember to do it correctly every time they want to use OpenTofu/Terraform. It’s also a tedious process, and requires several steps to complete correctly. Worse yet, it requires that users repeat this process often, as the credentials you get back from the assume-role command expire. This is especially problematic if the OpenTofu/Terraform run is expected to take longer than the role assumption duration, and can expire mid-run. A final option is to modify your AWS provider with the assume_role configuration and your S3 backend with the role_arn parameter. The downside to managing your role assumption with the AWS provider is that all runs have to be performed with the same IAM role. This can be problematic if you have different users who assume different roles, depending on their need for elevated access, and as a best practice, the role assumed by CI/CD pipelines should be different from the role assumed by developers. The way in which these roles are assumed also differ, as developers might use a web-based SSO portal to acquire temporary credentials, while CI/CD pipelines might use OIDC and assume a role using a web identity token. To avoid these frustrating trade-offs, you can configure Terragrunt to assume an IAM role for you. Configuring Terragrunt to assume an IAM roleSection titled “Configuring Terragrunt to assume an IAM role” To tell Terragrunt to assume an IAM role, just set the --iam-assume-role command line argument: terragrunt apply --iam-assume-role \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\" Alternatively, you can set the TG_IAM_ASSUME_ROLE environment variable: export TG_IAM_ASSUME_ROLE=\"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\"terragrunt apply Additionally, you can specify an iam_role property in the terragrunt config: iam_role = \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\" Terragrunt will resolve the value of the option by first looking for the cli argument, then looking for the environment variable, then defaulting to the value specified in the config file. Terragrunt will call the sts assume-role API on your behalf and expose the credentials it gets back as environment variables when running OpenTofu/Terraform. The advantage of this approach is that you can store your AWS credentials in a secret store and never write them to disk in plaintext, you get fresh credentials on every run of Terragrunt, without the complexity of calling assume-role yourself, and you don’t have to modify your OpenTofu/Terraform code or backend configuration at all. Leveraging OIDC role assumptionSection titled “Leveraging OIDC role assumption” In addition, you can combine the --iam-assume-role flag with the --iam-assume-role-web-identity-token to use the AssumeRoleWithWebIdentity API instead of the AssumeRole API. This is especially convenient in the context of CI/CD pipelines, as it’s generally a best practice to assume roles there via OIDC. Configuring OIDC role assumption largely works like the --iam-assume-role flag, with the addition of the --iam-assume-role-web-identity-token flag. One special aspect of the --iam-assume-role-web-identity-token flag is that it can use both a token, and the path to a file containing the token. As a command line argument: terragrunt apply --iam-assume-role \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\" --iam-assume-role-web-identity-token \"$TOKEN\" As environment variables: export TG_IAM_ASSUME_ROLE=\"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\"export TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN=\"$TOKEN\"terragrunt apply In the Terragrunt configuration: iam_role = \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\"iam_web_identity_token = get_env(\"AN_OIDC_TOKEN\") Auth provider commandSection titled “Auth provider command” Finally, there is also a special flag that allows you to use an external command to provide the role assumption credentials. This is the most powerful and flexible option for setting up Terragrunt authentication, but it does require a bit more setup. This technique is especially useful in the following circumstances: In a CI/CD pipelines, where you might want to use different role assumption mechanisms for different stages of the pipeline (like a read-only, plan role during pull request open, and a read-write, apply role during merge). On a shared development repository, where you might want to use different roles for different developers, or even different roles for the same developer, depending on the task at hand. In a setup where units in different accounts depend on each other, and you want to assume a different role for each account. The --auth-provider-cmd flag allows you to specify a command that can be executed by Terragrunt to fetch credentials at runtime. terragrunt apply --auth-provider-cmd /path/to/auth-script.sh As with all other flags, you can also set this as an environment variable: export TG_AUTH_PROVIDER_CMD=\"/path/to/auth-script.sh\"terragrunt apply When Terragrunt executes this script, it will expect a response in stdout that obeys the following schema: { \"awsCredentials\": { \"ACCESS_KEY_ID\": \"\", \"SECRET_ACCESS_KEY\": \"\", \"SESSION_TOKEN\": \"\" }, \"awsRole\": { \"roleARN\": \"\", \"sessionName\": \"\", \"duration\": 0, \"webIdentityToken\": \"\" }, \"envs\": { \"ANY_KEY\": \"\" }} All top-level objects are optional, and you can provide multiple. awsCredentials is the standard AWS credential object, which can be used to set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and (optionally) AWS_SESSION_TOKEN environment variables before running OpenTofu/Terraform. awsRole is the role assumption object, which can be used to dynamically perform role assumption on the roleARN role with the sessionName session name, for a duration of time, and with a webIdentityToken if needed. Terragrunt will automatically refresh this role assumption when the duration expires. envs is a map of environment variables that will be set before running OpenTofu/Terraform. Given that the working directory of Terragrunt execution is the same as the command, you can author logic in your script to determine which credentials are appropriate to return based on the context of the Terragrunt run. This feature is integrated with the Gruntwork Pipelines product to provide a secure and flexible way to manage assumption of different roles in different accounts based on context. Required PermissionsSection titled “Required Permissions” You are ultimately responsible for ensuring that the IAM role you are assuming has the minimal and necessary permissions required to perform the activity you are attempting. At a minimum, however there is some guidance that you can follow for ensuring that you have sufficient permissions. Granting the following permissions on an IAM role: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowAllDynamoDBActionsOnAllTerragruntTables\", \"Effect\": \"Allow\", \"Action\": \"dynamodb:*\", \"Resource\": [ \"arn:aws:dynamodb:*:1234567890:table/terragrunt*\" ] }, { \"Sid\": \"AllowAllS3ActionsOnTerragruntBuckets\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::terragrunt*\", \"arn:aws:s3:::terragrunt*/*\" ] } ]} Will grant Terragrunt more than enough permissions to perform what it needs to do in AWS (replacing 1234567890 with your AWS account ID, and terragrunt* with the desired names of your Terragrunt resources). Note that these permissions might be too broad for your circumstances, however. A more minimal policy might look like the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowCreateAndListS3ActionsOnSpecifiedTerragruntBucket\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketVersioning\", \"s3:GetBucketAcl\", \"s3:GetBucketLogging\", \"s3:CreateBucket\", \"s3:PutBucketPublicAccessBlock\", \"s3:PutBucketTagging\", \"s3:PutBucketPolicy\", \"s3:PutBucketVersioning\", \"s3:PutEncryptionConfiguration\", \"s3:PutBucketAcl\", \"s3:PutBucketLogging\", \"s3:GetEncryptionConfiguration\", \"s3:GetBucketPolicy\", \"s3:GetBucketPublicAccessBlock\", \"s3:PutLifecycleConfiguration\", \"s3:PutBucketOwnershipControls\" ], \"Resource\": \"arn:aws:s3:::BUCKET_NAME\" }, { \"Sid\": \"AllowGetAndPutS3ActionsOnSpecifiedTerragruntBucketPath\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\": \"arn:aws:s3:::BUCKET_NAME/some/path/here\" }, { \"Sid\": \"AllowCreateAndUpdateDynamoDBActionsOnSpecifiedTerragruntTable\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:PutItem\", \"dynamodb:GetItem\", \"dynamodb:DescribeTable\", \"dynamodb:DeleteItem\", \"dynamodb:CreateTable\" ], \"Resource\": \"arn:aws:dynamodb:*:*:table/TABLE_NAME\" } ]} As you can see, the permissions are getting locked down, and the risk you run by adopting these permissions is that you might not realize that you need certain permissions until you run into an error. It’s generally a best practice to start with permissions that are too narrow, and expand them as necessary. Additionally, while Terragrunt can provision the S3 bucket and DynamoDB table it uses for S3 state storage, it doesn’t need to. You can create these resources manually, then grant Terragrunt permissions to interact with them (but not create them). A policy that allows this would look like the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:GetBucketLocation\", \"s3:List*\" ], \"Resource\": [ \"arn:aws:s3:::<BucketName>\" ], \"Effect\": \"Allow\" }, { \"Action\": [ \"s3:DeleteObject\", \"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::<BucketName>/*\" ], \"Effect\": \"Allow\" }, { \"Sid\": \"AllowCreateAndUpdateDynamoDBActionsOnSpecifiedTerragruntTable\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:PutItem\", \"dynamodb:GetItem\", \"dynamodb:DescribeTable\", \"dynamodb:DeleteItem\", ], \"Resource\": \"arn:aws:dynamodb:*:*:table/TABLE_NAME\" } ]} You’ll want to make sure that you set configurations like skip_bucket_versioning in remote_state to prevent Terragrunt from attempting to validate the bucket or table is in the proper configuration without requisite permissions. Edit page Previous Extra Arguments Next Hooks",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.151Z"
  },
  {
    "title": "Hooks",
    "url": "https://terragrunt.gruntwork.io/docs/features/hooks/",
    "content": "Hooks Before Hooks, After Hooks and Error Hooks are a feature of terragrunt that make it possible to define custom actions that will be called before/after running an tofu/terraform command. They allow you to orchestrate certain operations around IaC updates so that you have a consistent way to run custom code before or after running OpenTofu/Terraform. Here’s an example: terraform { before_hook \"before_hook\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Running OpenTofu\"] } after_hook \"after_hook\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Finished running OpenTofu\"] run_on_error = true } error_hook \"import_resource\" { commands = [\"apply\"] execute = [\"echo\", \"Error Hook executed\"] on_errors = [ \".*\", ] }} In this example configuration, whenever Terragrunt runs tofu apply or tofu plan (or the terraform equivalent), three things will happen: Before Terragrunt runs tofu/terraform, it will output Running OpenTofu to the console. After Terragrunt runs tofu/terraform, it will output Finished running OpenTofu, regardless of whether the command failed. If an error occurs during the tofu apply command, Terragrunt will output Error Hook executed. You can learn more about all the various configuration options supported in the reference docs for the terraform block. Hook ContextSection titled “Hook Context” All hooks add extra environment variables when executing the hook’s run command: TG_CTX_TF_PATH TG_CTX_COMMAND TG_CTX_HOOK_NAME For example: terraform { before_hook \"test_hook\" { commands = [\"apply\"] execute = [\"hook.sh\"] }} Where hook.sh is: echo \"TF_PATH=${TG_CTX_TF_PATH} COMMAND=${TG_CTX_COMMAND} HOOK_NAME=${TG_CTX_HOOK_NAME}\" Will result in the following output when Terragrunt runs tofu apply/terraform apply: TF_PATH=tofu COMMAND=apply HOOK_NAME=test_hook Note that hooks are executed within the working directory where OpenTofu/Terraform would be run. If using the source attribute for the terraform block, this will result in the hook running in the hidden .terragrunt-cache directory. This also means that you can use tofu/terraform commands within hooks to access any outputs needed for hook logic. For example: # Get the bucket_name output from OpenTofu/Terraform stateBUCKET_NAME=\"$(\"$TG_CTX_TF_PATH\" output -raw bucket_name)\" # Use the AWS CLI to list the contents of the bucketaws s3 ls \"s3://$BUCKET_NAME\" Note that the TG_CTX_TF_PATH environment variable is used here to ensure compatibility, regardless of the value of tf-path. This can be a useful practice if you are migrating between OpenTofu or Terraform. You will also have access to all the inputs set in the terragrunt.hcl file as environment variables prefixed by TF_VAR_, as that’s how the variables are set for use in OpenTofu/Terraform. For example, if you have the following inputs block in your terragrunt.hcl file: inputs = { bucket_name = \"my-bucket\"} You can access the bucket_name input in your hook as follows: # Get the bucket_name input from the terragrunt.hcl fileBUCKET_NAME=\"$TF_VAR_bucket_name\" # Use the AWS CLI to list the contents of the bucketaws s3 ls \"s3://$BUCKET_NAME\" Orchestrating execution outside IaCSection titled “Orchestrating execution outside IaC” Hooks can be used to handle operations that need to happen, but are not directly related to the OpenTofu/Terraform. For example, you may be using Terragrunt to manage an AWS ECS service. You can use a before_hook to build and push a new image to the Elastic Container Registry (ECR) before running tofu apply. terraform { before_hook \"build_and_push_image\" { commands = [\"plan\", \"apply\"] execute = [\"./build_and_push_image.sh\"] }} Where build_and_push_image.sh is something like: #!/usr/bin/env bash set -eou pipefail ACCOUNT_ID=\"123456789012\"REGION=\"us-east-1\"REPOSITORY=\"my-repository\"TAG=\"latest\" IMAGE_TAG=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPOSITORY}:${TAG}\" # Build the Docker imagedocker build -t \"$IMAGE_TAG\" . # Push the Docker image to ECRaws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.comdocker push \"$IMAGE_TAG\" The hard-coding of values in the script could be replaced with context, as shown in the previous section. Similarly, you may want to smoke-test newly deployed infrastructure after running tofu apply. terraform { after_hook \"smoke_test\" { commands = [\"apply\"] execute = [\"./smoke_test.sh\"] run_on_error = true }} Where smoke_test.sh is something like: #!/usr/bin/env bash set -eou pipefail # Get the URL for the service from OpenTofu/Terraform stateSERVICE_URL=\"$(\"$TG_CTX_TF_PATH\" output -raw service_url)\" # Use curl to check the service is upcurl -sSf \"$SERVICE_URL\" You might even decide to integrate with a product like Terratest for more complex testing. Hook OrderingSection titled “Hook Ordering” You can have multiple before and after hooks. Each hook will execute in the order they are defined. For example: terraform { before_hook \"before_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Will run OpenTofu\"] } before_hook \"before_hook_2\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Running OpenTofu\"] }} This configuration will cause Terragrunt to output Will run OpenTofu and then Running OpenTofu before the call to OpenTofu/Terraform. Tflint hookSection titled “Tflint hook” Before Hooks or After Hooks natively support tflint, a linter for OpenTofu/Terraform code. It will validate the OpenTofu/Terraform code used by Terragrunt, and its inputs. Here’s an example: terraform { before_hook \"before_hook\" { commands = [\"apply\", \"plan\"] execute = [\"tflint\"] }} The .tflint.hcl should exist in the same folder as terragrunt.hcl or one of it’s parents. If Terragrunt can’t find a .tflint.hcl file, it won’t execute tflint and return an error. All configurations should be in a config block in this file, as per Tflint’s docs. plugin \"aws\" { enabled = true version = \"0.21.0\" source = \"github.com/terraform-linters/tflint-ruleset-aws\"} config { module = true} ConfigurationSection titled “Configuration” By default, tflint is executed with the internal tflint built into Terragrunt, which will evaluate parameters passed in. Any desired extra configuration should be added in the .tflint.hcl file. It will work with a .tflint.hcl file in the current folder or any parent folder. To utilize an alternative configuration file, use the --config flag with the path to the configuration file. If there is a need to run tflint from the operating system directly, use the extra parameter --external-tflint. This will result in usage of the tflint binary found in the PATH environment variable. For example: terraform { before_hook \"tflint\" { commands = [\"apply\", \"plan\"] execute = [\"tflint\" , \"--external-tflint\", \"--minimum-failure-severity=error\", \"--config\", \"custom.tflint.hcl\"] }} Authentication for tflint rulesetsSection titled “Authentication for tflint rulesets” Public rulesets tflint works without any authentication for public rulesets (hosted on public repositories). Private rulesets If you want to run the tflint hook with custom rulesets defined in a private repository, you will need to export a valid GITHUB_TOKEN token. TroubleshootingSection titled “Troubleshooting” flag provided but not defined: -act-as-bundled-plugin error If you have an .tflint.hcl file that is empty, or uses the terraform ruleset without version or source constraint, it can return the following error: Failed to initialize plugins; Unrecognized remote plugin message: Incorrect Usage. flag provided but not defined: -act-as-bundled-plugin To fix this, make sure that the configuration for the terraform ruleset, in the .tflint.hcl file contains a version constraint: plugin \"terraform\" { enabled = true version = \"0.2.1\" source = \"github.com/terraform-linters/tflint-ruleset-terraform\"} Edit page Previous Authentication Next Auto-init",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.184Z"
  },
  {
    "title": "Auto-init",
    "url": "https://terragrunt.gruntwork.io/docs/features/auto-init/",
    "content": "Auto-init Auto-Init is a feature of Terragrunt that makes it so that terragrunt init does not need to be called explicitly before other terragrunt commands. When Auto-Init is enabled (the default), terragrunt will automatically call tofu init/terraform init before other commands (e.g. terragrunt plan) when terragrunt detects that any of the following are true: init has never been called. source needs to be downloaded. The .terragrunt-init-required file is in the downloaded module directory (.terragrunt-cache/aaa/bbb/modules/<module>). The modules or remote state used by a module have changed since the previous call to init. As mentioned, extra_arguments can be configured to allow customization of the tofu init command. Note that there might be cases where Terragrunt does not detect that tofu init needs to be called. In such cases, OpenTofu/Terraform may fail, and re-running terragrunt init can resolve the issue. Disabling Auto-InitSection titled “Disabling Auto-Init” In some cases, it might be desirable to disable Auto-Init. For example, you might want to specify a different -plugin-dir option to tofu init (and don’t want to have it set in extra_arguments). To disable Auto-Init, use the --no-auto-init command line option or set the TG_NO_AUTO_INIT environment variable to true. Disabling Auto-Init requires you to explicitly run terragrunt init before executing any other Terragrunt commands for that configuration. If Auto-Init is disabled and Terragrunt detects that init should have been run, it will throw an error. Edit page Previous Hooks Next Feature Flags, Errors and Excludes",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.209Z"
  },
  {
    "title": "Feature Flags, Errors and Excludes",
    "url": "https://terragrunt.gruntwork.io/docs/features/runtime-control/",
    "content": "Feature Flags, Errors and Excludes Sometimes, you need to have Terragrunt behave differently at runtime due to specific context that you have in your environment. The following configuration blocks have been designed to work together in concert to provide you a great deal of flexibility in how Terragrunt behaves at runtime. Feature FlagsSection titled “Feature Flags” Defined using the feature configuration block, Terragrunt allows for the control of specific features at runtime using feature flags. For example: feature \"s3_version\" { default = \"v1.0.0\"} terraform { source = \"git::git@github.com:acme/infrastructure-modules.git//storage/s3?ref=${feature.s3_version.value}\"} The configuration above allows you to set a default version for the s3_version feature flag, controlling the tag used for fetching the s3 module from the infrastructure-modules repository. At runtime, you can override the default value by doing one of the following: terragrunt apply --feature s3_version=v1.1.0 Or by setting the corresponding environment variable: export TG_FEATURE=\"s3_version=v1.1.0\"terragrunt apply This can be a useful way to opt in to new features or to test changes in your infrastructure. Setting a different version of an OpenTofu/Terraform module in a lower environment can be useful for testing changes before rolling them out to production. Users will always use the default version unless they explicitly set a different value. ErrorsSection titled “Errors” Defined using the errors configuration block, Terragrunt allows for fine-grained control of errors at runtime. For example: errors { # Retry block for transient errors retry \"transient_errors\" { retryable_errors = [\".*Error: transient network issue.*\"] max_attempts = 3 sleep_interval_sec = 5 } # Ignore block for known safe-to-ignore errors ignore \"known_safe_errors\" { ignorable_errors = [ \".*Error: safe warning.*\", \"!.*Error: do not ignore.*\" ] message = \"Ignoring safe warning errors\" signals = { alert_team = false } }} This configuration allows for control over how Terragrunt handles errors at runtime. In the example above, Terragrunt will retry up to three times with a five-second pause between each retry for any error that matches the regex .*Error: transient network issue.*. It will also ignore any error that matches the regex .*Error: safe warning.*, but will not ignore any error that matches the regex .*Error: do not ignore.*. When it ignores an error that it can safely ignore, it will output the message Ignoring safe warning errors, and will generate a file named error-signals.json in the working directory with the following content: { \"alert_team\": false} You can learn more about how this configuration block works in the documentation linked above, but for now, what’s important to know is that it allows you to determine what Terragrunt should do when it encounters an error at runtime. Note that these configurations can also be adjusted dynamically. You can use a combination of feature flags and errors to control how Terragrunt behaves at runtime. Say, for example, a developer was trying to roll out a new version of your module that is known to be potentially flaky. You want to integrate your new module update with the rest of your team, but you don’t want to break runs that aren’t ready for the new module. You could use a feature flag to control introduction of that new module, and an error block to ignore any errors that you know are safe to ignore. feature \"enable_flaky_module\" { default = false} locals { version = feature.enable_flaky_module.value ? \"v1.0.0\" : \"v1.1.0\"} terraform { source = \"git::git@github.com:acme/infrastructure-modules.git//storage/s3?ref=${local.version}\"} errors { # Ignore errors when set ignore \"flaky_module_errors\" { ignorable_errors = feature.enable_flaky_module.value ? [ \".*Error: flaky module error.*\" ] : [] message = \"Ignoring flaky module error\" signals = { send_notification = true } }} In this example, the enable_flaky_module feature flag sets both the version of the module, and the error handling behavior for the unit that consumes it. This would allow the developer to integrate the unit configuration update with the rest of the codebase, enable the flag that introduces the module update in a lower environment, and then ignore any errors that are known to be safe to ignore. This pattern allows for greater speed of integration with larger codebases, and can be a useful tool for managing risk in your infrastructure. ExcludesSection titled “Excludes” Defined using the exclude configuration block, Terragrunt allows for the exclusion of specific units at runtime. For example: locals { day_of_week = formatdate(\"EEE\", timestamp()) ban_deploy = contains([\"Fri\", \"Sat\", \"Sun\"], local.day_of_week)} exclude { if = local.ban_deploy actions = [\"apply\", \"destroy\"]} In this example, the exclude block will prevent the apply command from running in a given unit on Fridays, Saturdays, and Sundays, as all good DevOps engineers know that deploying that close to a weekend is a recipe for disaster. While a toy example, this demonstrates how you can use the exclude block to use dynamic information at runtime to control the run queue. You can use this block to prevent certain units from running in certain environments, or to prevent certain commands from running in certain units. Note that, just like with the other blocks mentioned so far, you can use a combination of configurations mentioned here to ensure that Terragrunt behaves exactly as you need it to at runtime. A more practical use of the exclude block would be to control which environments are run in run --all commands. For example: feature \"dev\" { default = true} exclude { if = !feature.dev.value actions = [\"all_except_output\"]} feature \"stage\" { default = false} exclude { if = !feature.stage.value actions = [\"all_except_output\"]} feature \"prod\" { default = false} exclude { if = !feature.prod.value actions = [\"all_except_output\"]} In this example, the dev, stage and prod directories have their own root configurations that are included by all units in their respective environments. The assumption of a configuration like this is that each environment is fully self-contained, and that the team has a desire to always update dev units, but wants to opt in changes to stage and prod units. In this setup, any run --all command like the following: terragrunt run --all plan Will exclude all units in both the stage and prod directories, as the feature block in each of those directories is set to false by default. As a result, the only units that are run are those in the dev directory. When a user wants to opt in to updates for the stage environment, they could do something like this: terragrunt run --all --feature stage=true plan They can even mix and match feature flags to opt-in/out of multiple environments at once: terragrunt run --all --feature dev=false --feature stage=true --feature prod=true plan This allows for a great deal of flexibility in how you programmatically control the behavior of Terragrunt at runtime. Exclusion from the Run QueueSection titled “Exclusion from the Run Queue” The exclude block will only exclude the unit from the run queue, which is only relevant in the context of a run --all command. A user could still explicitly navigate to the unit directory and run the command manually. If you would like to explicitly prevent a command from being run, even if a user was to navigate to the unit directory and run the command manually, you can use a combination of the exclude block and a before_hook block to prevent the command from running. For example: locals { day_of_week = formatdate(\"EEE\", timestamp()) ban_deploy = contains([\"Fri\", \"Sat\", \"Sun\"], local.day_of_week)} exclude { if = local.ban_deploy actions = [\"apply\", \"destroy\"]} terraform { before_hook \"prevent_deploy\" { commands = [\"apply\", \"destroy\"] execute = local.ban_deploy ? [\"bash\", \"-c\", \"echo 'Deploying on weekends is not allowed. Go home.' && exit 1\"] : [] }} Note that this will result in an exit code of 1, rather than merely excluding the unit from the run queue, which is slightly different behavior. Edit page Previous Auto-init Next Provider Cache Server",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.247Z"
  },
  {
    "title": "Provider Cache Server",
    "url": "https://terragrunt.gruntwork.io/docs/features/provider-cache-server/",
    "content": "Provider Cache Server Terragrunt has the ability to cache OpenTofu/Terraform providers across all OpenTofu/Terraform runs. The Provider Cache Server feature ensures that each provider is only ever downloaded and stored on disk exactly once by running a local provider cache server while Terragrunt runs OpenTofu/Terraform commands. The Provider Cache Server is a performance optimization. For more details on performance optimizations, their tradeoffs, and other performance tips, read the dedicated Performance documentation. Why caching is usefulSection titled “Why caching is useful” Let’s imagine that your project consists of 50 Terragrunt units, and each of them uses the same aws provider. Without caching, each of them will download the provider from the Internet, and store it in its own .terraform directory. For clarity, the downloadable archive terraform-provider-aws_5.36.0_darwin_arm64.zip has a size of ~100MB, and when unzipped it takes up ~450MB of disk space. It’s easy to calculate that initializing such a project with 50 modules will cost you 5GB of traffic and 22.5GB of free space instead of 100MB and 450MB using the cache. Why OpenTofu/Terraform’s built-in provider caching doesn’t workSection titled “Why OpenTofu/Terraform’s built-in provider caching doesn’t work” OpenTofu/Terraform has a provider caching feature, the Provider Plugin Cache, that does the job well… unless you run multiple OpenTofu/Terraform processes simultaneously, such as when you use terragrunt run --all. Then the OpenTofu/Terraform processes begin conflict by overwriting each other’s cache, which causes an error such as Error: Failed to install provider. As a result, Terragrunt previously had to disable concurrency for init steps in run --all, which is significantly slower. If you enable Terragrunt Provider Caching, as described in this section, that will no longer be necessary, and you should see significant performance improvements with init, as well as significant savings in terms of bandwidth and disk space usage. UsageSection titled “Usage” Terragrunt Provider Cache is currently considered an experimental feature, so it is disabled by default. To enable it, you need to use the flag provider-cache: terragrunt run --all apply --provider-cache or the environment variable TG_PROVIDER_CACHE: TG_PROVIDER_CACHE=1 terragrunt run --all apply By default, cached providers are stored in terragrunt/providers folder, which is located in the user cache directory: $HOME/.terragrunt-cache/terragrunt/providers on Unix systems $HOME/Library/Caches/terragrunt/providers on Darwin %LocalAppData%\\terragrunt\\providers on Windows The file structure of the cache directory is identical to the OpenTofu/Terraform plugin_cache_dir directory. If you already have a directory with providers cached by OpenTofu/Terraform plugin_cache_dir, you can set this path using the flag provider-cache-dir, to enable the Provider Cache Server to reuse existing cached providers. terragrunt plan \\--provider-cache \\--provider-cache-dir /new/path/to/cache/dir or the environment variable TG_PROVIDER_CACHE_DIR: TG_PROVIDER_CACHE=1 \\TG_PROVIDER_CACHE_DIR=/new/path/to/cache/dir \\terragrunt plan By default, Terragrunt only caches providers from the following registries: registry.terraform.io, registry.opentofu.org. You can override this list using the flag provider-cache-registry-names: terragrunt apply \\--provider-cache \\--provider-cache-registry-names example1.com \\--provider-cache-registry-names example2.com or the environment variable TG_PROVIDER_CACHE_REGISTRY_NAMES: TG_PROVIDER_CACHE=1 \\TG_PROVIDER_CACHE_REGISTRY_NAMES=example1.com,example2.com \\terragrunt apply How Terragrunt Provider Caching worksSection titled “How Terragrunt Provider Caching works” Start a server on localhost. This is the Terragrunt Provider Cache server. Configure OpenTofu/Terraform instances to use the Terragrunt Provider Cache server as a remote registry: Create local CLI config file .terraformrc for each module that concatenates the user configuration from the OpenTofu/Terraform CLI config file with additional sections: provider-installation forces OpenTofu/Terraform to look for the required providers in the cache directory and create symbolic links to them, if not found, then request them from the remote registry. host forces OpenTofu/Terraform to forward all provider requests through the Terragrunt Provider Cache server. The address link contains UUID and is unique for each module, used by Terragrunt Provider Cache server to associate modules with the requested providers. Set environment variables: TF_CLI_CONFIG_FILE sets to use just created local CLI config .terragrunt-cache/.terraformrc TFTOKEN* sets per-remote-registry tokens for authentication to Terragrunt Provider Cache server. Any time Terragrunt is going to run init: Call tofu/terraform init. This gets OpenTofu/Terraform to request all the providers it needs from the Terragrunt Provider Cache server. The Terragrunt Provider Cache server will download the provider from the remote registry, unpack and store it into the cache directory or create a symlink if the required provider exists in the user plugins directory. Note that the Terragrunt Provider Cache server will ensure that each unique provider is only ever downloaded and stored on disk once, handling concurrency (from multiple OpenTofu/Terraform and Terragrunt instances) correctly. Along with the provider, the cache server downloads hashes and signatures of the providers to check that the files are not corrupted. The Terragrunt Provider Cache server returns the HTTP status 423 Locked to OpenTofu/Terraform. This is because we do not want OpenTofu/Terraform to actually download any providers as a result of calling tofu/terraform init; we only use that command to request the Terragrunt Provider Cache Server to start caching providers. At this point, all providers are downloaded and cached, so finally, we run terragrunt init a second time, which will find all the providers it needs in the cache, and it’ll create symlinks to them nearly instantly, with no additional downloading. Note that if a OpenTofu/Terraform module doesn’t have a lock file, OpenTofu/Terraform does not use the cache, so it would end up downloading all the providers from scratch. To work around this, we generate .terraform.lock.hcl based on the request made by tofu/terraform init to the Terragrunt Provider Cache server. Since terraform init only requests the providers that need to be added/updated, we can keep track of them using the Terragrunt Provider Cache server and update the OpenTofu/Terraform lock file with the appropriate hashes without having to parse tf configs. Reusing providers from the user plugins directorySection titled “Reusing providers from the user plugins directory” Some plugins for some operating systems may not be available in the remote registries. Thus, the cache server will not be able to download the requested provider. As an example, plugin template v2.2.0 for darwin-arm64, see Template v2.2.0 does not have a package available - Mac M1. The workaround is to compile the plugin from source code and put it into the user plugins directory or use the automated solution https://github.com/kreuzwerker/m1-terraform-provider-helper. For this reason, the cache server first tries to create a symlink from the user’s plugin directory if the required provider already exists there: %APPDATA%\\terraform.d\\plugins on Windows ~/.terraform.d/plugins on other systems How forwarding requests through the Provider Cache Server worksSection titled “How forwarding requests through the Provider Cache Server works” OpenTofu/Terraform has an official documented setting network_mirror, that works great, but has one major drawback for the local cache server - the need to use an HTTPS connection with a trusted certificate. Fortunately, there is another way - using the undocumented host setting, which allows OpenTofu/Terraform to create connections to the caching server over HTTP. Provider Cache with providers lock commandSection titled “Provider Cache with providers lock command” If you run providers lock with enabled Terragrunt Provider Cache, Terragrunt creates the provider cache and generates the lock file on its own, without running terraform providers lock at all. terragrunt providers lock -platform=linux_amd64 -platform=darwin_arm64 -platform=freebsd_amd64 \\--provider-cache Configure the Provider Cache ServerSection titled “Configure the Provider Cache Server” Since the Provider Cache Server is essentially a Private Registry server that accepts requests from OpenTofu/Terraform, downloads and saves providers to the cache directory, there are a few more flags that are unlikely to be needed, but are useful to know about: provider-cache-hostname - Default: localhost. provider-cache-port - Default: Assigned random port automatically. provider-cache-token - Default: Generated randomly. To enhance security, the Terragrunt Provider Cache has authentication to prevent unauthorized connections from third-party applications. You can set your own token using any character set. terragrunt apply \\--provider-cache \\--provider-cache-host 192.168.0.100 \\--provider-cache-port 5758 \\--provider-cache-token my-secret or using environment variables: TG_PROVIDER_CACHE=1 \\TG_PROVIDER_CACHE_HOST=192.168.0.100 \\TG_PROVIDER_CACHE_PORT=5758 \\TG_PROVIDER_CACHE_TOKEN=my-secret \\terragrunt apply Edit page Previous Feature Flags, Errors and Excludes Next IaC Engines",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.284Z"
  },
  {
    "title": "IaC Engines",
    "url": "https://terragrunt.gruntwork.io/docs/features/engine/",
    "content": "IaC Engines IaC engines allow you to customize and configure how IaC updates are orchestrated by Terragrunt. This feature is still experimental and not recommended for general production usage. To try it out, all you need to do is include the following in your terragrunt.hcl: engine { source = \"github.com/gruntwork-io/terragrunt-engine-opentofu\" version = \"v0.0.22\"} This example leverages the official OpenTofu engine, publicly available on GitHub. This engine currently leverages the locally available installation of the tofu binary, just like Terragrunt does by default without use of engine configurations. It provides a convenient example of how to build engines for Terragrunt. In the future, this engine will expand in capability to include additional features and configurations. Since this functionality is experimental and not recommended for production, set the following environment variable to enable it: export TG_EXPERIMENTAL_ENGINE=1 Use CasesSection titled “Use Cases” IaC Engines were introduced to offer advanced users of Terragrunt a level of customization over how exactly IaC updates are performed with a given set of Terragrunt configurations. Without usage of IaC Engines, Terragrunt will determine how IaC updates will be performed by doing things like invoking the tofu or terraform binary directly. For most users, this is fine. However, advanced users have more complex use cases that require more control over how those IaC updates are executed, given certain Terragrunt configurations. e.g. Emitting custom logging or metrics whenever the tofu binary is executed. Running tofu in a remote environment, such as a separate Kubernetes pod from the one executing Terragrunt. Using different versions of tofu for different Terragrunt configurations in the same run --all execution. HTTPS SourcesSection titled “HTTPS Sources” Use an HTTP(S) URL to specify the path to the engine: engine { source = \"https://github.com/gruntwork-io/terragrunt-engine-opentofu/releases/download/v0.0.22/terragrunt-iac-engine-opentofu_rpc_v0.0.22_linux_amd64.zip\"} Local SourcesSection titled “Local Sources” Specify a local absolute path as the source: engine { source = \"/home/users/iac-engines/terragrunt-iac-engine-opentofu_v0.0.22\"} ParametersSection titled “Parameters” source: (Required) The source of the plugin. Multiple engine approaches are supported, including GitHub repositories, HTTP(S) paths, and local absolute paths. version: (Optional) The version of the engine to download from GitHub releases. If not specified, the latest release is always downloaded. type: (Optional) Currently, the only supported type is rpc. meta: (Optional) A block for setting engine-specific metadata. This can include various configuration settings required by the engine. CachingSection titled “Caching” Engines are cached locally by default to enhance performance and minimize repeated downloads. The cached engines are stored in the following directory by default: ~/.cache/terragrunt/plugins/iac-engine/rpc/<version> If you need to use a different path, set the environment variable TG_ENGINE_CACHE_PATH accordingly. Downloaded engines are checked for integrity using the SHA256 checksum GPG key. If the checksum does not match, the engine is not executed. To disable this feature, set the environment variable: export TG_ENGINE_SKIP_CHECK=0 To configure a custom log level for the engine, set the TG_ENGINE_LOG_LEVEL environment variable to one of debug, info, warn, error. export TG_ENGINE_LOG_LEVEL=debug Engine MetadataSection titled “Engine Metadata” The meta block is used to pass metadata to the engine. This metadata can be used to configure the engine or pass additional information to the engine. The metadata block is a map of key-value pairs. Engines can read the information passed via the metadata map to configure themselves. engine { source = \"github.com/gruntwork-io/terragrunt-engine-opentofu\" # Optionally set metadata for the engine. meta = { key_1 = [\"value1\", \"value2\"] key_2 = \"1.6.0\" }} Configurations you might want to set with meta include: Connection configurations Tool versions Feature flags Other configurations that the engine might want to be variable in different terragrunt.hcl files Edit page Previous Provider Cache Server Next Content Addressable Store (CAS)",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.318Z"
  },
  {
    "title": "Content Addressable Store (CAS)",
    "url": "https://terragrunt.gruntwork.io/docs/features/cas/",
    "content": "Content Addressable Store (CAS) Terragrunt supports a Content Addressable Store (CAS) to deduplicate content across multiple Terragrunt configurations. This feature is still experimental and not recommended for general production usage. The CAS is used to speed up both catalog cloning and OpenTofu/Terraform source cloning by avoiding redundant downloads of Git repositories. To use the CAS, you will need to enable the cas experiment. UsageSection titled “Usage” When you enable the cas experiment, Terragrunt will automatically use the CAS when cloning any compatible source (Git repositories). Catalog UsageSection titled “Catalog Usage” catalog { urls = [ \"git@github.com:acme/modules.git\" ]} OpenTofu/Terraform Source UsageSection titled “OpenTofu/Terraform Source Usage” terraform { source = \"git@github.com:acme/infrastructure-modules.git//vpc?ref=v1.0.0\"} When Terragrunt clones a repository while using the CAS, if the repository is not found in the CAS, Terragrunt will clone the repository from the original URL and store it in the CAS for future use. When generating a repository from the CAS, Terragrunt will hard link entries from the CAS to the new repository. This allows Terragrunt to deduplicate content across multiple repositories. In the event that hard linking fails due to some operating system / host incompatibility with hard links, Terragrunt will fall back to performing copies of the content from the CAS. StorageSection titled “Storage” The CAS is stored in the ~/.cache/terragrunt/cas directory. This directory can be safely deleted at any time, as Terragrunt will automatically regenerate the CAS as needed. Avoid partial deletions of the CAS directory without care, as that might result in partially cloned repositories and unexpected behavior. Edit page Previous IaC Engines Next Run Report",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.373Z"
  },
  {
    "title": "Run Report",
    "url": "https://terragrunt.gruntwork.io/docs/features/run-report/",
    "content": "Run Report Terragrunt uses an internal data store to track the results of runs when multiple are done at once. You can view this data, both with a high-level summary that is displayed at the end of each run, and via a detailed report that can be requested on-demand (coming soon). Run SummarySection titled “Run Summary” By default, when performing a queue-based run (e.g. terragrunt run --all plan), Terragrunt will emit some additional information to the console after the run is complete. $ terragrunt run --all plan # Omitted for brevity... ❯❯ Run Summary 3 units 62ms ──────────────────────────── Succeeded 3 This output is called the “Run Summary”. It provides at-a-glance information about the run that was just performed, including the following (as relevant): Duration: The duration of the run. Units: The number of units that were run. Succeeded: The number of units that succeeded (if any did). Failed: The number of units that failed (if any did). Excluded: The number of units that were excluded from the run (if any were). Early Exits: The number of units that exited early, due to a failure in a dependency (if any did). Showing Unit DurationsSection titled “Showing Unit Durations” You can enable showing the duration of each unit in the run summary by using the --summary-per-unit flag. $ terragrunt run --all plan --summary-per-unit # Omitted for brevity... ❯❯ Run Summary 3 units 10m ────────────────────────────── Succeeded (3) long-running-unit 10m medium-running-unit 12s short-running-unit 5ms The units are sorted by duration, with the longest-running units shown first. Disabling the summarySection titled “Disabling the summary” You can disable the summary output by using the --summary-disable flag. terragrunt run --all plan --summary-disable The internal report will still be tracked, and is available for generation if requested. Run ReportSection titled “Run Report” Optionally, you can also generate a detailed report of the run, which has all the information used to generate the run summary. terragrunt run --all plan --report-file report.csv You can specify the format of the report using the --report-format flag, which supports either csv or json: terragrunt run --all plan --report-file report.json --report-format json The format can also be inferred from the file extension. If no format is specified and the file has no extension, CSV will be used by default: # Will generate a CSV reportterragrunt run --all plan --report-file report # Will generate a JSON reportterragrunt run --all plan --report-file report.json # Will generate a CSV reportterragrunt run --all plan --report-file report.csv The report will be generated in the specified format at the given path in the current working directory. Here’s an example of what the CSV format looks like: Name,Started,Ended,Result,Reason,Causefirst-exclude,2025-06-05T16:28:41-04:00,2025-06-05T16:28:41-04:00,excluded,exclude block,second-exclude,2025-06-05T16:28:41-04:00,2025-06-05T16:28:41-04:00,excluded,exclude block,first-failure,2025-06-05T16:28:41-04:00,2025-06-05T16:28:42-04:00,failed,run error,first-success,2025-06-05T16:28:41-04:00,2025-06-05T16:28:41-04:00,succeeded,,second-failure,2025-06-05T16:28:41-04:00,2025-06-05T16:28:42-04:00,failed,run error,second-success,2025-06-05T16:28:41-04:00,2025-06-05T16:28:41-04:00,succeeded,,second-early-exit,2025-06-05T16:28:42-04:00,2025-06-05T16:28:42-04:00,early exit,run error,first-early-exit,2025-06-05T16:28:42-04:00,2025-06-05T16:28:42-04:00,early exit,run error, And here’s an example of what the JSON format looks like: [ { \"Name\": \"first-exclude\", \"Started\": \"2025-06-05T16:28:41-04:00\", \"Ended\": \"2025-06-05T16:28:41-04:00\", \"Result\": \"excluded\", \"Reason\": \"exclude block\" }, { \"Name\": \"first-success\", \"Started\": \"2025-06-05T16:28:41-04:00\", \"Ended\": \"2025-06-05T16:28:41-04:00\", \"Result\": \"succeeded\" }] You can use this file to determine details for each unit run, including the name of the unit, the start and end times, the result, the reason for that result, and the cause for that reason. Note that in the JSON format, empty fields (Reason and Cause) are omitted entirely rather than being set to empty values. In general, the schema for this report should change infrequently, but we’ll try to keep it up to date here. You can also generate a JSON schema file for the report, so that you have a programmatic way to validate that the report is going to conform to an expected schema. terragrunt run --all plan --report-schema-file report.schema.json The schema will be generated at the given path in the current working directory. The generated schema conforms to the JSON Schema standard. This generated schema will look like the following: { \"items\": { \"$schema\": \"https://json-schema.org/draft/2020-12/schema\", \"$id\": \"https://terragrunt.gruntwork.io/schemas/run/report/v1/schema.json\", \"properties\": { \"Started\": { \"type\": \"string\", \"format\": \"date-time\" }, \"Ended\": { \"type\": \"string\", \"format\": \"date-time\" }, \"Reason\": { \"type\": \"string\", \"enum\": [ \"retry succeeded\", \"error ignored\", \"run error\", \"--queue-exclude-dir\", \"exclude block\", \"ancestor error\" ] }, \"Cause\": { \"type\": \"string\" }, \"Name\": { \"type\": \"string\" }, \"Result\": { \"type\": \"string\", \"enum\": [ \"succeeded\", \"failed\", \"early exit\", \"excluded\" ] } }, \"additionalProperties\": false, \"type\": \"object\", \"required\": [ \"Started\", \"Ended\", \"Name\", \"Result\" ], \"title\": \"Terragrunt Run Report Schema\", \"description\": \"Schema for Terragrunt run report\" }, \"type\": \"array\", \"title\": \"Terragrunt Run Report Schema\", \"description\": \"Array of Terragrunt runs\"} Note the $id field, which is used to identify the schema. This is useful to quickly determine which version of the schema is being used. You can also fetch the schema remotely from that URL. ResultsSection titled “Results” Results are high level outcomes of a unit run, and will always be one of the following: succeeded: The unit run succeeded. failed: The unit run failed. excluded: The unit was excluded from the run. early exit: The unit exited early, due to a failure in a dependency. ReasonsSection titled “Reasons” Reasons are more granular details of those results, and will always be one of the following, based on the result of the unit run: succeeded: “: When the unit run succeeded without any special conditions, an empty string will be found here. retry succeeded: When the unit run initially failed, but was retried due to a retry block, and succeeded on a subsequent attempt, you can expect to see a value of retry succeeded here. error ignored: When the unit run failed, but the error was ignored due to an ignore block, you can expect to see a value of error ignored here. failed: run error: When the unit run failed due to a run error, you can expect to see a value of run error here. excluded: exclude block: When the unit was excluded from the run due to an exclude block, you can expect to see a value of exclude block here. --queue-exclude-dir: When the unit was excluded from the run due use of a --queue-exclude-dir flag, you can expect to see a value of --queue-exclude-dir here. early exit: ancestor error: When the unit exited early due to an error in the run of a dependency, you can expect to see a value of ancestor error here. CausesSection titled “Causes” Causes indicate the specific reason for a given result, and are generally not guessable. These provide information on the exact mechanism that caused the result. error ignored: You will find the name of the ignore block that resulted in the error being ignored. run error: You will find the actual error message of the unit that failed. ancestor error: You will find the name of the unit that failed. Note The retry succeeded reason does not have a cause. The reason for this is that backwards compatibility with the retryable_errors attribute prevents consistent reporting of the cause, as the retryable_errors attribute doesn’t have a label. In the future, once the retryable_errors attribute is removed, a cause can be added here. Edit page Previous Content Addressable Store (CAS) Next Automatic Provider Cache Dir",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.422Z"
  },
  {
    "title": "Automatic Provider Cache Dir",
    "url": "https://terragrunt.gruntwork.io/docs/features/auto-provider-cache-dir/",
    "content": "Automatic Provider Cache Dir This feature has been stabilized and is now enabled by default when using OpenTofu >= 1.10. Automatic Provider Cache Dir is a feature of Terragrunt that automatically configures OpenTofu’s native provider caching mechanism by setting the TF_PLUGIN_CACHE_DIR environment variable. This enables efficient provider caching without the need to manually configure provider cache directories or use Terragrunt’s provider cache server. When using OpenTofu >= 1.10, Terragrunt will automatically configure OpenTofu to use a shared provider cache directory, which provides several benefits: Improved performance: Providers are downloaded once and reused across multiple configurations Reduced bandwidth usage: Eliminates redundant provider downloads Better concurrency: OpenTofu 1.10+ handles concurrent access to the provider cache safely Simplified setup: No need for manual provider cache configuration RequirementsSection titled “Requirements” The Automatic Provider Cache Dir feature has specific requirements: OpenTofu version >= 1.10 is required Only works with OpenTofu (not Terraform) If requirements are not met, the experiment silently does nothing UsageSection titled “Usage” When using OpenTofu >= 1.10, this feature is enabled by default. No additional configuration is required: terragrunt run --all apply How it WorksSection titled “How it Works” When enabled, Terragrunt automatically: Detects OpenTofu version and ensures it meets the minimum requirement (>= 1.10) Sets up provider cache directory using the default cache location or a custom path Configures TF_PLUGIN_CACHE_DIR environment variable for OpenTofu processes Ensures directory exists with proper permissions The default provider cache directory is located at: $HOME/.terragrunt-cache/providers on Unix systems $HOME/Library/Caches/terragrunt/providers on macOS %LocalAppData%\\terragrunt\\providers on Windows Customizing the Cache DirectorySection titled “Customizing the Cache Directory” You can customize the provider cache directory using the --provider-cache-dir flag: terragrunt apply --provider-cache-dir /custom/path/to/cache Or with environment variables: TG_PROVIDER_CACHE_DIR='/custom/path/to/cache' terragrunt apply Disabling Auto Provider Cache DirSection titled “Disabling Auto Provider Cache Dir” You can disable the feature for specific runs using the --no-auto-provider-cache-dir flag: terragrunt run --all apply --no-auto-provider-cache-dir This is particularly useful when: You want manual control over provider caching for specific environments Testing configurations without provider caching Using custom provider cache configurations Comparison with Provider Cache ServerSection titled “Comparison with Provider Cache Server” Terragrunt also provides a Provider Cache Server feature. Here’s when to use each: Use Auto Provider Cache Dir when: Using OpenTofu 1.10+ You want a simple, low-maintenance caching solution You prefer native OpenTofu caching mechanisms You need good concurrent access handling Use Provider Cache Server when: Using older versions of OpenTofu/Terraform You need advanced caching features You want to share providers across different filesystems You need custom registry configurations TroubleshootingSection titled “Troubleshooting” If the feature doesn’t seem to be working: Check OpenTofu version: Ensure you’re using OpenTofu 1.10 or later Check cache directory: Ensure the cache directory is accessible and has proper permissions Review environment variables: Verify TF_PLUGIN_CACHE_DIR is not already set by another tool You can enable debug logging to see more information: terragrunt apply --log-level debug Edit page Previous Run Report Next Filters",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.449Z"
  },
  {
    "title": "Filters",
    "url": "https://terragrunt.gruntwork.io/docs/features/filter/",
    "content": "Filters Experimental Feature This feature is currently experimental and not yet complete. See the filter-flag experiment documentation for details on what is and isn’t supported.Usage of the filter flag requires usage of the filter-flag experiment, like so:terragrunt find --experiment filter-flag --filter 'foo'Examples below will omit the --experiment filter-flag flag for brevity.Currently, the --filter flag is only available in the find and list commands. Support for the run command is planned for a future release. The --filter flag provides a sophisticated querying syntax for targeting specific units and stacks in Terragrunt commands. This unified approach offers powerful filtering capabilities using a flexible query language. Filter Syntax OverviewSection titled “Filter Syntax Overview” The filter syntax allows you to target units and stacks using several different approaches. Usage of the filter flag in a command can look like this: $ terragrunt find --filter './prod/** | name=web'prod/services/web Where the result of find above might have been: $ terragrunt findprod/services/webprod/services/apiprod/data/dbdev/services/webdev/services/apidev/data/db For the following file tree: Directoryprod Directoryservices Directoryweb <— Matched by the filter terragrunt.hclDirectoryapi terragrunt.hclDirectorydata Directorydb terragrunt.hclDirectorydev Directoryservices Directoryweb terragrunt.hclDirectoryapi terragrunt.hclDirectorydata Directorydb terragrunt.hcl Filter TypesSection titled “Filter Types” There are several different types of filters, and particular ways in which they can be combined to achieve different results. You can learn more about that below. Name-Based FilteringSection titled “Name-Based Filtering” Match units and stacks by their name. This is the simplest form of filtering. # Exact matchterragrunt find --filter app1 # Glob patternterragrunt find --filter 'app*' Directoryapps Directoryapp1 <— Matched by the first and second filter terragrunt.hclDirectoryapp2 <— Matched only by the second filter terragrunt.hclDirectoryother terragrunt.hcl Note Note that app1 and app2 were selected within the apps directory. Filtering on names will match any unit/stack that has a name that matches the filter. Path-Based FilteringSection titled “Path-Based Filtering” Match units and stacks by their file system path. # Relative pathsterragrunt find --filter './envs/prod/apps/app1'terragrunt find --filter './envs/stage/**' # Absolute pathsterragrunt find --filter '/absolute/path/to/envs/dev/apps/*' # Wrapped paths (useful for explicitly indicating that an expression is for a path if ambiguous)terragrunt find --filter '{./envs/prod/apps/app2}' Directoryenvs Directoryprod Directoryapps Directoryapp1 <— Matched by the first filter terragrunt.hclDirectoryapp2 <— Matched by the fourth filter terragrunt.hclDirectorystage Directoryapps Directoryapp1 <— Matched by the second filter terragrunt.hclDirectoryapp2 <— Also matched by the second filter terragrunt.hclDirectorydev Directoryapps Directoryapp1 <— Matched by the third filter terragrunt.hclDirectoryapp2 <— Also matched by the third filter terragrunt.hcl Note Note that globs used in path-based filtering will not recursively match nested directories unless you use the ** wildcard.(That’s why ./envs/stage/** is used above) Attribute-Based FilteringSection titled “Attribute-Based Filtering” Match units and stacks by their configuration attributes. # Filter by component typeterragrunt find --filter 'type=unit'terragrunt find --filter 'type=stack' # Filter by external dependency statusterragrunt find --dependencies --external --filter 'external=false'terragrunt find --dependencies --external --filter 'external=true' # Explicitly filter by name (useful for explicitly indicating that an expression is for a name if ambiguous)terragrunt find --filter 'name=stack*' Directory. Directoryunit1 <— Selected by the first filter terragrunt.hclDirectorystack1 <— Selected by the second and fifth filter terragrunt.stack.hclDirectory.. Directorydependencies <— Note that this directory is sibling to the current working directory Directorydependency-of-app1 <— Matched by the fourth filter, but not the third filter terragrunt.hcl NegationSection titled “Negation” Exclude units and stacks using the ! prefix. # Exclude by nameterragrunt find --filter '!app1' # Exclude by pathterragrunt find --filter '!./prod/**' # Exclude by typeterragrunt find --filter '!type=stack' Directoryenvs Directoryprod Directoryapps Directoryapp1 <— Excluded by both the first and second filter terragrunt.hclDirectoryapp2 <— Matched by all filters except the second filter terragrunt.hclDirectorystacks Directorystack1 <— Excluded by both the second and third filter terragrunt.stack.hclDirectorystage Directoryapps Directoryapp1 <— Matched by all filters except the first filter terragrunt.hclDirectoryapp2 <— Matched by all filters terragrunt.hclDirectorystacks Directorystack1 <— Matched by all filters except the third filter terragrunt.stack.hcl Intersection (Refinement)Section titled “Intersection (Refinement)” Use the | operator to refine results. The right side of the operator is applied to the results from the left side. # Find all components in ./prod/** that are also unitsterragrunt find --filter './prod/** | type=unit' # Find all components in ./prod/** that are not unitsterragrunt find --filter './prod/** | !type=unit' # You can chain as many filters as you want to further refine the resultsterragrunt find --filter './dev/** | type=unit | !name=unit1' Directoryprod Directoryunits Directoryunit1 <— Matched by first filter terragrunt.hclDirectoryunit2 <— Matched by first filter terragrunt.hclDirectorystacks Directorystack1 <— Matched by second filter terragrunt.stack.hclDirectorystack2 <— Matched by second filter terragrunt.stack.hclDirectorydev Directoryunits Directoryunit1 terragrunt.hclDirectoryunit2 <— Matched by third filter terragrunt.hclDirectorystacks Directorystack1 terragrunt.stack.hclDirectorystack2 terragrunt.stack.hcl Union (Multiple Filters)Section titled “Union (Multiple Filters)” Specify multiple --filter flags to combine results using OR logic. # Find components named 'unit1' OR 'stack1'terragrunt find --filter unit1 --filter stack1 # Find components in ./envs/prod/* OR ./envs/stage/*terragrunt find --filter './envs/prod/*' --filter './envs/stage/*' # Find components named 'stack2' _except_ those in ./envs/prod/* and ./envs/stage/*terragrunt find --filter stack2 --filter '!./envs/prod/**' --filter '!./envs/stage/**' Directoryenvs Directoryprod Directoryunit1 <— Matched by the first filter and the second filter terragrunt.hclDirectoryunit2 <— Matched by the second filter terragrunt.hclDirectorystack1 <— Matched by the first filter and the second filter terragrunt.stack.hclDirectorystack2 <— Matched by the second filter terragrunt.stack.hclDirectorystage Directoryunit1 <— Matched by the first filter and the second filter terragrunt.hclDirectoryunit2 <— Matched by the second filter terragrunt.hclDirectorystack1 <— Matched by the first filter and the second filter terragrunt.stack.hclDirectorystack2 <— Matched by the second filter terragrunt.stack.hclDirectorydev Directoryunit1 <— Matched by the first filter terragrunt.hclDirectoryunit2 terragrunt.hclDirectorystack1 <— Matched by the first filter terragrunt.stack.hclDirectorystack2 <— Matched by the third filter terragrunt.stack.hcl Unions of negated filters When a filter query starts with a negation (!), the result is applied after all positive filters have been applied.This means that if you have a filter query like this:terragrunt find --filter '!type=unit' --filter 'name=unit1'The result will be the components that are not units and are named unit1.As a result, you should be able to expect any negative filter to take effect, regardless of how other positive filters may result in the addition of results. Usage with CommandsSection titled “Usage with Commands” The following commands all support the --filter flag, and use it to filter results in the same way: find list run (planned for a future release) hcl fmt (planned for a future release) hcl validate (planned for a future release) This flag is intended to be a flexible way to target specific infrastructure that allows you to dry-run infrastructure targeting using discovery commands (like find and list) before running a command that actually affects infrastructure (like run). Note Note that these commands may have different logic for discovery, which is the process of surfacing the initial set of components to filter, but the filtering logic itself should be the same. Comparison with Queue Control FlagsSection titled “Comparison with Queue Control Flags” The --filter flag provides a unified alternative to multiple queue control flags: Legacy FlagFilter Equivalent--queue-include-dir--filter './path/*'--queue-exclude-dir--filter '!./path/*'--queue-exclude-external--filter '!external=true'--queue-include-external--filter 'external=true' Edit page Previous Automatic Provider Cache Dir Next Overview",
    "section": "features",
    "lastUpdated": "2025-10-23T16:40:58.499Z"
  },
  {
    "title": "Overview",
    "url": "https://terragrunt.gruntwork.io/docs/reference/hcl/",
    "content": "Overview Terragrunt configuration is defined in HCL files. This uses the same HCL syntax as OpenTofu/Terraform itself. Here’s an example: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} dependencies { paths = [\"../vpc\", \"../mysql\", \"../redis\"]} The core of Terragrunt configuration is that of the unit, which is canonically defined using terragrunt.hcl files. Terragrunt also supports JSON-serialized HCL defined in terragrunt.hcl.json files. Where terragrunt.hcl is mentioned in documentation, you can always use terragrunt.hcl.json instead. When determining the configuration for a unit, Terragrunt figures out the path to its configuration file according to the following rules: The value of the --config command-line option, if specified. The value of the TG_CONFIG environment variable, if defined. A terragrunt.hcl file in the current working directory, if it exists. A terragrunt.hcl.json file in the current working directory, if it exists. If none of these are found, exit with an error. Refer to the following pages for a complete reference of supported features in the terragrunt configuration file: Blocks Attributes Functions Configuration parsing orderSection titled “Configuration parsing order” It is important to be aware of the terragrunt configuration parsing order when using features like locals and dependency outputs, where you can reference attributes of other blocks in the config in your inputs. For example, because locals are evaluated before dependency blocks, you cannot bind outputs from dependency into locals. On the other hand, for the same reason, you can use locals in the dependency blocks. Currently terragrunt parses the config in the following order: include block locals block Evaluation of values for iam_role, iam_assume_role_duration, iam_assume_role_session_name, and iam_web_identity_token attributes, if defined dependencies block dependency blocks, including calling terragrunt output on the dependent units to retrieve the outputs Everything else The config referenced by include A merge operation between the config referenced by include and the current config. Blocks that are parsed earlier in the process will be made available for use in the parsing of later blocks. Similarly, you cannot use blocks that are parsed later earlier in the process (e.g you can’t reference dependency in locals, include, or dependencies blocks). Note that the parsing order is slightly different when using the --all flag of the run command. When using the --all flag, Terragrunt parses the configuration twice. In the first pass, it follows the following parsing order: include block of all configurations in the tree locals block of all configurations in the tree dependency blocks of all configurations in the tree, but does NOT retrieve the outputs terraform block of all configurations in the tree dependencies block of all configurations in the tree The results of this pass are then used to build the dependency graph of the units in the stack. Once the graph is constructed, Terragrunt will loop through the units and run the specified command. It will then revert to the single configuration parsing order specified above for each unit as it runs the command. This allows Terragrunt to avoid resolving dependency on units that haven’t been applied yet when doing a clean deployment from scratch with run --all apply. StacksSection titled “Stacks” When multiple units, each with their own terragrunt.hcl file exist in child directories of a single parent directory, that parent directory becomes a stack. New to stacks? For a comprehensive introduction to the concept, see our Stacks guide. What is a terragrunt.stack.hcl file?Section titled “What is a terragrunt.stack.hcl file?” A terragrunt.stack.hcl file is a blueprint that defines how to generate Terragrunt configuration programmatically. It tells Terragrunt: What units to create. Where to get their configurations from. Where to place them in the directory structure. What values to pass to each unit. The Two Types of BlocksSection titled “The Two Types of Blocks” unit blocks - Define Individual Infrastructure ComponentsSection titled “unit blocks - Define Individual Infrastructure Components” Purpose: Define a single, deployable piece of infrastructure. Use case: When you want to create a single piece of isolated infrastructure (e.g. a specific VPC, database, or application). Result: Generates a single terragrunt.hcl file in the specified path. stack blocks - Define Reusable Infrastructure PatternsSection titled “stack blocks - Define Reusable Infrastructure Patterns” Purpose: Define a collection of related units that can be reused. Use case: When you have a common, multi-unit pattern (like “dev environment” or “three-tier web application”) that you want to deploy multiple times. Result: Generates another terragrunt.stack.hcl file that can contain more units or stacks. Comparison: unit vs stack blocksSection titled “Comparison: unit vs stack blocks” Aspectunit blockstack blockPurposeDefine a single infrastructure componentDefine a reusable collection of componentsWhen to useFor specific, one-off infrastructure piecesFor patterns of infrastructure pieces that you want provisioned togetherGenerated outputA directory with a single terragrunt.hcl fileA directory with a terragrunt.stack.hcl file The Complete WorkflowSection titled “The Complete Workflow” Author: Write a terragrunt.stack.hcl file with unit and/or stack blocks. Generate: Run terragrunt stack generate to create the actual units*. Deploy: Run terragrunt stack run apply to deploy all units**. * Multiple commands (like stack run or run --all) automatically generate units from terragrunt.stack.hcl files for you. ** You can also just use run --all apply to deploy all units in the stack. Example: Simple Stack with UnitsSection titled “Example: Simple Stack with Units” unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { engine = \"postgres\" version = \"13\" vpc_path = \"../vpc\" }} Running terragrunt stack generate creates: terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.hclterragrunt.values.hclDirectorydatabase terragrunt.hclterragrunt.values.hcl Example: Nested Stack with Reusable PatternsSection titled “Example: Nested Stack with Reusable Patterns” stack \"dev\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"dev\" values = { environment = \"development\" cidr = \"10.0.0.0/16\" }} stack \"prod\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"prod\" values = { environment = \"production\" cidr = \"10.1.0.0/16\" }} The referenced stack might contain: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = values.environment cidr = values.cidr }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { environment = values.environment vpc_path = \"../vpc\" }} For more information on these configuration blocks, see: unit stack locals These special configurations are used by the stack generate command (and all the other stack prefixed commands) to generate units programmatically, on demand. The units they generate are valid unit configurations, and can be read and used as if they were manually authored. Included ConfigurationsSection titled “Included Configurations” When configurations are included via the include configuration block, Terragrunt expects configurations to be valid unit configurations. Generally speaking, any HCL file found in a Terragrunt project that isn’t named terragrunt.hcl, terragrunt.stack.hcl or .terraform.lock.hcl is expected to be partial unit configurations that will be included by a Terragrunt unit. Formatting HCL filesSection titled “Formatting HCL files” You can rewrite the HCL files to a canonical format using the hclfmt command built into terragrunt. Similar to tofu fmt, this command applies a subset of the OpenTofu/Terraform language style conventions, along with other minor adjustments for readability. By default, this command will recursively search for hcl files and format all of them for a given stack. Consider the following file structure: Directoryroot root.hclDirectoryprod terragrunt.hclDirectorydev terragrunt.hclDirectoryqa terragrunt.hclDirectoryservices services.hclDirectoryservice01 terragrunt.hcl If you run terragrunt hcl fmt at the root, this will update: root/root.hcl root/prod/terragrunt.hcl root/dev/terragrunt.hcl root/qa/terragrunt.hcl root/qa/services/services.hcl root/qa/services/service01/terragrunt.hcl You can set --diff option. terragrunt hcl fmt --diff will output the diff in a unified format which can be redirected to your favourite diff tool. diff utility must be presented in PATH. Additionally, there’s a flag --check. terragrunt hcl fmt --check will only verify if the files are correctly formatted without rewriting them. The command will return exit status 1 if any matching files are improperly formatted, or 0 if all matching .hcl files are correctly formatted. You can exclude directories from the formatting process by using the --exclude-dir flag. For example, terragrunt hcl fmt --exclude-dir=qa/services. If you want to format a single file, you can use the --file flag. For example, terragrunt hcl fmt --file qa/services/services.hcl. Edit page Previous Filters Next Blocks",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.538Z"
  },
  {
    "title": "Blocks",
    "url": "https://terragrunt.gruntwork.io/docs/reference/hcl/blocks/",
    "content": "Blocks Terragrunt HCL configuration uses configuration blocks when there’s a structural configuration that needs to be defined for Terragrunt. Think of configuration blocks as a way to control different systems used by Terragrunt, whereas attributes are used to define values for those systems. terraformSection titled “terraform” The terraform block is used to configure how Terragrunt will interact with OpenTofu/Terraform. This includes specifying where to find the OpenTofu/Terraform configuration files, any extra arguments to pass to the tofu/terraform binary, and any hooks to run before or after calling OpenTofu/Terraform. The terraform block supports the following arguments: source (attribute): Specifies where to find OpenTofu/Terraform configuration files. This parameter supports the same syntax as the module source parameter for OpenTofu/Terraform module blocks except for the Terraform registry (see below note), including local file paths, Git URLs, and Git URLS with ref parameters. Terragrunt will download all the code in the repo (i.e. the part before the double-slash //) so that relative paths work correctly between modules in that repo. The source parameter can be configured to pull OpenTofu/Terraform modules from any Terraform module registry using the tfr protocol. The tfr protocol expects URLs to be provided in the format tfr://REGISTRY_HOST/MODULE_SOURCE?version=VERSION. For example, to pull the terraform-aws-modules/vpc/aws module from the public Terraform registry, you can use the following as the source parameter: tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=3.3.0. If you wish to access a private module registry (e.g., Terraform Cloud/Enterprise), you can provide the authentication to Terragrunt as an environment variable with the key TG_TF_REGISTRY_TOKEN. This token can be any registry API token. The tfr protocol supports a shorthand notation where the REGISTRY_HOST can be omitted to default to the public registry. The default registry depends on the wrapped executable: for Terraform, it is registry.terraform.io, and for Opentofu, it is registry.opentofu.org. Additionally, if the environment variable TG_TF_DEFAULT_REGISTRY_HOST is set, this value will be used as the default registry host instead, overriding the standard defaults for the wrapped executable. If you use tfr:/// (note the three /). For example, the following will fetch the terraform-aws-modules/vpc/aws module from the public registry: tfr:///terraform-aws-modules/vpc/aws?version=3.3.0. You can also use submodules from the registry using //. For example, to use the iam-policy submodule from the registry module terraform-aws-modules/iam, you can use the following: tfr:///terraform-aws-modules/iam/aws//modules/iam-policy?version=4.3.0. include_in_copy (attribute): A list of glob patterns (e.g., [\"*.txt\"]) that should always be copied from the same directory containing terragrunt.hcl into the OpenTofu/Terraform working directory. When you use the source param in your Terragrunt config and run terragrunt <command>, Terragrunt will download the code specified at source into a scratch folder (.terragrunt-cache, by default), copy the code in your current working directory into the same scratch folder, and then run tofu <command> (or terraform <command>) in that scratch folder. By default, Terragrunt excludes hidden files and folders during the copy step. This feature allows you to specify glob patterns of files that should always be copied from the Terragrunt working directory. Additional notes: The path should be specified relative to the source directory. This list is also used when using a local file source (e.g., source = \"../modules/vpc\"). For example, if your OpenTofu/Terraform module source contains a hidden file that you want to copy over (e.g., a .python-version file), you can specify that in this list to ensure it gets copied over to the scratch copy (e.g., include_in_copy = [\".python-version\"]). exclude_from_copy (attribute): A list of glob patterns (e.g., [\"*.txt\"]) that should always be skipped from the same directory containing terragrunt.hcl when copying into the OpenTofu/Terraform working directory. All examples valid for include_in_copy can be used here. Note that using include_in_copy and exclude_from_copy are not mutually exclusive. If a file matches a pattern in both include_in_copy and exclude_from_copy, it will not be included. If you would like to ensure that the file is included, make sure the patterns you use for include_in_copy do not match the patterns in exclude_from_copy. Note that if you wish to exclude files from being copied from a terraform module source, you should use the before_hook feature. copy_terraform_lock_file (attribute): In certain use cases, you don’t want to check the terraform provider lock file into your source repository from your working directory as described in Lock File Handling. This attribute allows you to disable the copy of the generated or existing .terraform.lock.hcl from the temp folder into the working directory. Default is true. extra_arguments (block): Nested blocks used to specify extra CLI arguments to pass to the tofu/terraform binary. Learn more about its usage in the Keep your CLI flags DRY use case overview. Supports the following arguments: arguments (required) : A list of CLI arguments to pass to tofu/terraform. commands (required) : A list of tofu/terraform sub commands that the arguments will be passed to. env_vars (optional) : A map of key value pairs to set as environment variables when calling tofu/terraform. required_var_files (optional): A list of file paths to OpenTofu/Terraform vars files (.tfvars) that will be passed in to terraform as -var-file=<your file>. optional_var_files (optional): A list of file paths to OpenTofu/Terraform vars files (.tfvars) that will be passed in to tofu/terraform like required_var_files, only any files that do not exist are ignored. before_hook (block): Nested blocks used to specify command hooks that should be run before tofu/terraform is called. Hooks run from the directory with the OpenTofu/Terraform module, except for hooks related to read-config and init-from-module. These hooks run in the terragrunt configuration directory (the directory where terragrunt.hcl lives). Supports the following arguments: commands (required) : A list of tofu/terraform sub commands for which the hook should run before. execute (required) : A list of command and arguments that should be run as the hook. For example, if execute is set as [\"echo\", \"Foo\"], the command echo Foo will be run. working_dir (optional) : The path to set as the working directory of the hook. Terragrunt will switch directory to this path before running the hook command. Defaults to the terragrunt configuration directory for read-config and init-from-module hooks, and the OpenTofu/Terraform module directory for other command hooks. run_on_error (optional) : If set to true, this hook will run even if a previous hook hit an error, or in the case of “after” hooks, if the OpenTofu/Terraform command hit an error. Default is false. suppress_stdout (optional) : If set to true, the stdout output of the executed commands will be suppressed. This can be useful when there are scripts relying on OpenTofu/Terraform’s output and any other output would break their parsing. if (optional) : hook will be skipped when the argument is set or evaluates to false. after_hook (block): Nested blocks used to specify command hooks that should be run after tofu/terraform is called. Hooks run from the terragrunt configuration directory (the directory where terragrunt.hcl lives). Supports the same arguments as before_hook. error_hook (block): Nested blocks used to specify command hooks that run when an error is thrown. The error must match one of the expressions listed in the on_errors attribute. Error hooks are executed after the before/after hooks. In addition to supporting before and after hooks for all OpenTofu/Terraform commands, the following specialized hooks are also supported: read-config (after hook only): read-config is a special hook command that you can use with the after_hook subblock to run an action immediately after terragrunt finishes loading the config. This hook will run on every invocation of terragrunt. Note that you can only use this hook with after_hooks. Any before_hooks with the command read-config will be ignored. The working directory for hooks associated with this command will be the terragrunt config directory. init-from-module and init: Terragrunt has two stages of initialization: one is to download remote configurations using go-getter; the other is Auto-Init, which configures the backend and downloads provider plugins and modules. If you wish to run a hook when Terragrunt is using go-getter to download remote configurations, use init-from-module for the command. If you wish to execute a hook when Terragrunt is using tofu init/terraform init for Auto-Init, use init for the command. For example, an after_hook for the command init-from-module will run after terragrunt clones the module, while an after_hook for the command init will run after terragrunt runs tofu init/terraform init on the cloned module. Hooks for both init-from-module and init only run if the requisite stage needs to run. That is, if terragrunt detects that the module is already cloned in the terragrunt cache, this stage will be skipped and thus the hooks will not run. Similarly, if terragrunt detects that it does not need to run init in the auto init feature, the init stage is skipped along with the related hooks. The working directory for hooks associated with init-from-module will run in the terragrunt config directory, while the working directory for hooks associated with init will be the OpenTofu/Terraform module. Complete Example: terraform { # Pull the OpenTofu/Terraform configuration at the github repo \"acme/infrastructure-modules\", under the subdirectory # \"networking/vpc\", using the git tag \"v0.0.1\". source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\" # For any OpenTofu/Terraform commands that use locking, make sure to configure a lock timeout of 20 minutes. extra_arguments \"retry_lock\" { commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] } # You can also specify multiple extra arguments for each use case. Here we configure terragrunt to always pass in the # `common.tfvars` var file located by the parent terragrunt config. extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] required_var_files = [\"${get_parent_terragrunt_dir()}/common.tfvars\"] } # The following are examples of how to specify hooks # Before apply or plan, run \"echo Foo\". before_hook \"before_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Foo\"] } # Before apply, run \"echo Bar\". Note that blocks are ordered, so this hook will run after the previous hook to # \"echo Foo\". In this case, always \"echo Bar\" even if the previous hook failed. before_hook \"before_hook_2\" { commands = [\"apply\"] execute = [\"echo\", \"Bar\"] run_on_error = true } # Note that you can use interpolations in subblocks. Here, we configure it so that before apply or plan, print out the # environment variable \"HOME\". before_hook \"interpolation_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", get_env(\"HOME\", \"HelloWorld\")] run_on_error = false } # After running apply or plan, run \"echo Baz\". This hook is configured so that it will always run, even if the apply # or plan failed. after_hook \"after_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Baz\"] run_on_error = true } # After an error occurs during apply or plan, run \"echo Error Hook executed\". This hook is configured so that it will run # after any error, with the \".*\" expression. error_hook \"error_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Error Hook executed\"] on_errors = [ \".*\", ] } # A special after hook to always run after the init-from-module step of the Terragrunt pipeline. In this case, we will # copy the \"foo.tf\" file located by the parent terragrunt.hcl file to the current working directory. after_hook \"init_from_module\" { commands = [\"init-from-module\"] execute = [\"cp\", \"${get_parent_terragrunt_dir()}/foo.tf\", \".\"] } # A special after_hook. Use this hook if you wish to run commands immediately after terragrunt finishes loading its # configurations. If \"read-config\" is defined as a before_hook, it will be ignored as this config would # not be loaded before the action is done. after_hook \"read-config\" { commands = [\"read-config\"] execute = [\"bash\", \"script/get_aws_credentials.sh\"] }} Local File Path Example with allowed hidden files: terraform { # Pull the OpenTofu/Terraform configuration from the local file system. Terragrunt will make a copy of the source folder in the # Terragrunt working directory (typically `.terragrunt-cache`). source = \"../modules/networking/vpc\" # Always include the following file patterns in the Terragrunt copy. include_in_copy = [ \".security_group_rules.json\", \"*.yaml\", ]} A note about using modules from the registrySection titled “A note about using modules from the registry” The key design of Terragrunt is to act as a preprocessor to convert shared service modules in the registry into a root module. In OpenTofu/Terraform, modules can be loosely categorized into two types: Root Module: An OpenTofu/Terraform module that is designed for running tofu init/terraform init and the other workflow commands (apply, plan, etc.). This is the entrypoint module for deploying your infrastructure. Root modules are identified by the presence of key blocks that setup configuration about how OpenTofu/Terraform behaves, like backend blocks (for configuring state) and provider blocks (for configuring how OpenTofu/Terraform interacts with the cloud APIs). Shared Module: A OpenTofu/Terraform module that is designed to be included in other OpenTofu/Terraform modules through module blocks. These modules are missing many of the key blocks that are required for running the workflow commands of OpenTofu/Terraform. Terragrunt further distinguishes shared modules between service modules and modules: Shared Service Module: An OpenTofu/Terraform module that is designed to be standalone and applied directly. These modules are not root modules in that they are still missing the key blocks like backend and provider, but aside from that do not need any additional configuration or composition to deploy. For example, the terraform-aws-modules/vpc module can be deployed by itself without composing with other modules or resources. Shared Module: An OpenTofu/Terraform module that is designed to be composed with other modules. That is, these modules must be embedded in another OpenTofu/Terraform module and combined with other resources or modules. For example, the consul-security-group-rules module Terragrunt started off with features that help directly deploy Root Modules, but over the years have implemented many features that allow you to turn Shared Service Modules into Root Modules by injecting the key configuration blocks that are necessary for OpenTofu/Terraform modules to act as Root Modules. Modules on the Terraform Registry are primarily designed to be used as Shared Modules. That is, you won’t be able to git clone the underlying repository and run tofu init/terraform init or apply directly on the module without modification. Unless otherwise specified, almost all the modules will require composition with other modules/resources to deploy. When using modules in the registry, it helps to think about what blocks and resources are necessary to operate the module, and translating those into Terragrunt blocks that generate them. Note that often, Terragrunt may not be able to deploy modules from the registry. While Terragrunt has features to turn any Shared Module into a Root Module, there are two key technical limitations that prevent Terragrunt from converting ALL shared modules: Every complex input must have a type associated with it. Otherwise, OpenTofu/Terraform will interpret the input that Terragrunt passes through as string. This includes list and map. Derived sensitive outputs must be marked as sensitive. Refer to the terraform tutorial on sensitive variables for more information on this requirement. If you run into issues deploying a module from the registry, chances are that module is not a Shared Service Module, and thus not designed for use with Terragrunt. Depending on the technical limitation, Terragrunt may be able to support the transition to root module. Please always file an issue on the terragrunt repository with the module + error message you are encountering, instead of the module repository. remote_stateSection titled “remote_state” The remote_state block is used to configure how Terragrunt will set up the remote state configuration of your OpenTofu/Terraform code. You can read more about Terragrunt’s remote state functionality in Keep your remote state configuration DRY use case overview. The remote_state block supports the following arguments: backend (attribute): Specifies which remote state backend will be configured. This should be one of the available backends that Opentofu/Terraform supports. disable_init (attribute): When true, skip automatic initialization of the backend by Terragrunt. Some backends have support in Terragrunt to be automatically created if the storage does not exist. Currently, s3 and gcs are the two backends with support for automatic creation. Defaults to false. disable_dependency_optimization (attribute): When true, disable optimized dependency fetching for terragrunt modules using this remote_state block. See the documentation for dependency block for more details. generate (attribute): Configure Terragrunt to automatically generate a .tf file that configures the remote state backend. This is a map that expects two properties: path: The path where the generated file should be written. If a relative path, it’ll be relative to the Terragrunt working dir (where the OpenTofu/Terraform code lives). if_exists (attribute): What to do if a file already exists at path. Valid values are: overwrite (overwrite the existing file) overwrite_terragrunt (overwrite the existing file if it was generated by terragrunt; otherwise, error) skip (skip code generation and leave the existing file as-is) error (exit with an error) config (attribute): An arbitrary map that is used to fill in the backend configuration in OpenTofu/Terraform. All the properties will automatically be included in the OpenTofu/Terraform backend block (with a few exceptions: see below). encryption (attribute): A map that is used to configure state and plan encryption in OpenTofu. The properties will be transformed into an encryption block in the OpenTofu terraform block. The properties are specific to the respective key_provider (see below). For example, if you had the following remote_state block: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} This is equivalent to the following OpenTofu/Terraform code: terraform { backend \"s3\" { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} Note that remote_state can also be set as an attribute. This is useful if you want to set remote_state dynamically. For example, if in common.hcl you had: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} Then in a terragrunt.hcl file, you could dynamically set remote_state as an attribute as follows: locals { # Load the data from common.hcl common = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))} # Set the remote_state config dynamically to the remote_state config in common.hclremote_state = local.common.remote_state backendSection titled “backend” Note that Terragrunt does special processing of the config attribute for the s3 and gcs remote state backends, and supports additional keys that are used to configure the automatic initialization feature of Terragrunt. For the s3 backend, the following additional properties are supported in the config attribute: region - (Optional) The region of the S3 bucket. profile - (Optional) This is the AWS profile name as set in the shared credentials file. endpoint - (Optional) A custom endpoint for the S3 API. endpoints: (Optional) A configuration map for custom service API (starting with Terraform 1.6). s3 - (Optional) A custom endpoint for the S3 API. Overrides endpoint argument. dynamodb - (Optional) A custom endpoint for the DynamoDB API. Overrides dynamodb_endpoint argument. encrypt - (Optional) Whether to enable server-side encryption of the state file. If disabled, a log warning will be issued in the console output to notify the user. If skip_bucket_ssencryption is enabled, the log will be written as a debug log. role_arn - (Optional) The role to be assumed. shared_credentials_file - (Optional) This is the path to the shared credentials file. If this is not set and a profile is specified, ~/.aws/credentials will be used. external_id - (Optional) The external ID to use when assuming the role. session_name - (Optional) The session name to use when assuming the role. dynamodb_table - (Optional) The name of a DynamoDB table to use for state locking and consistency. The table must have a primary key named LockID. If not present, locking will be disabled. use_lockfile - (Optional) When true, enables native S3 locking using S3 object conditional writes for state locking. This feature requires OpenTofu >= 1.10. Can be used simultaneously with dynamodb_table during migration (both locks must be acquired successfully), but typically used as a replacement for DynamoDB locking. skip_bucket_versioning: When true, the S3 bucket that is created to store the state will not be versioned. skip_bucket_ssencryption: When true, the S3 bucket that is created to store the state will not be configured with server-side encryption. skip_bucket_accesslogging: DEPRECATED If provided, will be ignored. A log warning will be issued in the console output to notify the user. skip_bucket_root_access: When true, the S3 bucket that is created will not be configured with bucket policies that allow access to the root AWS user. skip_bucket_enforced_tls: When true, the S3 bucket that is created will not be configured with a bucket policy that enforces access to the bucket via a TLS connection. skip_bucket_public_access_blocking: When true, the S3 bucket that is created will not have public access blocking enabled. disable_bucket_update: When true, disable update S3 bucket if not equal configured in config block enable_lock_table_ssencryption: When true, the synchronization lock table in DynamoDB used for remote state concurrent access will be configured with server-side encryption. s3_bucket_tags: A map of key value pairs to associate as tags on the created S3 bucket. dynamodb_table_tags: A map of key value pairs to associate as tags on the created DynamoDB remote state lock table. accesslogging_bucket_tags: A map of key value pairs to associate as tags on the created S3 bucket to store de access logs. disable_aws_client_checksums: When true, disable computing and checking checksums on the request and response, such as the CRC32 check for DynamoDB. See #1059 for issue where this is a useful workaround. accesslogging_bucket_name: (Optional) When provided as a valid string, create an S3 bucket with this name to store the access logs for the S3 bucket used to store OpenTofu/Terraform state. If not provided, or string is empty or invalid S3 bucket name, then server access logging for the S3 bucket storing the Opentofu/Terraform state will be disabled. Note: When access logging is enabled supported encryption for state bucket is only AES256. Reference: S3 server access logging accesslogging_target_object_partition_date_source: (Optional) When provided as a valid string, it configures the PartitionDateSource option. This option is part of the TargetObjectKeyFormat and PartitionedPrefix AWS configurations, allowing you to configure the log object key format for the access log files. Reference: Logging requests with server access logging. accesslogging_target_prefix: (Optional) When provided as a valid string, set the TargetPrefix for the access log objects in the S3 bucket used to store Opentofu/Terraform state. If set to emptystring, then TargetPrefix will be set to empty string. If attribute is not provided at all, then TargetPrefix will be set to default value TFStateLogs/. This attribute won’t take effect if the accesslogging_bucket_name attribute is not present. skip_accesslogging_bucket_acl: When set to true, the S3 bucket where access logs are stored will not be configured with bucket ACL. skip_accesslogging_bucket_enforced_tls: When set to true, the S3 bucket where access logs are stored will not be configured with a bucket policy that enforces access to the bucket via a TLS connection. skip_accesslogging_bucket_public_access_blocking: When set to true, the S3 bucket where access logs are stored will not have public access blocking enabled. skip_accesslogging_bucket_ssencryption: When set to true, the S3 bucket where access logs are stored will not be configured with server-side encryption. bucket_sse_algorithm: (Optional) The algorithm to use for server-side encryption of the state bucket. Defaults to aws:kms. bucket_sse_kms_key_id: (Optional) The KMS Key to use when the encryption algorithm is aws:kms. Defaults to the AWS Managed aws/s3 key. assume_role: (Optional) A configuration map to use when assuming a role (starting with Terraform 1.6 for Terraform). Override top level arguments role_arn - (Required) The role to be assumed. duration - (Optional) The duration the credentials will be valid. external_id - (Optional) The external ID to use when assuming the role. policy - (Optional) Policy JSON to further restrict the role. policy_arns - (Optional) A list of policy ARNs to further restrict the role. session_name - (Optional) The session name to use when assuming the role. source_identity - (Optional) The source identity to use when assuming the role. tags - (Optional) A map of key value pairs used as assume role session tags. transitive_tag_keys - (Optional) A list of tag keys that to be passed. assume_role_with_web_identity - (Optional) A configuration map to use when assuming a role with a web identity token. role_arn - (Required) The role to be assumed. duration - (Optional) The duration the credentials will be valid. policy - (Optional) Policy JSON to further restrict the role. policy_arns - (Optional) A list of policy ARNs to further restrict the role. session_name - (Optional) The session name to use when assuming the role. web_identity_token - (Required) The web identity token to use when assuming the role. web_identity_token_file - (Optional) The path to the file containing the web identity token to use when assuming the role. For the gcs backend, the following additional properties are supported in the config attribute: skip_bucket_creation: When true, Terragrunt will skip the auto initialization routine for setting up the GCS bucket for use with remote state. skip_bucket_versioning: When true, the GCS bucket that is created to store the state will not be versioned. enable_bucket_policy_only: When true, the GCS bucket that is created to store the state will be configured to use uniform bucket-level access. project: The GCP project where the bucket will be created. location: The GCP location where the bucket will be created. gcs_bucket_labels: A map of key value pairs to associate as labels on the created GCS bucket. credentials: Local path to Google Cloud Platform account credentials in JSON format. access_token: A temporary [OAuth 2.0 access token] obtained from the Google Authorization server. Example with S3: # Configure OpenTofu/Terraform state to be stored in S3, in the bucket \"my-tofu-state\" in us-east-1 under a key that is# relative to included terragrunt config. For example, if you had the following folder structure:## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hcl## And the following is defined in the root terragrunt.hcl config that is included in the child, the state file for the# child module will be stored at the key \"child/tofu.tfstate\".## Note that since we are not using any of the skip args, this will automatically create the S3 bucket# \"my-tofu-state\" and DynamoDB table \"my-lock-table\" if it does not already exist.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { backend \"s3\" {}} Example with GCS: # Configure OpenTofu/Terraform state to be stored in GCS, in the bucket \"my-tofu-state\" in the \"my-tofu\" GCP project in# the eu region under a key that is relative to included terragrunt config. This will also apply the labels# \"owner=terragrunt_test\" and \"name=tofu_state_storage\" to the bucket if it is created by Terragrunt.## For example, if you had the following folder structure:## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hcl## And the following is defined in the root terragrunt.hcl config that is included in the child, the state file for the# child module will be stored at the key \"child/tofu.tfstate\".## Note that since we are not using any of the skip args, this will automatically create the GCS bucket# \"my-tofu-state\" if it does not already exist.remote_state { backend = \"gcs\" config = { project = \"my-tofu\" location = \"eu\" bucket = \"my-tofu-state\" prefix = \"${path_relative_to_include()}/tofu.tfstate\" gcs_bucket_labels = { owner = \"terragrunt_test\" name = \"tofu_state_storage\" } }} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { backend \"gcs\" {}} Example with S3 using native S3 locking (OpenTofu >= 1.10): # Configure OpenTofu/Terraform state to be stored in S3 with native S3 locking instead of DynamoDB.# This uses S3 object conditional writes for state locking, which requires OpenTofu >= 1.10.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} Example with S3 using both DynamoDB and native S3 locking during migration (OpenTofu >= 1.10): # Configure OpenTofu/Terraform state with dual locking during migration from DynamoDB to S3 native locking.# Both locks must be successfully acquired before operations can proceed.# After the migration period, remove dynamodb_table to use only S3 native locking.# Note: This won't delete the DynamoDB table, it will just be unused.# You can delete it manually after the migration period.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" # Remove this after migration period use_lockfile = true # New native S3 locking }} encryptionSection titled “encryption” The encryption map needs a key_provider property, which can be set to one of pbkdf2, aws_kms or gcp_kms. Documentation for each provider type and its possible configuration can be found in the OpenTofu docs. A terragrunt.hcl file configuring PBKDF2 encryption could look like this: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" } encryption = { key_provider = \"pbkdf2\" passphrase = get_env(\"PBKDF2_PASSPHRASE\") }} This would result in the following OpenTofu code: terraform { backend \"s3\" { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" } encryption { key_provider \"pbkdf2\" \"default\" { passphrase = \"SUPERSECRETPASSPHRASE\" } method \"aes_gcm\" \"default\" { keys = key_provider.pbkdf2.default } state { method = method.aes_gcm.default } plan { method = method.aes_gcm.default } }} includeSection titled “include” The include block is used to specify inheritance of Terragrunt configuration files. The included config (also called the parent) will be merged with the current configuration (also called the child) before processing. You can learn more about the inheritance properties of Terragrunt in the Filling in remote state settings with Terragrunt section of the “Keep your remote state configuration DRY” use case overview. You can have more than one include block, but each one must have a unique label. It is recommended to always label your include blocks. Bare includes (include block with no label - e.g., include {}) are currently supported for backward compatibility, but is deprecated usage and support may be removed in the future. include blocks support the following arguments: name (label): You can define multiple include blocks in a single terragrunt config. Each include block must be labeled with a unique name to differentiate it from the other includes. e.g., if you had a block include \"remote\" {}, you can reference the relevant exposed data with the expression include.remote. path (attribute): Specifies the path to a Terragrunt configuration file (the parent config) that should be merged with this configuration (the child config). expose (attribute, optional): Specifies whether or not the included config should be parsed and exposed as a variable. When true, you can reference the data of the included config under the variable include. Defaults to false. Note that the include variable is a map of include labels to the parsed configuration value. merge_strategy (attribute, optional): Specifies how the included config should be merged. Valid values are: no_merge (do not merge the included config), shallow (do a shallow merge - default), deep (do a deep merge of the included config). NOTE: At this time, Terragrunt only supports a single level of include blocks. That is, Terragrunt will error out if an included config also has an include block defined. If you are interested in this feature, please follow #1566 to be notified when nested include blocks are supported. Special case for shallow merge: When performing a shallow merge, all attributes and blocks are merged shallowly with replacement, except for dependencies blocks (NOT dependency block). dependencies blocks are deep merged: that is, all the lists of paths from included configurations are concatenated together, rather than replaced in override fashion. Examples: Single includeSection titled “Single include” # If you have the following folder structure, and the following contents for ./child/terragrunt.hcl, this will include# and merge the configurations in the root.hcl file.## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hclremote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} inputs = { remote_state_config = include.root.remote_state} terraform { backend \"s3\" {}} Multiple includesSection titled “Multiple includes” # If you have the following folder structure, and the following contents for ./child/terragrunt.hcl, this will include# and merge the configurations in the root.hcl, while only loading the data in the region.hcl# configuration.## .# ├── root.hcl# ├── region.hcl# └── child# └── terragrunt.hclremote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} locals { region = \"production\"} include \"remote_state\" { path = find_in_parent_folders(\"root.hcl\") expose = true} include \"region\" { path = find_in_parent_folders(\"region.hcl\") expose = true merge_strategy = \"no_merge\"} inputs = { remote_state_config = include.remote_state.remote_state region = include.region.locals.region} terraform { backend \"s3\" {}} Limitations on accessing exposed configSection titled “Limitations on accessing exposed config” In general, you can access all attributes on include when they are exposed (e.g., include.locals, include.inputs, etc.). However, to support run --all, Terragrunt is unable to expose all attributes when the included config has a dependency block. To understand this, consider the following example: dependency \"vpc\" { config_path = \"${get_terragrunt_dir()}/../vpc\"} inputs = { vpc_name = dependency.vpc.outputs.name} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} dependency \"alb\" { config_path = ( include.root.inputs.vpc_name == \"mgmt\" ? \"../alb-public\" : \"../alb-private\" )} inputs = { alb_id = dependency.alb.outputs.id} In the child terragrunt.hcl, the dependency path for the alb depends on whether the VPC is the mgmt VPC or not, which is determined by the dependency.vpc in the root config. This means that the output from dependency.vpc must be available to parse the dependency.alb config. This causes problems when performing a run --all apply operation. During a run --all operation, Terragrunt first parses all the dependency blocks to build a dependency tree of the Terragrunt modules to figure out the order of operations. If all the paths are static references, then Terragrunt can determine all the dependency paths before any module has been applied. In this case there is no problem even if other config blocks access dependency, as by the time Terragrunt needs to parse those blocks, the upstream dependencies would have been applied during the run --all apply. However, if those dependency blocks depend on upstream dependencies, then there is a problem as Terragrunt would not be able to build the dependency tree without the upstream dependencies being applied. Therefore, to ensure that Terragrunt can build the dependency tree in a run --all operation, Terragrunt enforces the following limitation to exposed include config: If the included configuration has any dependency blocks, only locals and include are exposed and available to the child include and dependency blocks. There are no restrictions for other blocks in the child config (e.g., you can reference inputs from the included config in child inputs). Otherwise, if the included config has no dependency blocks, there is no restriction on which exposed attributes you can access. For example, the following alternative configuration is valid even if the alb dependency is still accessing the inputs attribute from the included config: inputs = { vpc_name = \"mgmt\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} dependency \"vpc\" { config_path = \"../vpc\"} dependency \"alb\" { config_path = ( include.root.inputs.vpc_name == \"mgmt\" ? \"../alb-public\" : \"../alb-private\" )} inputs = { vpc_name = dependency.vpc.outputs.name alb_id = dependency.alb.outputs.id} What is deep merge? When the merge_strategy for the include block is set to deep, Terragrunt will perform a deep merge of the included config. For Terragrunt config, deep merge is defined as follows: For simple types, the child overrides the parent. For lists, the two attribute lists are combined together in concatenation. For maps, the two maps are combined together recursively. That is, if the map keys overlap, then a deep merge is performed on the map value. For blocks, if the label is the same, the two blocks are combined together recursively. Otherwise, the blocks are appended like a list. This is similar to maps, with block labels treated as keys. However, due to internal implementation details, some blocks are not deep mergeable. This will change in the future, but for now, terragrunt performs a shallow merge (that is, block definitions in the child completely override the parent definition). The following blocks have this limitation: - remote_state - generate Similarly, the locals block is deliberately omitted from the merge operation by design. That is, you will not be able to access parent config locals in the child config, and vice versa in a merge. However, you can access the parent locals in child config if you use the expose feature. Finally, dependency blocks have special treatment. When doing a deep merge, dependency blocks from both child and parent config are accessible in both places. For example, consider the following setup: dependency \"vpc\" { config_path = \"../vpc\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id db_id = dependency.mysql.outputs.db_id} include \"root\" { path = find_in_parent_folders(\"root.hcl\") merge_strategy = \"deep\"} dependency \"mysql\" { config_path = \"../mysql\"} inputs = { security_group_id = dependency.vpc.outputs.security_group_id} In the example, note how the parent is accessing the outputs of the mysql dependency even though it is not defined in the parent. Similarly, the child is accessing the outputs of the vpc dependency even though it is not defined in the child. Full example: remote_state { backend = \"s3\" config = { encrypt = true bucket = \"__FILL_IN_BUCKET_NAME__\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-west-2\" }} dependency \"vpc\" { # This will get overridden by child terragrunt.hcl configs config_path = \"\" mock_outputs = { attribute = \"hello\" old_attribute = \"old val\" list_attr = [\"hello\"] map_attr = { foo = \"bar\" } } mock_outputs_allowed_terraform_commands = [\"apply\", \"plan\", \"destroy\", \"output\"]} inputs = { attribute = \"hello\" old_attribute = \"old val\" list_attr = [\"hello\"] map_attr = { foo = \"bar\" test = dependency.vpc.outputs.new_attribute }} include \"root\" { path = find_in_parent_folders(\"root.hcl\") merge_strategy = \"deep\"} remote_state { backend = \"local\"} dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { attribute = \"mock\" new_attribute = \"new val\" list_attr = [\"mock\"] map_attr = { bar = \"baz\" } }} inputs = { attribute = \"mock\" new_attribute = \"new val\" list_attr = [\"mock\"] map_attr = { bar = \"baz\" } dep_out = dependency.vpc.outputs} # Merged terragrunt.hcl # Child override parent completely due to deep merge limitationremote_state { backend = \"local\"} # mock_outputs are merged together with deep mergedependency \"vpc\" { config_path = \"../vpc\" # Child overrides parent mock_outputs = { attribute = \"mock\" # Child overrides parent old_attribute = \"old val\" # From parent new_attribute = \"new val\" # From child list_attr = [ \"hello\", # From parent \"mock\", # From child ] map_attr = { foo = \"bar\" # From parent bar = \"baz\" # From child } } # From parent mock_outputs_allowed_terraform_commands = [\"apply\", \"plan\", \"destroy\", \"output\"]} # inputs are merged together with deep mergeinputs = { attribute = \"mock\" # Child overrides parent old_attribute = \"old val\" # From parent new_attribute = \"new val\" # From child list_attr = [ \"hello\", # From parent \"mock\", # From child ] map_attr = { foo = \"bar\" # From parent bar = \"baz\" # From child test = dependency.vpc.outputs.new_attribute # From parent, referencing dependency mock output from child } dep_out = dependency.vpc.outputs # From child} localsSection titled “locals” The locals block is used to define aliases for Terragrunt expressions that can be referenced elsewhere in configuration. The locals block does not have a defined set of arguments that are supported. Instead, all the arguments passed into locals are available under the reference local.<local name> throughout the file where the locals block is defined. Example: # Make the AWS region a reusable variable within the configurationlocals { aws_region = \"us-east-1\"} inputs = { region = local.aws_region name = \"${local.aws_region}-bucket\"} Complex localsSection titled “Complex locals” Some local variables can be complex types, such as list or map. For example: locals { # Define a list of regions regions = [\"us-east-1\", \"us-west-2\", \"eu-west-1\"] # Define a map of regions to their corresponding bucket names region_to_bucket_name = { us-east-1 = \"east-bucket\" us-west-2 = \"west-bucket\" eu-west-1 = \"eu-bucket\" } # The first region is accessed like this first_region = local.regions[0] # The bucket name for us-east-1 is accessed like this us_east_1_bucket = local.region_to_bucket_name[\"us-east-1\"]} These complex types can also arise when using values derived from reading other files. For example: locals { region = \"us-east-1\"} locals { # Load the data from region.hcl region_hcl = read_terragrunt_config(find_in_parent_folders(\"region.hcl\")) # Access the region from the loaded file region = local.region_hcl.locals.region} inputs = { bucket_name = \"${local.region}-bucket\"} Similarly, you might want to define this shared data using other serialization formats, like JSON or YAML: region: us-east-1 locals { # Load the data from region.json region_yml = yamldecode(file(find_in_parent_folders(\"region.yml\"))) # Access the region from the loaded file region = local.region_yml.region} inputs = { bucket_name = \"${local.region}-bucket\"} Computed localsSection titled “Computed locals” When reading Terragrunt HCL configurations, you might read in a computed configuration: locals { computed_value = run_cmd(\"--terragrunt-quiet\", \"python3\", \"-c\", \"print('Hello,')\")} locals { # Load the data from computed.hcl computed = read_terragrunt_config(find_in_parent_folders(\"computed.hcl\")) # Access the computed value from the loaded file computed_value = \"${local.computed.locals.computed_value} world!\" # <-- This will be \"Hello, world!\"} Note that this can be a powerful feature, but it can easily lead to performance issues if you are not careful, as each read will require a full parse of the HCL file and potentially execute expensive computation. Use this feature judiciously. dependencySection titled “dependency” The dependency block is used to configure module dependencies. Each dependency block exports the outputs of the target module as block attributes you can reference throughout the configuration. You can learn more about dependency blocks in the Dependencies between modules section of the “Execute Opentofu/Terraform commands on multiple modules at once” use case overview. You can define more than one dependency block. Each label you provide to the block identifies another dependency that you can reference in your config. The dependency block supports the following arguments: name (label): You can define multiple dependency blocks in a single terragrunt config. As such, each block needs a name to differentiate between the other blocks, which is what the first label of the block is used for. You can reference the specific dependency output by the name. E.g if you had a block dependency \"vpc\", you can reference the outputs and inputs of this dependency with the expressions dependency.vpc.outputs and dependency.vpc.inputs. config_path (attribute): Path to a Terragrunt module (folder with a terragrunt.hcl file) that should be included as a dependency in this configuration. enabled (attribute): When false, excludes the dependency from execution. Defaults to true. skip_outputs (attribute): When true, skip calling terragrunt output when processing this dependency. If mock_outputs is configured, set outputs to the value of mock_outputs. Otherwise, outputs will be set to an empty map. Put another way, setting skip_outputs means “use mocks all the time if mock_outputs are set.” mock_outputs (attribute): A map of arbitrary key value pairs to use as the outputs attribute when no outputs are available from the target module, or if skip_outputs is true. However, it’s generally recommended not to set skip_outputs if using mock_outputs, because skip_outputs means “use mocks all the time if they are set” whereas mock_outputs means “use mocks only if real outputs are not available.” Use locals instead when skip_outputs = true. mock_outputs_allowed_terraform_commands (attribute): A list of Terraform commands for which mock_outputs are allowed. If a command is used where mock_outputs is not allowed, and no outputs are available in the target module, Terragrunt will throw an error when processing this dependency. mock_outputs_merge_with_state (attribute): DEPRECATED. Use mock_outputs_merge_strategy_with_state. When true, mock_outputs and the state outputs will be merged. That is, the mock_outputs will be treated as defaults and the real state outputs will overwrite them if the keys clash. mock_outputs_merge_strategy_with_state (attribute): Specifies how any existing state should be merged into the mocks. Valid values are no_merge (default) - any existing state will be used as is. If the dependency does not have an existing state (it hasn’t been applied yet), then the mocks will be used shallow - the existing state will be shallow merged into the mocks. Mocks will only be used where the output does not already exist in the dependency’s state deep_map_only - the existing state will be deeply merged into the mocks. If an output is a map, the mock key will be used where that key does not exist in the state. Lists will not be merged Example: # Run `terragrunt output` on the module at the relative path `../vpc` and expose them under the attribute# `dependency.vpc.outputs`dependency \"vpc\" { config_path = \"../vpc\" # Configure mock outputs for the `validate` command that are returned when there are no outputs available (e.g the # module hasn't been applied yet. mock_outputs_allowed_terraform_commands = [\"validate\"] mock_outputs = { vpc_id = \"fake-vpc-id\" }} # Another dependency, available under the attribute `dependency.rds.outputs`dependency \"rds\" { config_path = \"../rds\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id db_url = dependency.rds.outputs.db_url} IMPORTANT: The dependency.<name>.inputs field has been deprecated and removed. You can only access dependency outputs via dependency.<name>.outputs. If you were previously using dependency.<name>.inputs, you should refactor your configuration to use dependency.<name>.outputs instead. Can I speed up dependency fetching? dependency blocks are fetched in parallel at each source level, but will serially parse each recursive dependency. For example, consider the following chain of dependencies: account --> vpc --> securitygroup --> ecs ^ / ecr -- In this chain, the ecr and securitygroup module outputs will be fetched concurrently when applying the ecs module, but the outputs for account and vpc will be fetched serially as terragrunt needs to recursively walk through the tree to retrieve the outputs at each level. This recursive parsing happens due to the necessity to parse the entire terragrunt.hcl configuration (including dependency blocks) in full before being able to call tofu output/terraform output. However, terragrunt includes an optimization to only fetch the lowest level outputs (securitygroup and ecr in this example) provided that the following conditions are met in the immediate dependencies: The remote state is managed using remote_state blocks. The dependency optimization feature flag is enabled (disable_dependency_optimization = false, which is the default). The remote_state block itself does not depend on any dependency outputs (locals and include are ok). You are not relying on before_hook, after_hook, or extra_arguments to the tofu init/terraform init call. NOTE: terragrunt will not automatically detect this and you will need to explicitly opt out of the dependency optimization flag. If these conditions are met, terragrunt will only parse out the remote_state blocks and use that to pull down the state for the target module without parsing the dependency blocks, avoiding the recursive dependency retrieval. dependenciesSection titled “dependencies” The dependencies block is used to enumerate all the Terragrunt modules that need to be applied in order for this module to be able to apply. Note that this is purely for ordering the operations when using run --all commands of OpenTofu/Terraform. This does not expose or pull in the outputs like dependency blocks. The dependencies block supports the following arguments: paths (attribute): A list of paths to modules that should be marked as a dependency. Example: # When applying this terragrunt config in an `run --all` command, make sure the modules at \"../vpc\" and \"../rds\" are# handled first.dependencies { paths = [\"../vpc\", \"../rds\"]} generateSection titled “generate” The generate block can be used to arbitrarily generate a file in the terragrunt working directory (where tofu/terraform is called). This can be used to generate common OpenTofu/Terraform configurations that are shared across multiple OpenTofu/Terraform modules. For example, you can use generate to generate the provider blocks in a consistent fashion by defining a generate block in the parent terragrunt config. The generate block supports the following arguments: name (label): You can define multiple generate blocks in a single terragrunt config. As such, each block needs a name to differentiate between the other blocks. path (attribute): The path where the generated file should be written. If a relative path, it’ll be relative to the Terragrunt working dir (where the OpenTofu/Terraform code lives). if_exists (attribute): What to do if a file already exists at path. Valid values are: overwrite (overwrite the existing file) overwrite_terragrunt (overwrite the existing file if it was generated by terragrunt; otherwise, error) skip (skip code generation and leave the existing file as-is) error (exit with an error) if_disabled (attribute): What to do if a file already exists at path and disable is set to true (skip by default) Valid values are: remove (remove the existing file) remove_terragrunt (remove the existing file if it was generated by terragrunt; otherwise, error) skip (skip removing and leave the existing file as-is). comment_prefix (attribute): A prefix that can be used to indicate comments in the generated file. This is used by terragrunt to write out a signature for knowing which files were generated by terragrunt. Defaults to #. Optional. disable_signature (attribute): When true, disables including a signature in the generated file. This means that there will be no difference between overwrite_terragrunt and overwrite for the if_exists setting. Defaults to false. Optional. contents (attribute): The contents of the generated file. disable (attribute): Disables this generate block. Example: # When using this terragrunt config, terragrunt will generate the file \"provider.tf\" with the aws provider block before# calling to OpenTofu/Terraform. Note that this will overwrite the `provider.tf` file if it already exists.generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\" version = \"= 2.3.1\" allowed_account_ids = [\"1234567890\"]}EOF} Note that generate can also be set as an attribute. This is useful if you want to set generate dynamically. For example, if in common.hcl you had: generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\" version = \"= 2.3.1\" allowed_account_ids = [\"1234567890\"]}EOF} Then in a terragrunt.hcl file, you could dynamically set generate as an attribute as follows: locals { # Load the data from common.hcl common = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))} # Set the generate config dynamically to the generate config in common.hclgenerate = local.common.generate engineSection titled “engine” The engine block is used to configure experimental Terragrunt engine configuration. More details in engine section. featureSection titled “feature” The feature block is used to configure feature flags in HCL for a specific Terragrunt Unit. Each feature flag must include a default value. Feature flags can be overridden via the --feature CLI option. feature \"string_flag\" { default = \"test\"} feature \"run_hook\" { default = false} terraform { before_hook \"feature_flag\" { commands = [\"apply\", \"plan\", \"destroy\"] execute = feature.run_hook.value ? [\"sh\", \"-c\", \"feature_flag_script.sh\"] : [ \"sh\", \"-c\", \"exit\", \"0\" ] }} inputs = { string_feature_flag = feature.string_flag.value} Setting feature flags through CLI: terragrunt --feature run_hook=true apply terragrunt --feature run_hook=true --feature string_flag=dev apply Setting feature flags through env variables: export TG_FEATURE=run_hook=trueterragrunt apply export TG_FEATURE=run_hook=true,string_flag=devterragrunt apply Note that the default value of the feature block is evaluated as an expression dynamically. What this means is that the value of the flag can be set via a Terragrunt expression at runtime. This is useful for scenarios where you want to integrate with external feature flag services like LaunchDarkly, AppConfig, etc. feature \"feature_name\" { default = run_cmd(\"--terragrunt-quiet\", \"<command-to-fetch-feature-flag-value>\")} Feature flags are used to conditionally control Terragrunt behavior at runtime, including the inclusion or exclusion of units. More on that in the exclude block. excludeSection titled “exclude” The exclude block in Terragrunt provides advanced configuration options to dynamically determine when and how specific units in the Terragrunt dependency graph are excluded. This feature allows for fine-grained control over which actions are executed and can conditionally exclude dependencies. Syntax: exclude { if = <boolean> # Boolean to determine exclusion. no_run = <boolean> # Boolean to prevent the unit from running (even when not using `--all`). actions = [\"<action>\", ...] # List of actions to exclude (e.g., \"plan\", \"apply\", \"all\", \"all_except_output\"). exclude_dependencies = <boolean> # Boolean to determine if dependencies should also be excluded.} Attributes: AttributeTypeDescriptionifbooleanCondition to dynamically determine whether the unit should be excluded.actionslist(string)Specifies which actions to exclude when the condition is met. Options: plan, apply, all, all_except_output etc.exclude_dependenciesbooleanIndicates whether the dependencies of the excluded unit should also be excluded (default: false).no_runbooleanWhen true and if is true, prevents the unit from running entirely for both single unit commands and run --all commands, but only when the current action matches the actions list. Examples: exclude { if = feature.feature_name.value # Dynamically exclude based on a feature flag. actions = [\"plan\", \"apply\"] # Exclude `plan` and `apply` actions. exclude_dependencies = false # Do not exclude dependencies.} In this example, the unit is excluded for the plan and apply actions only when feature.feature_name.value evaluates to true. Dependencies are not excluded. exclude { if = feature.is_dev_environment.value # Exclude only for development environments. actions = [\"all\"] # Exclude all actions. exclude_dependencies = true # Exclude dependencies along with the unit.} This configuration ensures the unit and its dependencies are excluded from all actions in the Terragrunt graph when the feature is_dev_environment evaluates to true. exclude { if = true # Explicitly exclude. actions = [\"all_except_output\"] # Allow `output` actions nonetheless. exclude_dependencies = false # Dependencies remain active.} This setup is useful for scenarios where output evaluation is still needed, even if other actions like plan or apply are excluded. exclude { if = true no_run = true actions = [\"plan\"]} This configuration prevents the unit from running when if is true AND the current action is “plan”. The no_run attribute works for both single unit commands (like terragrunt plan) and run --all commands (like terragrunt run --all plan), but only when the current action matches the actions list. Consider using this for units that are expensive to continuously update, and can be opted in when necessary. errorsSection titled “errors” The errors block contains all the configurations for handling errors. It supports different nested configuration blocks like retry and ignore to define specific error-handling strategies. Retry ConfigurationSection titled “Retry Configuration” The retry block within the errors block defines rules for retrying operations when specific errors occur. This is useful for handling intermittent errors that may resolve after a short delay or multiple attempts. Example: Retry Configuration errors { retry \"retry_example\" { retryable_errors = [\".*Error: transient.*\"] # Matches errors containing 'Error: transient' max_attempts = 5 # Retry up to 5 times sleep_interval_sec = 10 # Wait 10 seconds between retries }} Parameters: retryable_errors: A list of regex patterns to match errors that are eligible to be retried. e.g. \".*Error: transient.*\" matches errors containing Error: transient. max_attempts: The maximum number of retry attempts. e.g. 5 retries. sleep_interval_sec: Time (in seconds) to wait between retries. e.g. 10 seconds. Ignore ConfigurationSection titled “Ignore Configuration” The ignore block within the errors block defines rules for ignoring specific errors. This is useful when certain errors are known to be safe and should not prevent the run from proceeding. Example: Ignore Configuration errors { ignore \"ignore_example\" { ignorable_errors = [ \".*Error: safe-to-ignore.*\", # Ignore errors containing 'Error: safe-to-ignore' \"!.*Error: critical.*\" # Do not ignore errors containing 'Error: critical' ] message = \"Ignoring safe-to-ignore errors\" # Optional message displayed when ignoring errors signals = { safe_to_revert = true # Indicates the operation is safe to revert on failure } }} Parameters: ignorable_errors: A list of regex patterns to define errors to ignore. \"Error: safe-to-ignore.*\": Ignores errors containing Error: safe-to-ignore. \"!Error: critical.*\": Ensures errors containing Error: critical are not ignored. message (Optional): A warning message displayed when an error is ignored. Example: \"Ignoring safe-to-ignore errors\". signals (Optional): Key-value pairs used to emit signals to external systems. Example: safe_to_revert = true indicates it is safe to revert the operation if it fails. Populating values into the signals attribute results in a JSON file named error-signals.json being emitted on failure. This file can be inspected in CI/CD systems to determine the recommended course of action to address the failure. Example: If an error occurs and the author of the unit has signaled safe_to_revert = true, the CI/CD system could follow a standard process: Identify all units with files named error-signals.json. Checkout the previous commit for those units. Apply the units in their previous state, effectively reverting their updates. This approach ensures consistent and automated error handling in complex pipelines. Combined ExampleSection titled “Combined Example” Below is a combined example showcasing both retry and ignore configurations within the errors block. errors { # Retry block for transient errors retry \"transient_errors\" { retryable_errors = [\".*Error: transient network issue.*\"] max_attempts = 3 sleep_interval_sec = 5 } # Ignore block for known safe-to-ignore errors ignore \"known_safe_errors\" { ignorable_errors = [ \".*Error: safe warning.*\", \"!.*Error: do not ignore.*\" ] message = \"Ignoring safe warning errors\" signals = { alert_team = false } }} Take note that: All retry and ignore configurations must be defined within a single errors block. The retry block is prioritized over legacy retry fields (retryable_errors, retry_max_attempts, retry_sleep_interval_sec). Conditional logic can be used within ignorable_errors to enable or disable rules dynamically. Evaluation Order: Ignore Rules: Errors are checked against the ignore rules first. If an error matches, it is ignored and will not trigger a retry. Retry Rules: Once ignore rules are applied, the retry rules handle any remaining errors. Note: Only the first matching rule is applied. If there are multiple conflicting rules, any matches after the first one are ignored. Errors during source fetchingSection titled “Errors during source fetching” In addition to handling errors during OpenTofu/Terraform runs, the errors block will also handle errors that occur during source fetching. This can be particularly useful when fetching from artifact repositories that may be temporarily unavailable. Example: terraform { source = \"https://unreliable-source.com/module.zip\"} errors { retry \"source_fetch\" { retryable_errors = [\".*Error: transient network issue.*\"] max_attempts = 3 sleep_interval_sec = 5 }} unitSection titled “unit” The unit block is used to define a deployment unit within a Terragrunt stack file (terragrunt.stack.hcl). Each unit represents a distinct infrastructure component that should be deployed as part of the stack. Purpose: Define a single, deployable piece of infrastructure. Use case: When you want to create a single piece of isolated infrastructure (e.g. a specific VPC, database, or application). Result: Generates a single terragrunt.hcl file in the specified path. The unit block supports the following arguments: name (label): A unique identifier for the unit. This is used to reference the unit elsewhere in your configuration. source (attribute): Specifies where to find the Terragrunt configuration files for this unit. This follows the same syntax as the source parameter in the terraform block. path (attribute): The relative path where this unit should be deployed within the stack directory (.terragrunt-stack). Also take note of the no_dot_terragrunt_stack attribute below, which can impact this. values (attribute, optional): A map of values that will be passed to the unit as inputs. no_dot_terragrunt_stack (attribute, optional): A boolean flag (true or false). When set to true, the unit will not be placed inside the .terragrunt-stack directory but will instead be generated in the same directory where terragrunt.stack.hcl is located. This allows for a soft adoption of stacks, making it easier for users to start using terragrunt.stack.hcl without modifying existing directory structures, or performing state migrations. no_validation (attribute, optional): A boolean flag (true or false) that controls whether Terragrunt should validate the unit’s configuration. When set to true, Terragrunt will skip validation checks for this unit. Example: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-units.git//networking/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} Note that each unit must have a unique name and path within the stack. When values are specified, generated units will have access to those values via a special terragrunt.values.hcl file generated next to the terragrunt.hcl file of the unit. terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.values.hclterragrunt.hcl The terragrunt.values.hcl file will contain the values specified in the values block as top-level attributes: vpc_name = \"main\"cidr = \"10.0.0.0/16\" The unit will be able to leverage those values via values variables. inputs = { vpc_name = values.vpc_name cidr = values.cidr} Example usage of no_dot_terragrunt_stack attribute: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-units.git//networking/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} unit \"rds\" { source = \"git::git@github.com:acme/infrastructure-units.git//database/rds?ref=v0.0.1\" path = \"rds\" values = { engine = \"postgres\" version = \"13\" } no_dot_terragrunt_stack = true} With the above configuration, the resulting directory structure will be: terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.values.hclterragrunt.hclDirectoryrds terragrunt.values.hclterragrunt.hcl The vpc unit is placed inside .terragrunt-stack, as expected. The rds unit is generated in the same directory as terragrunt.stack.hcl, rather than inside .terragrunt-stack, due to no_dot_terragrunt_stack = true. Notes: The source value can be updated dynamically using the --source-map flag, just like terraform.source. A pre-created terragrunt.values.hcl file can be provided in the unit source (sibling to the terragrunt.hcl file used as the source of the unit). If present, this file will be used as the default values for the unit. However, if the values attribute is defined in the unit block, the generated terragrunt.values.hcl will replace the pre-existing file. Comparison: unit vs stack blocksSection titled “Comparison: unit vs stack blocks” Aspectunit blockstack blockPurposeDefine a single infrastructure componentDefine a reusable collection of componentsWhen to useFor specific, one-off infrastructure piecesFor patterns of infrastructure pieces that you want provisioned togetherGenerated outputA directory with a single terragrunt.hcl fileA directory with a terragrunt.stack.hcl file stackSection titled “stack” The stack block is used to define a stack of deployment units in a Terragrunt configuration file (terragrunt.stack.hcl). Stacks allow for nesting, enabling the organization of infrastructure components into modular, reusable groups, reducing redundancy and improving maintainability. Purpose: Define a collection of related units that can be reused. Use case: When you have a common, multi-unit pattern (like “dev environment” or “three-tier web application”) that you want to deploy multiple times. Result: Generates another terragrunt.stack.hcl file that can contain more units or stacks. Stacks are designed to be nestable, helping to mitigate the risk of stacks becoming too large or too repetitive. When a stack is generated, it can include nested stacks, ensuring that the configuration scales efficiently. The stack block supports the following arguments: name (label): A unique identifier for the stack. This is used to reference the stack elsewhere in your configuration. source (attribute): Specifies where to find the Terragrunt configuration files for this stack. This follows the same syntax as the source parameter in the terraform block. path (attribute): The relative path within .terragrunt-stack where this stack should be generated.If an absolute path is provided here, Terragrunt will generate the stack in that location, instead of generating it in a path relative to the .terragrunt-stack directory. Also take note of the no_dot_terragrunt_stack attribute below, which can impact this. values (attribute, optional): A map of custom values that can be passed to the stack. These values can be referenced within the stack’s configuration files, allowing for customization without modifying the stack source. no_dot_terragrunt_stack (attribute, optional): A boolean flag (true or false). When set to true, the stack will not be placed inside the .terragrunt-stack directory but will instead be generated in the same directory where terragrunt.stack.hcl is located. This allows for a soft adoption of stacks, making it easier for users to start using terragrunt.stack.hcl without modifying existing directory structures, or performing state migrations. no_validation (attribute, optional): A boolean flag (true or false) that controls whether Terragrunt should validate the stack’s configuration. When set to true, Terragrunt will skip validation checks for this stack. Example: stack \"services\" { source = \"github.com/gruntwork-io/terragrunt-stacks//stacks/mock/services?ref=v0.0.1\" path = \"services\" values = { project = \"dev-services\" cidr = \"10.0.0.0/16\" }} # ...unit \"vpc\" { # ... values = { cidr = values.cidr }} In this example, the services stack is defined with path services, which will be generated at .terragrunt-stack/services. The stack is also provided with custom values for project and cidr, which can be used within the stack’s configuration files. Terragrunt will recursively generate a stack using the contents of the .terragrunt-stack/services/terragrunt.stack.hcl file until the entire stack is fully generated. Notes: The source value can be updated dynamically using the --source-map flag, just like terraform.source. A pre-created terragrunt.values.hcl file can be provided in the stack source (sibling to the terragrunt.stack.hcl file used as the source of the stack). If present, this file will be used as the default values for the stack. However, if the values attribute is defined in the stack block, the generated terragrunt.values.hcl will replace the pre-existing file. Edit page Previous Overview Next Attributes",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.627Z"
  },
  {
    "title": "Attributes",
    "url": "https://terragrunt.gruntwork.io/docs/reference/hcl/attributes/",
    "content": "Attributes Terragrunt HCL configuration uses attributes when there are values that need to be defined for Terragrunt as a whole. Think of attributes as the values used for Terragrunt configuration, such as the inputs to pass an orchestrated OpenTofu/Terraform binary, or the download directory to use. inputsSection titled “inputs” The inputs attribute is a map that is used to specify the input variables and their values to pass in to OpenTofu/Terraform. Each entry of the map is passed to OpenTofu/Terraform using the environment variable mechanism. This means that each input will be set using the form TF_VAR_variablename, with the value in json encoded format. Note that because the values are being passed in with environment variables and json, the type information is lost when crossing the boundary between Terragrunt and OpenTofu/Terraform. You must specify the proper type constraint on the variable in OpenTofu/Terraform in order for OpenTofu/Terraform to process the inputs as the right type. Example: inputs = { string = \"string\" number = 42 bool = true list_string = [\"a\", \"b\", \"c\"] list_number = [1, 2, 3] list_bool = [true, false] map_string = { foo = \"bar\" } map_number = { foo = 42 bar = 12345 } map_bool = { foo = true bar = false baz = true } object = { str = \"string\" num = 42 list = [1, 2, 3] map = { foo = \"bar\" } } from_env = get_env(\"FROM_ENV\", \"default\")} Using this attribute is roughly equivalent to setting the corresponding TF_VAR_ attribute. For example, setting this in your terragrunt.hcl: inputs = { instance_type = \"t2.micro\" instance_count = 10 tags = { Name = \"example-app\" }} And running: terragrunt apply This is roughly equivalent to running: TF_VAR_instance_type=\"t2.micro\" \\TF_VAR_instance_count=10 \\TF_VAR_tags='{\"Name\":\"example-app\"}' \\tofu apply # or terraform apply Variable PrecedenceSection titled “Variable Precedence” Variables loaded in OpenTofu/Terraform will consequently use the following precedence order (with the highest precedence being lowest on the list): inputs set in terragrunt.hcl files. Explicitly set TF_VAR_ environment variables (these will override the inputs set in terragrunt.hcl if they conflict). terraform.tfvars files if present. terraform.tfvars.json files if present. Any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames. Any -var and -var-file options on the command line, in the order they are provided. download_dirSection titled “download_dir” The terragrunt download_dir string option can be used to override the default download directory. The precedence is as follows: --download-dir command line option → TG_DOWNLOAD_DIR env variable → download_dir attribute of the terragrunt.hcl file in the module directory → download_dir attribute of the included terragrunt.hcl. It supports all terragrunt functions, i.e. path_relative_from_include(). prevent_destroySection titled “prevent_destroy” Terragrunt prevent_destroy boolean flag allows you to protect selected OpenTofu/Terraform module. It will prevent destroy or run --all destroy command from actually destroying resources of the protected module. This is useful for modules you want to carefully protect, such as a database, or a module that provides auth. Example: terraform { source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"} prevent_destroy = true skipSection titled “skip” DEPRECATED: Use exclude instead. The terragrunt skip boolean flag can be used to protect units you don’t want any changes to, or just to skip units that don’t define any infrastructure by themselves. When set to true, all terragrunt commands will skip the selected module. Consider the following file structure: Directoryroot terragrunt.hclDirectoryprod terragrunt.hclDirectorydev terragrunt.hclDirectoryqa terragrunt.hcl In some cases, the root level terragrunt.hcl file is solely used to DRY up your OpenTofu/Terraform configuration by being included in the other terragrunt.hcl files. In this case, you do not want the run --all commands to process the root level terragrunt.hcl since it does not define any infrastructure by itself. To make the run --all commands skip the root level terragrunt.hcl file, you can set skip = true: skip = true The skip flag can be inherited from an included terragrunt.hcl file if skip is defined there, unless it is explicitly redefined in the current’s module terragrunt.hcl file. iam_roleSection titled “iam_role” The iam_role attribute can be used to specify an IAM role that Terragrunt should assume before invoking OpenTofu/Terraform. The precedence is as follows: --iam-assume-role command line option → TG_IAM_ASSUME_ROLE env variable → iam_role attribute of the terragrunt.hcl file in the module directory → iam_role attribute of the included terragrunt.hcl. Example: iam_role = \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\" Notes: Value of iam_role can reference local variables Definitions of iam_role included from other HCL files through include iam_assume_role_durationSection titled “iam_assume_role_duration” The iam_assume_role_duration attribute can be used to specify the STS session duration, in seconds, for the IAM role that Terragrunt should assume before invoking OpenTofu/Terraform. The precedence is as follows: --iam-assume-role-duration command line option → TG_IAM_ASSUME_ROLE_DURATION env variable → iam_assume_role_duration attribute of the terragrunt.hcl file in the module directory → iam_assume_role_duration attribute of the included terragrunt.hcl. Example: iam_assume_role_duration = 14400 iam_assume_role_session_nameSection titled “iam_assume_role_session_name” The iam_assume_role_session_name attribute can be used to specify the STS session name, for the IAM role that Terragrunt should assume before running OpenTofu/Terraform. The precedence is as follows: --iam-assume-role-session-name command line option → TG_IAM_ASSUME_ROLE_SESSION_NAME env variable → iam_assume_role_session_name attribute of the terragrunt.hcl file in the module directory → iam_assume_role_session_name attribute of the included terragrunt.hcl. iam_web_identity_tokenSection titled “iam_web_identity_token” The iam_web_identity_token attribute can be used along with iam_role to assume a role using AssumeRoleWithWebIdentity. iam_web_identity_token can be set to either the token value (typically using get_env()), or the path to a file on disk. The precedence is as follows: --iam-assume-role-web-identity-token command line option → TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN env variable → iam_web_identity_token attribute of the terragrunt.hcl file in the module directory → iam_web_identity_token attribute of the included terragrunt.hcl. The primary benefit of using AssumeRoleWithWebIdentity over regular AssumeRole is that it enables you to run terragrunt in your CI/CD pipelines without static AWS credentials. Git Provider ConfigurationSection titled “Git Provider Configuration” To use AssumeRoleWithWebIdentity in your CI/CD environment, you must first configure an AWS OpenID Connect provider to trust the OIDC service provided by your git provider. Follow the instructions below for whichever Git provider you use: GitLab: Configure OpenID Connect in AWS to retrieve temporary credentials GitHub: Configuring OpenID Connect in Amazon Web Services CircleCI: Using OpenID Connect tokens in jobs Once you have configured your OpenID Connect Provider and configured the trust policy of your IAM role according to the above instructions, you can configure Terragrunt to use the Web Identity Token in the following manner. If your Git provider provides the OIDC token as an environment variable, pass it in to the iam_web_identity_token as follows iam_role = \"arn:aws:iam::<AWS account number>:role/<IAM role name>\" iam_web_identity_token = get_env(\"<variable name>\") If your Git provider provides the OIDC token as a file, simply pass the file path to iam_web_identity_token iam_role = \"arn:aws:iam::<AWS account number>:role/<IAM role name>\" iam_web_identity_token = \"/path/to/token/file\" terraform_binarySection titled “terraform_binary” The terragrunt terraform_binary string option can be used to override the default binary Terragrunt calls (which is tofu). The precedence is as follows: --tf-path command line option → TG_TF_PATH env variable → terragrunt.hcl in the module directory → included terragrunt.hcl terraform_version_constraintSection titled “terraform_version_constraint” The terragrunt terraform_version_constraint string overrides the default minimum supported version of OpenTofu/Terraform. Terragrunt usually only officially supports the latest version of OpenTofu/Terraform, however, in some cases an older version of OpenTofu/Terraform is needed. Example: terraform_version_constraint = \">= 0.11\" terragrunt_version_constraintSection titled “terragrunt_version_constraint” The terragrunt terragrunt_version_constraint string can be used to specify which versions of the Terragrunt CLI can be used with your configuration. If the running version of Terragrunt doesn’t match the constraints specified, Terragrunt will produce an error and exit without taking any further actions. Example: terragrunt_version_constraint = \">= 0.23\" retryable_errorsSection titled “retryable_errors” DEPRECATED: Use errors instead. The terragrunt retryable_errors list can be used to override the default list of retryable errors with your own custom list. Default List: retryable_errors = [ \"(?s).*Failed to load state.*tcp.*timeout.*\", \"(?s).*Failed to load backend.*TLS handshake timeout.*\", \"(?s).*Creating metric alarm failed.*request to update this alarm is in progress.*\", \"(?s).*Error installing provider.*TLS handshake timeout.*\", \"(?s).*Error configuring the backend.*TLS handshake timeout.*\", \"(?s).*Error installing provider.*tcp.*timeout.*\", \"(?s).*Error installing provider.*tcp.*connection reset by peer.*\", \"NoSuchBucket: The specified bucket does not exist\", \"(?s).*Error creating SSM parameter: TooManyUpdates:.*\", \"(?s).*app.terraform.io.*: 429 Too Many Requests.*\", \"(?s).*ssh_exchange_identification.*Connection closed by remote host.*\", \"(?s).*Client\\\\.Timeout exceeded while awaiting headers.*\", \"(?s).*Could not download module.*The requested URL returned error: 429.*\",] Example: retryable_errors = [ \"(?s).*Error installing provider.*tcp.*connection reset by peer.*\", \"(?s).*ssh_exchange_identification.*Connection closed by remote host.*\"] Edit page Previous Blocks Next Functions",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.668Z"
  },
  {
    "title": "Functions",
    "url": "https://terragrunt.gruntwork.io/docs/reference/hcl/functions/",
    "content": "Functions Terragrunt allows you to use built-in functions anywhere in terragrunt.hcl, just like OpenTofu/Terraform! OpenTofu/Terraform built-in functionsSection titled “OpenTofu/Terraform built-in functions” All OpenTofu/Terraform built-in functions (as of v0.15.3) are supported in Terragrunt config files: terraform { source = \"../modules/${basename(get_terragrunt_dir())}\"} remote_state { backend = \"s3\" config = { bucket = trimspace(\" my-tofu-bucket \") region = join(\"-\", [\"us\", \"east\", \"1\"]) key = format(\"%s/tofu.tfstate\", path_relative_to_include()) }} Note: Any file* functions (file, fileexists, filebase64, etc.) are relative to the directory containing the terragrunt.hcl file they’re used in. Given the following structure: Directoryterragrunt common.tfvarsDirectoryassets Directorymysql assets.txtterragrunt.hcl Then assets.txt could be read with the following function call: file(\"assets/mysql/assets.txt\") Note: Terragrunt was originally able to take advantage of built-in OpenTofu/Terraform built-in functions automatically, as they were exposed via an exported package. Since v0.15.3, however, these functions are now internal to the respective codebases. As a result, Terragrunt users typically use different functions to resolve the same problems. e.g. Terragrunt users can execute arbitrary shell commands with run_cmd in whatever language they like instead of using a bespoke HCL function to solve a given problem. In the future, OpenTofu may expose these functions via a public package, which would allow Terragrunt to access them directly. Until such a time, Terragrunt will continue to provide its own set of functions to solve problems relevant to Terragrunt users. If there is a specific function you would like to see supported directly in Terragrunt, please open an issue requesting it. Make sure to include the use case you have in mind so we can understand the problem you are trying to solve, and why existing Terragrunt functions are not sufficient. find_in_parent_foldersSection titled “find_in_parent_folders” find_in_parent_folders searches up the directory tree from the current terragrunt.hcl file, and returns the absolute path to the first file in a parent folder with a given name, or exits with an error if no such file is found. This is primarily useful in an include block to automatically find the path to a parent Terragrunt configuration: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} The function can also be used to find parent folders. include \"root\" { path = find_in_parent_folders(\"some\")} You can also pass an optional second fallback parameter, which causes the function to return the fallback value (instead of exiting with an error) if the file in the name parameter cannot be found: include \"root\" { path = find_in_parent_folders(\"some-other-file-name.hcl\", \"fallback.hcl\")} Note that this function searches relative to the terragrunt.hcl file when called from a parent config. For example, if you had the following folder structure: root.hclDirectoryprod env.hclDirectorymysql terragrunt.hcl And the root root.hcl contained the following: locals { env_vars = read_terragrunt_config(find_in_parent_folders(\"env.hcl\"))} The find_in_parent_folders will search from the child terragrunt.hcl (prod/mysql/terragrunt.hcl) config, finding the env.hcl file in the prod directory. NOTE: This function has undocumented behavior that has since been deprecated. To learn more about this, see the Migrating from root terragrunt.hcl guide. path_relative_to_includeSection titled “path_relative_to_include” path_relative_to_include() returns the relative path between the current terragrunt.hcl file and the path specified in its include block. For example, consider the following folder structure: root.hclDirectoryprod Directorymysql terragrunt.hclDirectorystage Directorymysql terragrunt.hcl Imagine prod/mysql/terragrunt.hcl and stage/mysql/terragrunt.hcl include all settings from the root root.hcl file: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} The root root.hcl can use the path_relative_to_include() in its remote_state configuration to ensure each child stores its remote state at a different key: remote_state { backend = \"s3\" config = { bucket = \"my-tofu-bucket\" region = \"us-east-1\" key = \"${path_relative_to_include()}/tofu.tfstate\" }} The resulting key will be prod/mysql/tofu.tfstate for the prod mysql module and stage/mysql/tofu.tfstate for the stage mysql module. If you have include blocks, this function requires a name parameter when used in the child config to specify which include block to base the relative path on. Example: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"region\" { path = find_in_parent_folders(\"region.hcl\")} terraform { source = \"../modules/${path_relative_to_include(\"root\")}\"} path_relative_from_includeSection titled “path_relative_from_include” path_relative_from_include() returns the relative path between the path specified in its include block and the current terragrunt.hcl file (it is the counterpart of path_relative_to_include()). For example, consider the following folder structure: Directorysources Directorymysql *.tfDirectorysecrets Directorymysql *.tfDirectoryterragrunt root.hclcommon.tfvarsDirectorymysql terragrunt.hclDirectorysecrets Directorymysql terragrunt.hcl Imagine terragrunt/mysql/terragrunt.hcl and terragrunt/secrets/mysql/terragrunt.hcl include all settings from the root root.hcl file: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} The root root.hcl can use the path_relative_from_include() in combination with path_relative_to_include() in its source configuration to retrieve the relative OpenTofu/Terraform source code from the terragrunt configuration file: terraform { source = \"${path_relative_from_include()}/../sources//${path_relative_to_include()}\"} The resulting source will be ../../sources//mysql for mysql module and ../../../sources//secrets/mysql for secrets/mysql module. Another use case would be to add an extra argument to include the common.tfvars file for all subdirectories: terraform { extra_arguments \"common_var\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] arguments = [ \"-var-file=${get_terragrunt_dir()}/${path_relative_from_include()}/common.tfvars\", ] }} This allows proper retrieval of the common.tfvars from whatever the level of subdirectories we have. If you have include blocks, this function requires a name parameter when used in the child config to specify which include block to base the relative path on. Example: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"region\" { path = find_in_parent_folders(\"region.hcl\")} terraform { source = \"../modules/${path_relative_from_include(\"root\")}\"} get_envSection titled “get_env” get_env(NAME) return the value of variable named NAME or throws exceptions if that variable is not set. Example: remote_state { backend = \"s3\" config = { bucket = get_env(\"BUCKET\") }} get_env(NAME, DEFAULT) returns the value of the environment variable named NAME or DEFAULT if that environment variable is not set. Example: remote_state { backend = \"s3\" config = { bucket = get_env(\"BUCKET\", \"my-tofu-bucket\") }} Note that OpenTofu/Terraform will read environment variables that start with the prefix TF_VAR_, so one way to share a variable named foo between OpenTofu/Terraform and Terragrunt is to set its value as the environment variable TF_VAR_foo and to read that value in using this get_env() built-in function. get_platformSection titled “get_platform” get_platform() returns the current Operating System. Example: inputs = { platform = get_platform()} This function can also be used in a comparison to evaluate what to do based on the current operating system. Example: output \"platform\" { value = var.platform == \"darwin\" ? \"(value for MacOS)\" : \"(value for other OS's)\"} Some of the returned values can be: darwin freebsd linux windows get_repo_rootSection titled “get_repo_root” get_repo_root() returns the absolute path to the root of the Git repository: inputs { very_important_config = \"${get_repo_root()}/config/strawberries.conf\"} This function will error if the file is not located in a Git repository. get_path_from_repo_rootSection titled “get_path_from_repo_root” get_path_from_repo_root() returns the path from the root of the Git repository to the current directory: remote_state { backend = \"s3\" config = { bucket = \"tofu\" dynamodb_table = \"tofu\" encrypt = true key = \"${get_path_from_repo_root()}/tofu.tfstate\" session_name = \"tofu\" region = \"us-east-1\" }} This function will error if the file is not located in a Git repository. get_path_to_repo_rootSection titled “get_path_to_repo_root” get_path_to_repo_root() returns the relative path to the root of the Git repository: terraform { source = \"${get_path_to_repo_root()}//modules/example\"} This function will error if the file is not located in a Git repository. get_terragrunt_dirSection titled “get_terragrunt_dir” get_terragrunt_dir() returns the directory where the Terragrunt configuration file (by default terragrunt.hcl) lives. This is useful when you need to use relative paths with remote OpenTofu/Terraform configurations and you want those paths relative to your Terragrunt configuration file and not relative to the temporary directory where Terragrunt downloads the code. For example, imagine you have the following file structure: common.tfvarsDirectoryfrontend-app terragrunt.hcl Inside tofu-code/frontend-app/terragrunt.hcl you might try to write code that looks like this: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] arguments = [ \"-var-file=../common.tfvars\" # Note: This relative path will NOT work correctly! ] }} Note how the source parameter is set, so Terragrunt will download the frontend-app code from the modules repo into a temporary folder and run tofu/terraform in that temporary folder. Note also that there is an extra_arguments block that is trying to allow the frontend-app to read some shared variables from a common.tfvars file. Unfortunately, the relative path (../common.tfvars) won’t work, as it will be relative to the temporary folder! Moreover, you can’t use an absolute path, or the code won’t work on any of your teammates’ computers. To make the relative path work, you need to use get_terragrunt_dir() to combine the path with the folder where the terragrunt.hcl file lives: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] # With the get_terragrunt_dir() function, you can use relative paths! arguments = [ \"-var-file=${get_terragrunt_dir()}/../common.tfvars\" ] }} get_working_dirSection titled “get_working_dir” get_working_dir() returns the absolute path where Terragrunt runs OpenTofu/Terraform commands. This is useful when you need to manage substitutions of vars inside a *.tfvars file located right inside terragrunt’s tmp dir. get_parent_terragrunt_dirSection titled “get_parent_terragrunt_dir” get_parent_terragrunt_dir() returns the absolute directory where the Terragrunt parent configuration file lives (regardless of what it’s called). This is useful when you need to use relative paths with remote OpenTofu/Terraform configurations and you want those paths relative to your parent Terragrunt configuration file and not relative to the temporary directory where Terragrunt downloads the code. This function is very similar to get_terragrunt_dir() except it returns the root instead of the leaf of your terragrunt configurations. root.hclcommon.tfvarsDirectoryapp1 terragrunt.hclDirectorytests Directoryapp2 terragrunt.hclDirectoryapp3 terragrunt.hcl terraform { extra_arguments \"common_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] arguments = [ \"-var-file=${get_parent_terragrunt_dir()}/common.tfvars\" ] }} The common.tfvars located in the root folder will be included by all applications, whatever their relative location to the root. If you have include blocks, this function requires a name parameter when used in the child config to specify which include block to base the parent dir on. Example: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"region\" { path = find_in_parent_folders(\"region.hcl\")} terraform { source = \"${get_parent_terragrunt_dir(\"root\")}/modules/vpc\"} get_original_terragrunt_dirSection titled “get_original_terragrunt_dir” get_original_terragrunt_dir() returns the directory where the original Terragrunt configuration file (by default terragrunt.hcl) lives. This is primarily useful when one Terragrunt config is being read from another: e.g., if /tofu-code/terragrunt.hcl calls read_terragrunt_config(\"/foo/bar.hcl\"), and within bar.hcl, you call get_original_terragrunt_dir(), you’ll get back /tofu-code. get_terraform_commands_that_need_varsSection titled “get_terraform_commands_that_need_vars” get_terraform_commands_that_need_vars() returns the list of OpenTofu/Terraform commands that accept -var and -var-file parameters. This function is used when defining extra_arguments. terraform { extra_arguments \"common_var\" { commands = get_terraform_commands_that_need_vars() arguments = [\"-var-file=${get_aws_account_id()}.tfvars\"] }} get_terraform_commands_that_need_inputSection titled “get_terraform_commands_that_need_input” get_terraform_commands_that_need_input() returns the list of OpenTofu/Terraform commands that accept the -input=(true or false) parameter. This function is used when defining extra_arguments. terraform { # Force OpenTofu/Terraform to not ask for input value if some variables are undefined. extra_arguments \"disable_input\" { commands = get_terraform_commands_that_need_input() arguments = [\"-input=false\"] }} get_terraform_commands_that_need_lockingSection titled “get_terraform_commands_that_need_locking” get_terraform_commands_that_need_locking() returns the list of terraform commands that accept the -lock-timeout parameter. This function is used when defining extra_arguments. terraform { # Force OpenTofu/Terraform to keep trying to acquire a lock for up to 20 minutes if someone else already has the lock extra_arguments \"retry_lock\" { commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] }} get_terraform_commands_that_need_parallelismSection titled “get_terraform_commands_that_need_parallelism” get_terraform_commands_that_need_parallelism() returns the list of terraform commands that accept the -parallelism parameter. This function is used when defining extra_arguments. terraform { # Force OpenTofu/Terraform to run with reduced parallelism extra_arguments \"parallelism\" { commands = get_terraform_commands_that_need_parallelism() arguments = [\"-parallelism=5\"] }} get_aws_account_aliasSection titled “get_aws_account_alias” get_aws_account_alias() returns the AWS account alias associated with the current set of credentials. If the alias cannot be found, it will return an empty string. Example: inputs = { account_alias = get_aws_account_alias()} Note: value returned by get_aws_account_alias() can change during parsing of HCL code, for example after evaluation of iam_role attribute. get_aws_account_idSection titled “get_aws_account_id” get_aws_account_id() returns the AWS account id associated with the current set of credentials. Example: remote_state { backend = \"s3\" config = { bucket = \"mycompany-${get_aws_account_id()}\" }} Note: value returned by get_aws_account_id() can change during parsing of HCL code, for example after evaluation of iam_role attribute. get_aws_caller_identity_arnSection titled “get_aws_caller_identity_arn” get_aws_caller_identity_arn() returns the ARN of the AWS identity associated with the current set of credentials. Example: inputs = { caller_arn = get_aws_caller_identity_arn()} Note: value returned by get_aws_caller_identity_arn() can change during parsing of HCL code, for example after evaluation of iam_role attribute. get_terraform_commandSection titled “get_terraform_command” get_terraform_command() returns the current terraform command in execution. Example: inputs = { current_command = get_terraform_command()} get_terraform_cli_argsSection titled “get_terraform_cli_args” get_terraform_cli_args() returns cli args for the current terraform command in execution. Example: inputs = { current_cli_args = get_terraform_cli_args()} get_default_retryable_errorsSection titled “get_default_retryable_errors” get_default_retryable_errors() returns default retryabled errors. Example: retryable_errors = concat(get_default_retryable_errors(), [\"my custom error\"]) get_aws_caller_identity_user_idSection titled “get_aws_caller_identity_user_id” get_aws_caller_identity_user_id() returns the UserId of the AWS identity associated with the current set of credentials. Example: inputs = { caller_user_id = get_aws_caller_identity_user_id()} This allows uniqueness of the storage bucket per AWS account (since bucket name must be globally unique). It is also possible to configure variables specifically based on the account used: terraform { extra_arguments \"common_var\" { commands = get_terraform_commands_that_need_vars() arguments = [\"-var-file=${get_aws_account_id()}.tfvars\"] }} Note: value returned by get_aws_caller_identity_user_id() can change during parsing of HCL code, for example after evaluation of iam_role attribute. run_cmdSection titled “run_cmd” run_cmd(command, arg1, arg2…​) runs a shell command and returns the stdout as the result of the interpolation. The command is executed at the same folder as the terragrunt.hcl file. This is useful whenever you want to dynamically fill in arbitrary information in your Terragrunt configuration. Basic UsageSection titled “Basic Usage” As an example, you could write a script that determines the bucket and DynamoDB table name based on the AWS account, instead of hardcoding the name of every account: remote_state { backend = \"s3\" config = { bucket = run_cmd(\"./get_names.sh\", \"bucket\") dynamodb_table = run_cmd(\"./get_names.sh\", \"dynamodb\") }} Special ParametersSection titled “Special Parameters” The run_cmd function accepts some special flags that can alter how the function evaluates commands on your behalf. Placing these --terragrunt- prefixed flags as the first argument(s) of a run_cmd call will result in the behavior of run_cmd being adjusted. You can mix and match the flags in any order, so long as they precede the command you are running with run_cmd. ParameterDescriptionExample--terragrunt-quietRedacts run_cmd stdout from Terragrunt logs while still returning the value to HCL. This keeps sensitive information out of log files.run_cmd(\"--terragrunt-quiet\", \"./decrypt_secret.sh\", \"foo\")--terragrunt-global-cacheStores and reuses results in a global cache so the command only runs once per set of arguments, no matter which configuration references it. Useful when the output is directory-independent.run_cmd(\"--terragrunt-global-cache\", \"aws\", \"sts\", \"get-caller-identity\", \"--query\", \"Account\", \"--output\", \"text\")--terragrunt-no-cacheSkips the cache entirely and forces the command to run on every evaluation. Use this when the output changes frequently (timestamps, tokens, random IDs, etc.).run_cmd(\"--terragrunt-no-cache\", \"date\", \"+%s\") Terragrunt caches run_cmd results by default to avoid running the same command multiple times during parsing. The cache key includes the directory of the terragrunt.hcl file and the command arguments unless you opt into global caching or disable caching entirely. Parameters --terragrunt-global-cache and --terragrunt-no-cache are mutually exclusive, Terragrunt will return an error if both are provided. ExamplesSection titled “Examples” Suppress output for sensitive values: # Output is redacted in logs, but still available to Terragruntsuper_secret_value = run_cmd(\"--terragrunt-quiet\", \"./decrypt_secret.sh\", \"foo\") Note: This will prevent terragrunt from displaying the output from the command in its output. However, the value could still be displayed in the OpenTofu/Terraform output if OpenTofu/Terraform does not treat it as a sensitive value. Use global cache for directory-independent commands: # Same result regardless of which directory run_cmd is called fromaccount_id = run_cmd(\"--terragrunt-global-cache\", \"aws\", \"sts\", \"get-caller-identity\", \"--query\", \"Account\", \"--output\", \"text\") Disable caching for dynamic values: # Generates a new UUID every time run_cmd is evaluatedbuild_id = run_cmd(\"--terragrunt-no-cache\", \"uuidgen\") # Gets current timestamp on each parse of the Terragrunt configurationtimestamp = run_cmd(\"--terragrunt-no-cache\", \"date\", \"+%s\") Combine multiple parameters: # Disable cache AND suppress output for a sensitive dynamic valuesession_token = run_cmd(\"--terragrunt-no-cache\", \"--terragrunt-quiet\", \"./generate-temp-token.sh\") ### Caching Behavior By default, invocations of `run_cmd` are cached based on the current directory and executed command, so cached values are reused later rather than executed multiple times. Here's an example: ```hcl# terragrunt.hcl locals { uuid = run_cmd(\"echo\", \"uuid1\", uuid()) uuid2 = run_cmd(\"echo\", \"uuid2\", uuid()) uuid3 = run_cmd(\"echo\", \"uuid3\", uuid()) potato = run_cmd(\"echo\", \"potato\") potato2 = run_cmd(\"echo\", \"potato\") carrot = run_cmd(\"echo\", \"carrot\")}inputs = { potato3 = run_cmd(\"echo\", \"potato\") uuid3 = run_cmd(\"echo\", \"uuid3\", uuid()) uuid4 = run_cmd(\"echo\", \"uuid4\", uuid()) carrot2 = run_cmd(\"echo\", \"carrot\")} Output: $ terragrunt inituuid1 b48379e1-924d-2403-8789-c72d50be964cuuid1 9f3a8398-b11f-5314-7783-dad176ee487duuid1 649ac501-e5db-c935-1499-c59fb7a75625uuid2 2d65972b-3fa9-181f-64fe-dcd574d944d0uuid3 e345de60-9cfa-0455-79b7-af0d053a15a5potatouuid3 7f90a4ed-96e3-1dd8-5fee-91b8c8e07650uuid2 8638fe79-c589-bebd-2a2a-3e6b96f7fc34uuid3 310d0447-f0a6-3f67-efda-e6b1521fa1fbuuid4 f8e80cc6-1892-8db7-bd63-6089fef00c01uuid2 289ff371-8021-54c6-2254-72de9d11392auuid3 baa19863-1d99-e0ef-11f2-ede830d1c58acarrot Key observations from the output: carrot and potato appear once because subsequent invocations used cached values uuid1, uuid2, and uuid3 appear multiple times because each call to uuid() generates a different cache key uuid3 appears one extra time because it’s declared in both locals and inputs uuid4 appears once since it’s declared in inputs, which is evaluated once This caching behavior can be modified using the special parameters described in the Special Parameters section above. read_terragrunt_configSection titled “read_terragrunt_config” read_terragrunt_config(config_path, [default_val]) parses the terragrunt config at the given path and serializes the result into a map that can be used to reference the values of the parsed config. This function will expose all blocks and attributes of a terragrunt config. For example, suppose you had a config file called common.hcl that contains common input variables: inputs = { stack_name = \"staging\" account_id = \"1234567890\"} You can read these inputs in another config by using read_terragrunt_config, and merge them into the inputs: locals { common_vars = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))} inputs = merge( local.common_vars.inputs, { # additional inputs }) This function also takes in an optional second parameter which will be returned if the file does not exist: locals { common_vars = read_terragrunt_config(find_in_parent_folders(\"i-dont-exist.hcl\", \"i-dont-exist.hcl\"), {inputs = {}})} inputs = merge( local.common_vars.inputs, # This will be {} { # additional inputs }) Note that this function will also render dependency blocks. That is, the parsed config will make the outputs of the dependency blocks available. For example, suppose you had the following config in a file called common_deps.hcl: dependency \"vpc\" { config_path = \"${get_terragrunt_dir()}/../vpc\"} You can access the outputs of the vpc dependency through the parsed outputs of read_terragrunt_config: locals { common_deps = read_terragrunt_config(find_in_parent_folders(\"common_deps.hcl\"))} inputs = { vpc_id = local.common_deps.dependency.vpc.outputs.vpc_id} Notes: read_terragrunt_config can be also used to read terragrunt.stack.hcl and terragrunt.values.hcl files. sops_decrypt_fileSection titled “sops_decrypt_file” sops_decrypt_file(file_path) decrypts a yaml, json, ini, env or “raw text” file encrypted with sops. sops is an editor of encrypted files that supports YAML, JSON, ENV, INI and BINARY formats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, Hashicorp Vault and PGP. This allows static secrets to be stored encrypted within your Terragrunt repository. For example, suppose you have some static secrets required to bootstrap your infrastructure in secrets.yaml, you can decrypt and merge them into the inputs by using sops_decrypt_file: locals { secret_vars = yamldecode(sops_decrypt_file(find_in_parent_folders(\"secrets.yaml\")))} inputs = merge( local.secret_vars, { # additional inputs }) If you absolutely need to fallback to a default value you can make use of the OpenTofu/Terraform try function: locals { secret_vars = try(jsondecode(sops_decrypt_file(find_in_parent_folders(\"no-secrets-here.json\"))), {})} inputs = merge( local.secret_vars, # This will be {} { # additional inputs }) get_terragrunt_source_cli_flagSection titled “get_terragrunt_source_cli_flag” get_terragrunt_source_cli_flag() returns the value passed in via the CLI --source or an environment variable TG_SOURCE. Note that this will return an empty string when either of those values are not provided. This is useful for constructing before and after hooks, or TF flags that only apply to local development (e.g., setting up debug flags, or adjusting the iam_role parameter). Some example use cases are: Setting debug logging when doing local development. Adjusting the kubernetes provider configuration so that it targets minikube instead of real clusters. Providing special mocks pulled in from the local dev source (e.g., something like mock_outputs = jsondecode(file(\"${get_terragrunt_source_cli_arg()}/dependency_mocks/vpc.json\"))). read_tfvars_fileSection titled “read_tfvars_file” read_tfvars_file(file_path) reads a .tfvars or .tfvars.json file and returns a map of the variables defined in it. This is useful for reading variables from a .tfvars file, merging them into the inputs, or using them in a locals block: locals { inputs_from_tfvars = jsondecode(read_tfvars_file(\"common.tfvars\"))} inputs = merge( local.inputs_from_tfvars, { # additional inputs }) Another example: locals { backend = jsondecode(read_tfvars_file(\"backend.tfvars\"))} remote_state { backend = \"s3\" config = { bucket = \"${get_env(\"TG_BUCKET_PREFIX\", \"tf-bucket\")}-${get_aws_account_id()}\" key = \"${path_relative_to_include()}/terraform-${local.aws_region}.tfstate\" region = local.backend.region } generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" }} mark_as_readSection titled “mark_as_read” mark_as_read(file_path) marks a file as read so that it can be picked up for inclusion by the queue-include-units-reading flag. This is useful for situations when you want to mark a file as read, but are not reading it using a native Terragrunt HCL function. For example: locals { filename = mark_as_read(\"/path/to/my/file-read-by-tofu.txt\") many_files = [for f in fileset(\"./config\", \"*.yaml\") : file(mark_as_read(abspath(\"${get_terragrunt_dir()}/config/${f}\")))]} inputs = { filename = local.filename many_files = local.many_files} By using mark_as_read on file-read-by-tofu.txt, you can ensure that the terragrunt.hcl file passing in the file-read-by-tofu.txt file as an input will be included in any run --all run where the flag --queue-include-units-reading file-read-by-tofu.txt is set. The same technique can be used to mark a file as read when a file is read using code in run_cmd. NOTE: Due to the way that Terragrunt enqueues files we require an absolute path for mark_as_read to avoid multiple inclusions. NOTE: Due to the way that Terragrunt parses configurations during a run --all, functions will only properly mark files as read if they are used in the locals block. Reading a file directly in the inputs block will not mark the file as read, as the inputs block is not evaluated until after the queue has been populated with units to run. constraint_checkSection titled “constraint_check” constraint_check(version, constraint) checks if a given version satisfies a given constraint. This particularly is useful for situations where you want to change the runtime behavior of Terragrunt based on the version of an OpenTofu/Terraform module. For example: feature \"module_version\" { default = \"1.2.3\"} locals { module_version = feature.module_version.value needs_v2_adjustments = constraint_check(local.module_version, \">= 2.0.0\")} terraform { source = \"github.com/my-org/my-module.git//?ref=v${local.module_version}\"} inputs = !local.needs_v2_adjustments ? { old_module_input_name = \"old_module_input_value\"} : { new_module_input_name = \"new_module_input_value\"} In this example, the v2.0.0 version of the module made a breaking change to rename an input variable from old_module_input_name to new_module_input_name. Instead of carefully coordinating the version update with the corresponding input change, users can set a feature flag to control opt-in of the new module version, and have Terragrunt dynamically adjust the input variable name based on the constraint check, that the module version is greater than or equal to 2.0.0. The HCL function supports all the same constraints that you can use for version constraints in terragrunt_version_constraint and terraform_version_constraint. Edit page Previous Attributes Next Overview",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.722Z"
  },
  {
    "title": "Overview",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/",
    "content": "Overview The Terragrunt CLI is designed to make it as easy as possible to manage infrastructure at any scale. To support that design, there are certain patterns that are used throughout the CLI. This document will help you understand those patterns so you can use the CLI more effectively. UsageSection titled “Usage” Most of the time, if you are trying to use Terragrunt to run a command that you would normally run with OpenTofu/Terraform, you can just replace tofu/ terraform with terragrunt. Terragrunt will pass the command to tofu/ terraform with the same arguments. terragrunt plan Terragrunt doesn’t always just pass the command. It frequently does some additional processing to make it easier to manage infrastructure at scale. For example, in the previous plan command, you wouldn’t have to explicitly run init like you would with tofu/ terraform. Terragrunt takes advantage of a feature called Auto-init to automatically run init when necessary. Using Terragrunt in this way is taking advantage of the OpenTofu Shortcuts that Terragrunt provides. OpenTofu Shortcuts Use Terragrunt as a drop-in replacement for OpenTofu/Terraform commands. Terragrunt also has some other commands that are unique to Terragrunt. Main CommandsSection titled “Main Commands” These are the main commands you will use with Terragrunt: exec Execute an arbitrary command, wrapped by Terragrunt. run Run OpenTofu/Terraform commands. Backend CommandsSection titled “Backend Commands” These are the commands that are used when working with OpenTofu/Terraform state backends: backend bootstrap Bootstrap OpenTofu/Terraform backend infrastructure. backend delete Delete backend state used by a unit. backend migrate Migrate OpenTofu/Terraform state from one unit to another. Stack CommandsSection titled “Stack Commands” These are the commands that are used when working with a terragrunt.stack.hcl file: stack clean Remove `.terragrunt-stack` directories created by `stack` commands. stack generate Generate a stack of units based on configurations in a terragrunt.stack.hcl file. stack output Get outputs from a stack of units. stack run Run a command against a stack of units defined in a terragrunt.stack.hcl file. Catalog CommandsSection titled “Catalog Commands” These are the commands that are used when working with a Terragrunt catalog: catalog Launch a Terminal User Interface (TUI) to browse and use OpenTofu/Terraform modules. scaffold Generate Terragrunt configuration files from a catalog. Discovery CommandsSection titled “Discovery Commands” These are the commands that are used to discover units in your Terragrunt project: find Find relevant Terragrunt configurations. list List Terragrunt configurations in your codebase. Configuration CommandsSection titled “Configuration Commands” These are the commands that are used to interact directly with Terragrunt configuration: render Render the Terragrunt configuration in the current working directory, with as much work done as possible beforehand (that is, with all includes merged, dependencies resolved/interpolated, function calls executed, etc). dag graph Graph the Directed Acyclic Graph (DAG) in DOT language. hcl fmt Recursively find HashiCorp Configuration Language (HCL) files and rewrite them into a canonical format. hcl validate Recursively find HashiCorp Configuration Language (HCL) files and validate them. info print Print out a short description of Terragrunt context. Global FlagsSection titled “Global Flags” There are some flags that are available to all Terragrunt commands: Global Flags Global flags for the Terragrunt CLI. Edit page Previous Functions Next OpenTofu Shortcuts",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.749Z"
  },
  {
    "title": "OpenTofu Shortcuts",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/opentofu-shortcuts/",
    "content": "OpenTofu Shortcuts Usage Terragrunt is an orchestration tool for OpenTofu/Terraform, so with a couple exceptions, you can generally use it as a drop-in replacement for OpenTofu/Terraform.Terragrunt has a shortcut for most OpenTofu commands. You can usually just replace `tofu` or `terraform` with `terragrunt` and it will do what you expect. Examples Run the apply command. terragrunt applyRun the plan command. terragrunt planRun the output command. terragrunt output Supported Shortcuts apply destroy force-unlock import init output plan refresh show state test validate Previous Overview Next run",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.778Z"
  },
  {
    "title": "run",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/run/",
    "content": "run Usage Run a command, passing arguments to an orchestrated tofu/terraform binary.This is the explicit, and most flexible form of running an IaC command with Terragrunt. Shortcuts can be found in \"terragrunt --help\" for common use-cases. Examples Run the plan command. terragrunt run plan# Shortcut:# terragrunt planRun the plan command, and pass additional arguments. terragrunt run -- output -json# Shortcut:# terragrunt output -json Running multiple units Note that the run command is a more explicit and flexible way to run OpenTofu/Terraform commands in comparison to OpenTofu shortcuts. The run command also supports the following flags that can be used to drive runs in multiple units: --all: Run the provided OpenTofu/Terraform command against all units in the current stack. --graph: Run the provided OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. Separating Arguments You may, at times, need to explicitly separate the arguments used for Terragrunt from those used for OpenTofu/Terraform. In those circumstances, you can use the argument -- to separate the Terragrunt flags from the OpenTofu/Terraform flags. terragrunt run -- plan -no-color Caution When using run --all plan with units that have dependencies (e.g. via dependency or dependencies blocks), the command will fail if those dependencies have never been deployed. This is because Terragrunt cannot resolve dependency outputs without existing state.To work around this issue, use mock outputs in dependency blocks. Caution Do not set TF_PLUGIN_CACHE_DIR when using run --all. This can cause concurrent access issues with the provider cache. Instead, use Terragrunt’s built-in Provider Cache Server.We are working with the OpenTofu team to improve this behavior in the future. Caution When using run --all with apply or destroy, Terragrunt automatically adds the -auto-approve flag due to limitations with shared stdin making individual approvals impossible. Flags --all Run the specified OpenTofu/Terraform command on the stack of units in the current directory. When this flag is set, Terragrunt will run the specified OpenTofu/Terraform command against all units in the current stack. This is useful when you want to apply changes across multiple units at once. For example: terragrunt run --all plan This will run the plan command against all units in the current stack. To learn more about how to use this flag, see the Stacks feature documentation. Note Terragrunt has a notion of “external dependencies”. These are units that are not part of the current stack, from the perspective of the current working directory, but are dependencies of units that are part of the current stack.When running an --all command, Terragrunt will discover any external dependencies and prompt you to confirm that you want to run them as well.If you would like to suppress this prompt, you can use the --non-interactive flag. You can also prevent this behavior by setting --queue-exclude-external. Danger When running a run --all destroy command, Terragrunt will destroy all dependencies of the units under the current working directory, in addition to the units themselves by default!If you wish to prevent external dependencies from being destroyed, add the --queue-exclude-external flag, or use the --exclude-dir flag once for each directory you wish to exclude. Caution Use run --all with care if you have unapplied dependencies.If you have a stack of Terragrunt units with dependencies between them via dependency blocks and you’ve never deployed them, then commands like run --all plan will fail, as it won’t be possible to resolve outputs of dependency blocks without applying them first.The solution for this is to take advantage of mock outputs in dependency blocks. Note Using run --all with apply or destroy silently adds the -auto-approve flag to the command line arguments passed to OpenTofu/Terraform due to issues with shared stdin making individual approvals impossible. Note Using the OpenTofu/Terraform -detailed-exitcode flag with the run --all command results in an aggregate exit code being returned, rather than the exit code of any particular unit.The algorithm for determining the aggregate exit code is as follows: If any unit throws a 1, Terragrunt will throw a 1. If any unit throws a 2, but nothing throws a 1, Terragrunt will throw a 2. If nothing throws a non-zero, Terragrunt will throw a 0. Type: bool Environment Variables: TG_ALL --auth-provider-cmd Run the provided command and arguments to authenticate Terragrunt dynamically when necessary. The command and arguments used to obtain authentication credentials dynamically. If specified, Terragrunt runs this command whenever it might need authentication. This includes HCL parsing, where it might be useful to authenticate with a cloud provider before running HCL functions like get_aws_account_id where authentication has to already have taken place. It can also be useful for HCL functions like run_cmd where it may be useful to be authenticated before calling the function. The output must be valid JSON of the following schema: { \"awsCredentials\": { \"ACCESS_KEY_ID\": \"\", \"SECRET_ACCESS_KEY\": \"\", \"SESSION_TOKEN\": \"\" }, \"awsRole\": { \"roleARN\": \"\", \"sessionName\": \"\", \"duration\": 0, \"webIdentityToken\": \"\" }, \"envs\": { \"ANY_KEY\": \"\" }} This allows Terragrunt to acquire different credentials at runtime without changing any terragrunt.hcl configuration. You can use this flag to set arbitrary credentials for continuous integration, authentication with providers other than AWS and more. As long as the standard output of the command passed to auth-provider-cmd results in JSON matching the schema above, corresponding environment variables will be set (and/or roles assumed) before Terragrunt begins parsing an terragrunt.hcl file or running an OpenTofu/Terraform command. The simplest approach to leverage this flag is to write a script that fetches desired credentials, and emits them to STDOUT in the JSON format listed above: #!/usr/bin/env bash echo -n '{\"envs\": {\"KEY\": \"a secret\"}}' You can use any technology for the authentication provider you’d like, however, as long as Terragrunt can execute it. The expected pattern for using this flag is to author a script/program that will dynamically fetch secret values from a secret store, etc. then emit them to STDOUT for consumption by Terragrunt. Note that more specific configurations (e.g. awsCredentials) take precedence over less specific configurations (e.g. envs). If you would like to set credentials for AWS with this method, you are encouraged to use awsCredentials instead of envs, as these keys will be validated to conform to the officially supported environment variables expected by the AWS SDK. Similarly, if you would like Terragrunt to assume an AWS role on your behalf, you are encouraged to use the awsRole configuration instead of envs. Other credential configurations will be supported in the future, but until then, if your provider authenticates via environment variables, you can use the envs field to fetch credentials dynamically from a secret store, etc before Terragrunt executes any IAC. Note The awsRole configuration is only used when the awsCredentials configuration is not present. If both are present, the awsCredentials configuration will take precedence. Caution When using iam-assume-role, AWS authentication takes place right before a Terraform run, after terragrunt.hcl files are fully parsed. This means HCL functions like get_aws_account_id and run_cmd will not run with the assumed role credentials. If you need roles to be assumed prior to parsing configurations, use auth-provider-cmd instead. Type: string Environment Variables: TG_AUTH_PROVIDER_CMD --config The path to the Terragrunt config file. Default is terragrunt.hcl. This flag allows you to specify a custom path to your Terragrunt configuration file. By default, Terragrunt looks for a file named terragrunt.hcl in the current directory. This is useful when you: Have multiple Terragrunt configurations in the same directory. Want to use a different naming convention for your configuration files. Need to test alternative configurations without modifying the default file. Example usage: terragrunt run plan --config custom-config.hcl Type: string Environment Variables: TG_CONFIG --json-out-dir Directory to store JSON plan files generated from plans via show -json. Note This flag only does anything when used with the --all flag for stack runs. It does nothing in single-unit runs.To generate JSON plan files for a single unit run, use the standard OpenTofu/Terraform -out flag like so:terragrunt run -- plan -out=/tmp/tfplanterragrunt run -- show -json /tmp/tfplan > /tmp/tfplan.json Generates machine-readable plan outputs per unit as tfplan.json by invoking show -json after the plan is created. Requires a plan to exist (e.g. created with --out-dir). If the directory does not exist, Terragrunt creates it. Relative paths are resolved against the root working directory, and Terragrunt mirrors the unit path under this directory (e.g. <json-out-dir>/<relative-unit-path>/tfplan.json). Examples: # Create native plan files and JSON plans for all unitsterragrunt run --all --out-dir /tmp/all --json-out-dir /tmp/all plan # Or store JSON plans separatelyterragrunt run --all --out-dir /tmp/plan --json-out-dir /tmp/json plan See the Stacks docs for more usage examples. Type: string Environment Variables: TG_JSON_OUT_DIR --dependency-fetch-output-from-state Fetch dependency outputs directly from the state file instead of using `tofu output`. This flag modifies how Terragrunt retrieves output values from dependent units. When enabled, Terragrunt will read the outputs directly from the state file instead of running terraform output or tofu output. The main benefit this flag provides is performance. Reading directly from state is typically faster than executing the OpenTofu/Terraform binary to get the same outputs. The limitation of this approach is that it is only supported by the S3 backend, and OpenTofu/Terraform may change the schema of the state file in the future, breaking this functionality. Caution Avoid using this flag without pinning the version of OpenTofu/Terraform you are using.There is no guarantee that OpenTofu/Terraform will maintain the existing schema of their state files, so there is also no guarantee that the flag will work as expected in future versions of OpenTofu/Terraform. They have not changed the schema of the state file in a long time, but there is no guarantee that they will not change it in the future. We are coordinating with the OpenTofu team to encourage stability in the state file schema, unless significant performance improvements can be made to OpenTofu output fetching to make this flag unnecessary. Tip Direct output fetching is a performance optimization. For more details on performance optimizations, their tradeoffs, and other performance tips, read the dedicated Performance documentation. Type: bool Environment Variables: TG_DEPENDENCY_FETCH_OUTPUT_FROM_STATE --disable-bucket-update When this flag is set, Terragrunt will not update the remote state resources. When enabled, Terragrunt will throw an error if it detects that remote state resources need to be updated. This is useful in scenarios where: You want to ensure state bucket configurations remain unchanged during operations You have separate processes for managing state bucket configurations The flag acts as a safety mechanism to prevent unintended modifications to your state storage infrastructure. Type: bool Environment Variables: TG_DISABLE_BUCKET_UPDATE --disable-command-validation When this flag is set, Terragrunt will not validate the tofu/terraform command. This flag disables Terragrunt’s built-in validation of OpenTofu/Terraform commands. When enabled, Terragrunt will pass commands through to OpenTofu/Terraform without checking if they are valid or supported. Type: bool Environment Variables: TG_DISABLE_COMMAND_VALIDATION --download-dir The path to download OpenTofu/Terraform modules into. Default is .terragrunt-cache in the working directory. Specifies a custom directory where Terragrunt will download and cache OpenTofu/Terraform modules. By default, modules are downloaded to .terragrunt-cache in the working directory. Note The download directory (.terragrunt-cache by default) can grow significantly over time with multiple versions of modules.Remember to add this directory to your .gitignore file and consider periodic cleanup of old cached content. Type: string Environment Variables: TG_DOWNLOAD_DIR --out-dir Directory to store plan files generated by plan/show and consumed by apply/destroy. Note This flag only does anything when used with the --all flag for stack runs. It does nothing in single-unit runs.To generate plan files for a single unit run, use the standard OpenTofu/Terraform -out flag like so:terragrunt run -- plan -out=/tmp/tfplan Specifies where Terragrunt writes native OpenTofu/Terraform plan files for each unit when running stack operations. Plans are written as tfplan.tfplan per unit. If the directory does not exist, Terragrunt creates it. Relative paths are resolved against the root working directory, and Terragrunt mirrors the unit path under this directory (e.g. <out-dir>/<relative-unit-path>/tfplan.tfplan). Examples: # Save plans for all units under /tmp/tfplanterragrunt run --all plan --out-dir /tmp/tfplan # Apply using the previously generated plans (per-unit paths are inferred)terragrunt run --all apply --out-dir /tmp/tfplan See the Stacks docs for more usage examples. Type: string Environment Variables: TG_OUT_DIR --engine-cache-path Cache path for Terragrunt engine files. Specifies the directory where Terragrunt will store its engine cache files. The engine cache helps improve performance by storing compiled configurations and other internal data. When not specified, Terragrunt will use a default location in the system’s temporary directory. You must also set the experimental-engine flag to enable engine usage. Type: string Environment Variables: TG_ENGINE_CACHE_PATH --engine-log-level Terragrunt engine log level. Controls the verbosity of logs from the Terragrunt engine. This is separate from the main Terragrunt logging and specifically affects engine-related operations like configuration parsing and dependency resolution. By default, the engine log level is set to that of the main Terragrunt logging level. You must also set the experimental-engine flag to enable engine usage. Type: string Environment Variables: TG_ENGINE_LOG_LEVEL --engine-skip-check Skip checksum check for Terragrunt engine files. When enabled, Terragrunt will skip the checksum validation of engine files. This can be useful during development or testing, and is currently the only way to use self-developed engines. Terragrunt currently only has support for verifying the signatures of official engines, though this will change in the future. In the meantime, you are encouraged to download engines from a trusted source, verify the integrity of the engine manually, and then reference the local path to the engine in your Terragrunt configuration. You must also set the experimental-engine flag to use engines. Type: bool Environment Variables: TG_ENGINE_SKIP_CHECK --experimental-engine Enable Terragrunt experimental engine. Enables usage of Terragrunt IaC engines. IaC engines are still experimental, as the API is unstable and may change in future minor versions of Terragrunt. If you are using a remote custom engine, you must also set the engine-skip-check flag to skip the signature check for the engine. Type: bool Environment Variables: TG_EXPERIMENTAL_ENGINE --feature Set feature flags for the HCL code. Allows enabling or disabling specific features in the HCL code. To learn more about feature flags, see the Feature Flags feature documentation. Type: string Environment Variables: TG_FEATURE --graph Run the provided OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. When this flag is set, Terragrunt will run the specified OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. The graph consists of all units that depend on the unit in the current working directory via a dependency or dependencies blocks, plus all the units that depend on those units, and all the units that depend on those units, and so on, recursively up the tree, up to the Git repository root. The command will be executed following the order of dependencies: it’ll run on the unit in the current working directory first, then on units that depend on it directly, then on the units that depend on those units, and so on. Note that if the command is destroy, it will run in the opposite order (the final dependents, then their dependencies, etc. up to the unit you ran the command in). Example Usage Given the following dependency graph: Running terragrunt run --graph plan in the eks module will lead to the following execution order: Group 1- Module project/eks Group 2- Module project/services/eks-service-1- Module project/services/eks-service-2 Group 3- Module project/services/eks-service-2-v2- Module project/services/eks-service-3- Module project/services/eks-service-5 Group 4- Module project/services/eks-service-3-v2- Module project/services/eks-service-4 Group 5- Module project/services/eks-service-3-v3 Notes: lambda units aren’t included in the graph because they are not dependent on the eks unit Execution is from bottom up based on dependencies Running terragrunt run --graph destroy in the eks unit will lead to the following execution order: Group 1- Module project/services/eks-service-2-v2- Module project/services/eks-service-3-v3- Module project/services/eks-service-4- Module project/services/eks-service-5 Group 2- Module project/services/eks-service-3-v2 Group 3- Module project/services/eks-service-3 Group 4- Module project/services/eks-service-1- Module project/services/eks-service-2 Group 5- Module project/eks Notes: Execution is in reverse order; first are destroyed “top” units and in the end eks lambda units aren’t affected at all To learn more about how to use this flag, see the Stacks feature documentation. Caution When running graph destroy, the execution order is reversed compared to other commands. Dependencies will be destroyed in reverse order to ensure resources are safely removed (dependents are destroyed before their dependencies).Always verify the execution plan before running destructive commands. Danger External dependencies (units outside your current working directory) are not automatically included in graph runs.You must explicitly include them using --queue-include-external if they need to be part of the execution. Type: bool Environment Variables: TG_GRAPH --iam-assume-role Assume the specified IAM role before executing OpenTofu/Terraform. Specifies an IAM role ARN that Terragrunt should assume before executing OpenTofu/Terraform commands. This is useful for managing resources across different AWS accounts or with different permission sets. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE --iam-assume-role-duration Session duration for IAM Assume Role session. Specifies how long the temporary credentials should remain valid when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_DURATION --iam-assume-role-session-name Name for the IAM Assumed Role session. Specifies a custom session name when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_SESSION_NAME --iam-assume-role-web-identity-token For AssumeRoleWithWebIdentity, the WebIdentity token. Specifies the WebIdentity token to use when assuming an IAM role using web identity federation. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN --inputs-debug Write debug.tfvars to working folder to help root-cause issues. When enabled, Terragrunt will write a debug.tfvars file to the working directory. This file contains the resolved input values and can be useful for debugging configuration issues. To learn more about how to use this flag, see the Debugging guide. Type: bool Environment Variables: TG_INPUTS_DEBUG --no-auto-approve Don't automatically append '-auto-approve' to the underlying OpenTofu/Terraform commands run with 'run --all'. When enabled, Terragrunt will not automatically append the -auto-approve flag to destructive commands like apply or destroy when running with --all. This means you’ll be prompted for confirmation before making changes. Type: bool Environment Variables: TG_NO_AUTO_APPROVE --no-auto-init Don't automatically run init on tofu/terraform commands. When enabled, Terragrunt will not automatically run init before other OpenTofu/Terraform commands. To learn more about how to use this flag, see the Auto-init feature documentation. Caution When --no-auto-init is enabled, you must manually run terragrunt init if needed. Terragrunt will fail if it detects that initialization is required but auto-init is disabled.This is particularly important when using custom plugin directories or when working in CI/CD environments. Type: bool Environment Variables: TG_NO_AUTO_INIT --no-auto-provider-cache-dir Disable the auto-provider-cache-dir feature even when the experiment is enabled. When enabled, Terragrunt will disable the auto-provider-cache-dir feature. This flag allows you to selectively opt-out of the automatic provider caching behavior. This is useful when you want to maintain control over provider caching in specific environments or scenarios. Type: bool Environment Variables: TG_NO_AUTO_PROVIDER_CACHE_DIR --no-auto-retry Don't automatically retry commands which fail with transient errors. When enabled, Terragrunt will not automatically retry commands that fail with transient errors (like network timeouts). By default, Terragrunt will retry such commands a limited number of times. To learn more about how Terragrunt handles retries, see the Errors section of the Runtime Control feature documentation. Type: bool Environment Variables: TG_NO_AUTO_RETRY --no-destroy-dependencies-check Disables Terragrunt's dependency validation during destroy operations. When enabled, Terragrunt will not verify if there are dependent units that would be affected by destroying the current unit. This results in faster destroy operations, but may result in orphaned resources. Caution Disabling dependency checks during destroy operations can lead to orphaned resources or broken infrastructure dependencies. Type: bool Environment Variables: TG_NO_DESTROY_DEPENDENCIES_CHECK --parallelism Parallelism for --all commands. Sets the maximum number of concurrent operations when running commands with --all. This helps control resource usage and API rate limits when working with multiple units. Caution When using --parallelism with provider caching, terraform init is always executed sequentially if OpenTofu/Terraform provider plugin cache is configured.To safely run concurrent operations with provider caching, enable the Provider Cache Server instead. Type: integer Environment Variables: TG_PARALLELISM --provider-cache Enables Terragrunt's provider caching. When enabled, Terragrunt will use its provider caching system to speed up provider downloads. For more information, see the Provider Cache Server feature documentation. Note The tofu init command is always executed sequentially if the provider plugin cache is used without the Provider Cache Server. Type: bool Environment Variables: TG_PROVIDER_CACHE --provider-cache-dir The path to the Terragrunt provider cache directory. By default, 'terragrunt/providers' folder in the user cache directory. Specifies the directory where Terragrunt will store cached provider files. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_DIR --provider-cache-hostname The hostname of the Terragrunt Provider Cache server. By default, 'localhost'. Sets the hostname for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_HOSTNAME --provider-cache-port The port of the Terragrunt Provider Cache server. By default, assigned automatically. Specifies the port number for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_PORT --provider-cache-registry-names The list of remote registries to cached by Terragrunt Provider Cache server. By default, 'registry.terraform.io', 'registry.opentofu.org'. Specifies which provider registries should be cached by the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_REGISTRY_NAMES --provider-cache-token The token for authentication to the Terragrunt Provider Cache server. By default, assigned automatically. Sets the authentication token for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_TOKEN --queue-exclude-dir Unix-style glob of directories to exclude from the queue of Units to run. Specifies directories to exclude when running commands with all. This flag can be specified multiple times to exclude multiple directories. When using the TG_QUEUE_EXCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note When using --queue-exclude-dir, only the specified units are excluded, not their dependencies.Dependencies of excluded units will still be run unless they are also explicitly excluded. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_EXCLUDE_DIR --queue-exclude-external Exclude external dependencies from the queue of Units to run. When enabled, Terragrunt will exclude external dependencies (dependencies outside the current working directory) from the queue when running commands with all. Note that an external dependency is a dependency that is outside the current Terragrunt working directory and is not within any directories specified by queue-include-dir. This flag is useful when you want to limit the scope of execution to only units within your current working directory structure. Type: bool Environment Variables: TG_QUEUE_EXCLUDE_EXTERNAL --queue-excludes-file Path to a file with a list of directories that need to be excluded when running `run --all` commands. Units in these directories will be excluded during execution of the commands. If a relative path is specified, it should be relative from —working-dir. This will only exclude the module, not its dependencies. This flag has been designed to integrate nicely with the hcl validate command, which can return a list of invalid files delimited by newlines when passed the --show-config-path flag. To integrate the two, you can run something like the following using bash process substitution: terragrunt run --all plan --queue-excludes-file <(terragrunt hcl validate --show-config-path || true) Type: string Default: .terragrunt-excludes Environment Variables: TG_QUEUE_EXCLUDES_FILE --queue-ignore-dag-order Ignore DAG order for --all commands. When enabled, Terragrunt will ignore the dependency order when running commands with --all. This means units will be executed in arbitrary order, which can be useful for read-only operations like plan. Note that this can lead to errors if used with commands that modify state, as dependencies might be processed in the wrong order. To learn more about how to use this flag, see the Stacks feature documentation. Type: bool Environment Variables: TG_QUEUE_IGNORE_DAG_ORDER --queue-ignore-errors Continue processing Units even if a dependency fails. When enabled, Terragrunt will continue processing remaining units even if one fails. This can be useful when you want to see all potential errors in your configuration, rather than stopping at the first failure. Note that this may lead to incomplete or inconsistent states if used with commands that modify infrastructure. To learn more about how to use this flag, see the Stacks feature documentation. Danger Using --queue-ignore-errors can lead to inconsistent state if dependencies fail but dependent modules continue running.Use this flag with caution, especially with apply or destroy commands.For more fine-grained error handling, see the errors block in the HCL reference. Type: bool Environment Variables: TG_QUEUE_IGNORE_ERRORS --queue-include-dir Unix-style glob of directories to include in the queue of Units to run. Specifies directories to include when running commands with --all. This flag can be specified multiple times to include multiple directories. When using the TG_QUEUE_INCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_INCLUDE_DIR --queue-include-external Include external dependencies in the queue of Units to run. When enabled, Terragrunt will include external dependencies (dependencies discovered outside the current working directory) in the queue when running commands with --all or --graph. Type: bool Environment Variables: TG_QUEUE_INCLUDE_EXTERNAL --queue-include-units-reading If flag is set, 'run --all' will only run the command against Terragrunt units that read the specified file via an HCL function or include. This flag works very similarly to the --queue-units-that-include flag, but instead of looking only for included configurations, it also looks for configurations that read a given file. When passed in, the --all command will include all units (modules) that read a given file into the queue. This is useful when you want to trigger an update on all units that read or include a given file using HCL functions in their configurations. Consider the following folder structure: Directoryreading-shared-hcl terragrunt.hclDirectoryalso-reading-shared-hcl terragrunt.hclDirectorynot-reading-shared-hcl terragrunt.hclshared.hcl Suppose that reading-shared-hcl and also-reading-shared-hcl both read shared.hcl in their configurations, like so: locals { shared = read_terragrunt_config(find_in_parent_folders(\"shared.hcl\"))} If you run the command run --all init --queue-include-units-reading shared.hcl from the root folder, both reading-shared-hcl and also-reading-shared-hcl will be run; not not-reading-shared-hcl. This is because the read_terragrunt_config HCL function has a special hook that allows Terragrunt to track that it has read the file shared.hcl. This hook is used by all native HCL functions that Terragrunt supports which read files. Note, however, that there are certain scenarios where Terragrunt may not be able to track that a file has been read this way. For example, you may be using a bash script to read a file via run_cmd, or reading the file via OpenTofu code. To support these use-cases, the mark_as_read function can be used to manually mark a file as read. That would look something like this: locals { filename = mark_as_read(\"file-read-by-tofu.txt\")} inputs = { filename = local.filename} Caution Due to how Terragrunt parses configurations during run --all, functions will only properly mark files as read if they are used in the locals block.Reading a file directly in the inputs block will not mark the file as read, as the inputs block is not evaluated until after the queue has already been populated. Type: string Environment Variables: TG_QUEUE_INCLUDE_UNITS_READING --queue-strict-include Only process the directories matched by --queue-include-dir. When enabled, Terragrunt will only process directories that match the patterns specified by --queue-include-dir. For example, with the following directory structure: Directoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectorystaging Directoryapp terragrunt.hclDirectorydb terragrunt.hcl Running terragrunt run --all plan --queue-include-dir \"prod/*\" would process all directories, but the --all flag includes by default when no excludes are provided, so the stage stack would also be included by default. Running terragrunt run --all plan --queue-include-dir \"prod/*\" --queue-strict-include tells Terragrunt to exclude by default, so it only include units prod/app and prod/db. Danger When using --queue-strict-include, all dependencies of included directories will be excluded if they are not explicitly included. This can lead to unexpected behavior if you’re not careful with your include patterns.Always verify the modules that will be included before running destructive commands. Type: bool Environment Variables: TG_QUEUE_STRICT_INCLUDE --report-file The path where a run report should be generated. By default, the format of the report will be automatically detected based on the file extension. A .csv extension will generate a CSV report, and a .json extension will generate a JSON report. Anything else will default to generating a CSV report. To explicitly specify the format of the report, use the report-format flag. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_FILE --report-format The format of the run report. The supported formats are: csv json The default is csv. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_FORMAT --report-schema-file When passed in, a JSON schema for the report will be generated at the specified path. This is useful when you need a programmatic way to validate that the report conforms to an expected schema. Example terragrunt run --all plan --report-schema-file report.schema.json The schema will be generated at the given path in the current working directory. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_SCHEMA_FILE --source Download OpenTofu/Terraform configurations from the specified source into a temporary folder, and run Terraform in that temporary folder. Specifies a source URL where Terragrunt should download the OpenTofu/Terraform configuration. This overrides any source parameters specified in the Terragrunt configuration files. The configuration will be downloaded into a temporary folder, and Terragrunt will execute OpenTofu/Terraform commands in that folder. Example: terragrunt run plan --source github.com/example/infrastructure//modules/vpc This is particularly useful when: Testing changes to modules without modifying the source in your configuration. Using a specific version or branch of your infrastructure modules. Running configurations from a different source than what’s specified in your Terragrunt files. Type: string Environment Variables: TG_SOURCE --source-map Replace any source URL (including the source URL of a config pulled in with dependency blocks) that has root source with dest. Allows you to replace source URLs that match a specified pattern with a different URL. This affects both direct source references and sources specified in dependency blocks. The format is source-regex=replacement-source. For example: terragrunt run plan --source-map \"git::ssh://git@github.com/org/repo.git=../local/repo\" This will replace any source URL that matches git::ssh://git@github.com/org/repo.git with ../local/repo. Caution Source mapping only performs literal matches on the URL portion. For example, a map key of ssh://git@github.com/org/repo.git will not match sources of the form git::ssh://git@github.com/org/repo.git. The latter requires a map key of git::ssh://git@github.com/org/repo.git. Type: string Environment Variables: TG_SOURCE_MAP --source-update Delete the contents of the temporary folder to clear out any old, cached source code before downloading new source code into it. When enabled, Terragrunt will delete any existing content in the temporary download directory before fetching new source code into it. This ensures you’re working with a fresh copy of the source code. Example: terragrunt run plan --source-update Type: bool Environment Variables: TG_SOURCE_UPDATE --summary-disable Disables the summary output at the end of a run. When enabled, Terragrunt will disable the summary output at the end of a run. For more information, see the Run Report feature. Type: bool Environment Variables: TG_SUMMARY_DISABLE --summary-per-unit Summarize each unit in the run summary. When enabled, Terragrunt will break down the run summary by unit. The units are sorted by result, then duration, with the longest-running units shown first. For more information, see the Run Report feature. Type: bool Environment Variables: TG_SUMMARY_PER_UNIT --tf-forward-stdout If specified, the output of OpenTofu/Terraform commands will be printed as is, without being integrated into the Terragrunt log. By default, Terragrunt integrates OpenTofu/Terraform output into its own logging system. When this flag is enabled, OpenTofu/Terraform output will be printed directly to stdout without Terragrunt’s logging wrapper. For example, without --tf-forward-stdout: 14:19:25.081 INFO [app] Running command: tofu plan -input=false14:19:25.174 STDOUT [app] tofu: OpenTofu used the selected providers to generate the following execution14:19:25.174 STDOUT [app] tofu: plan. Resource actions are indicated with the following symbols:14:19:25.174 STDOUT [app] tofu: + create14:19:25.174 STDOUT [app] tofu: OpenTofu will perform the following actions: With --tf-forward-stdout: 14:19:25.081 INFO [app] Running command: tofu plan -input=false OpenTofu used the selected providers to generate the following executionplan. Resource actions are indicated with the following symbols: + create OpenTofu will perform the following actions: Type: bool Environment Variables: TG_TF_FORWARD_STDOUT --tf-path Path to the OpenTofu/Terraform binary. Default is tofu (on PATH). Specifies the path to the OpenTofu/Terraform binary that Terragrunt should use. By default, Terragrunt will look for a binary named tofu in your system’s PATH. This is useful when: You have multiple versions of OpenTofu/Terraform installed. The binary is not in your PATH. You want to use a specific version for certain operations. You want to switch between OpenTofu and Terraform binaries. Example: terragrunt run plan --tf-path=/usr/local/bin/tofu Note that if you only have terraform installed, and it is available in your PATH, Terragrunt will automatically use that binary. NOTE: This will override the terraform binary that is used by terragrunt in all instances, including dependency lookups. This setting will also override any terraform_binary configuration values specified in the terragrunt.hcl config for both the top level, and dependency lookups. Type: string Environment Variables: TG_TF_PATH --units-that-include If flag is set, 'run --all' will only run the command against Terragrunt units that include the specified file. When specified, Terragrunt will only run commands against units that include the specified configuration file. This is useful for targeting specific configurations that share common included files. For example, with the following directory structure: Directorydev Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectory_envcommon app.hcldb.hcl If both dev/app/terragrunt.hcl and prod/app/terragrunt.hcl include files from _envcommon, you can target all units that include the app.hcl file: terragrunt run --all --units-that-include=./_envcommon/app.hcl plan This will only run the command against units that include the app.hcl file. Note: This flag is functionally covered by the --queue-include-units-reading flag, but is more explicitly focused on the include configuration block. Type: string Environment Variables: TG_UNITS_THAT_INCLUDE --use-partial-parse-config-cache Enables caching of includes during partial parsing operations. Will also be used for the --iam-assume-role option if provided. This flag can be used to drastically decrease time required for parsing Terragrunt configuration files. The effect will only show if a lot of similar includes are expected such as the root terragrunt configuration (e.g. root.hcl) include. NOTE: This is an experimental feature, use with caution. The reason you might want to use this flag is that Terragrunt frequently only needs to perform a partial parse of Terragrunt configurations. This is the case for scenarios like: Building the Directed Acyclic Graph (DAG) during a run --all command where only the dependency blocks need to be evaluated to determine run order. Parsing the terraform block to determine state configurations for fetching dependency outputs. Determining whether Terragrunt execution behavior has to change like for prevent_destroy or skip flags in configuration. These configurations are generally safe to cache, but due to the nature of HCL being a dynamic configuration language, there are some edge cases where caching these can lead to incorrect behavior. Once this flag has been tested thoroughly, we will consider making it the default behavior. Caution This is an experimental feature. While it can significantly improve performance with frequently included configurations, the caching behavior may lead to unexpected results in some edge cases due to HCL’s dynamic nature.Test thoroughly in your environment before using in production environments. Type: bool Environment Variables: TG_USE_PARTIAL_PARSE_CONFIG_CACHE --version-manager-file-name File names used during the computation of the cache key for the version manager files. By default, terragrunt is specifying: .terraform-version.tool-versions.mise.tomlmise.toml Type: string Environment Variables: TG_VERSION_MANAGER_FILE_NAME Previous OpenTofu Shortcuts Next exec",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.850Z"
  },
  {
    "title": "exec",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/exec/",
    "content": "exec Usage Execute an arbitrary command, wrapped by Terragrunt. Examples Execute 'echo \"Hello, Terragrunt!\"' via Terragrunt. terragrunt exec -- echo \"Hello, Terragrunt!\"Inspect `main.tf` file of module for Unit terragrunt exec --in-download-dir -- cat main.tf Difference between run and exec In contrast to the run command, which will always invoke OpenTofu/Terraform, the exec command allows for execution of any arbitrary command via Terragrunt. This can be useful, as it allows you full control over the process that is being orchestrated by Terragrunt, while taking advantage of Terragrunt’s features such as dependency resolution, inputs, and more. Interaction with configuration When using exec, you will have almost the exact same configuration context that you have when using run, including inputs. inputs = { message = \"Hello, Terragrunt!\"} Running the following command will show that the message input is available to the command: $ terragrunt exec -- env | grep 'TF_VAR_message'TF_VAR_message=Hello, Terragrunt! Flags --auth-provider-cmd Run the provided command and arguments to authenticate Terragrunt dynamically when necessary. The command and arguments used to obtain authentication credentials dynamically. If specified, Terragrunt runs this command whenever it might need authentication. This includes HCL parsing, where it might be useful to authenticate with a cloud provider before running HCL functions like get_aws_account_id where authentication has to already have taken place. It can also be useful for HCL functions like run_cmd where it may be useful to be authenticated before calling the function. The output must be valid JSON of the following schema: { \"awsCredentials\": { \"ACCESS_KEY_ID\": \"\", \"SECRET_ACCESS_KEY\": \"\", \"SESSION_TOKEN\": \"\" }, \"awsRole\": { \"roleARN\": \"\", \"sessionName\": \"\", \"duration\": 0, \"webIdentityToken\": \"\" }, \"envs\": { \"ANY_KEY\": \"\" }} This allows Terragrunt to acquire different credentials at runtime without changing any terragrunt.hcl configuration. You can use this flag to set arbitrary credentials for continuous integration, authentication with providers other than AWS and more. As long as the standard output of the command passed to auth-provider-cmd results in JSON matching the schema above, corresponding environment variables will be set (and/or roles assumed) before Terragrunt begins parsing an terragrunt.hcl file or running an OpenTofu/Terraform command. The simplest approach to leverage this flag is to write a script that fetches desired credentials, and emits them to STDOUT in the JSON format listed above: #!/usr/bin/env bash echo -n '{\"envs\": {\"KEY\": \"a secret\"}}' You can use any technology for the authentication provider you’d like, however, as long as Terragrunt can execute it. The expected pattern for using this flag is to author a script/program that will dynamically fetch secret values from a secret store, etc. then emit them to STDOUT for consumption by Terragrunt. Note that more specific configurations (e.g. awsCredentials) take precedence over less specific configurations (e.g. envs). If you would like to set credentials for AWS with this method, you are encouraged to use awsCredentials instead of envs, as these keys will be validated to conform to the officially supported environment variables expected by the AWS SDK. Similarly, if you would like Terragrunt to assume an AWS role on your behalf, you are encouraged to use the awsRole configuration instead of envs. Other credential configurations will be supported in the future, but until then, if your provider authenticates via environment variables, you can use the envs field to fetch credentials dynamically from a secret store, etc before Terragrunt executes any IAC. Note The awsRole configuration is only used when the awsCredentials configuration is not present. If both are present, the awsCredentials configuration will take precedence. Caution When using iam-assume-role, AWS authentication takes place right before a Terraform run, after terragrunt.hcl files are fully parsed. This means HCL functions like get_aws_account_id and run_cmd will not run with the assumed role credentials. If you need roles to be assumed prior to parsing configurations, use auth-provider-cmd instead. Type: string Environment Variables: TG_AUTH_PROVIDER_CMD --config The path to the Terragrunt config file. Default is terragrunt.hcl. This flag allows you to specify a custom path to your Terragrunt configuration file. By default, Terragrunt looks for a file named terragrunt.hcl in the current directory. This is useful when you: Have multiple Terragrunt configurations in the same directory. Want to use a different naming convention for your configuration files. Need to test alternative configurations without modifying the default file. Example usage: terragrunt run plan --config custom-config.hcl Type: string Environment Variables: TG_CONFIG --download-dir The path to download OpenTofu/Terraform modules into. Default is .terragrunt-cache in the working directory. Specifies a custom directory where Terragrunt will download and cache OpenTofu/Terraform modules. By default, modules are downloaded to .terragrunt-cache in the working directory. Note The download directory (.terragrunt-cache by default) can grow significantly over time with multiple versions of modules.Remember to add this directory to your .gitignore file and consider periodic cleanup of old cached content. Type: string Environment Variables: TG_DOWNLOAD_DIR --iam-assume-role Assume the specified IAM role before executing OpenTofu/Terraform. Specifies an IAM role ARN that Terragrunt should assume before executing OpenTofu/Terraform commands. This is useful for managing resources across different AWS accounts or with different permission sets. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE --iam-assume-role-duration Session duration for IAM Assume Role session. Specifies how long the temporary credentials should remain valid when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_DURATION --iam-assume-role-session-name Name for the IAM Assumed Role session. Specifies a custom session name when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_SESSION_NAME --iam-assume-role-web-identity-token For AssumeRoleWithWebIdentity, the WebIdentity token. Specifies the WebIdentity token to use when assuming an IAM role using web identity federation. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN --in-download-dir Run the provided command in the download directory. When enabled, Terragrunt will execute the provided command in the temporary download directory where modules are cached, rather than the current working directory. This is particularly useful when you need to: Inspect downloaded module contents. Execute commands against the actual module source code. Debug issues with downloaded modules. Example: # View the contents of main.tf in the downloaded moduleterragrunt exec --in-download-dir -- cat main.tf # List all files in the downloaded moduleterragrunt exec --in-download-dir -- ls -la Type: bool Environment Variables: TG_IN_DOWNLOAD_DIR --inputs-debug Write debug.tfvars to working folder to help root-cause issues. When enabled, Terragrunt will write a debug.tfvars file to the working directory. This file contains the resolved input values and can be useful for debugging configuration issues. To learn more about how to use this flag, see the Debugging guide. Type: bool Environment Variables: TG_INPUTS_DEBUG Previous run Next bootstrap",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.886Z"
  },
  {
    "title": "bootstrap",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/backend/bootstrap/",
    "content": "bootstrap Usage Bootstrap OpenTofu/Terraform backend infrastructure. Examples Provision backend resources defined in remote_state. terragrunt backend bootstrap Provision remote_state Using this command bootstraps the resources described in your remote_state block. If any of the resources described in the remote_state block need provisioning, bootstrap will provision them. If they are present, but configured in a way that differs from remote_state configuration, Terragrunt will attempt to update them when it is safe to do so. For example, if you have the following remote_state block: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" encrypt = true dynamodb_table = \"tf-lock\" accesslogging_bucket_name = \"mybucket-logs\" }} Then run the following: terragrunt backend bootstrap You’ll ensure the availability of: An S3 bucket named mybucket in the us-east-1 region with the following enabled: Server Side Encryption (SSE) Versioning TLS Enforcement A DynamoDB table named tf-lock in the us-east-1 region with SSE. An S3 bucket named mybucket-logs configured as the access log destination for the mybucket bucket. The bootstrap command is idempotent. If the resources already exist, bootstrap will not provision them again. --backend-bootstrap The flag --backend-bootstrap is equivalent to explicitly running the bootstrap command.Using it in conjunction with any run command will result in any required bootstrapping to be performed prior to initiating the run. Flags --all When this flag is set, Terragrunt will bootstrap all units discovered in the current working directory. Type: bool Environment Variables: TG_ALL --config Path to the Terragrunt configuration file to use when bootstrapping the resources. Type: string Environment Variables: TG_CONFIG --download-dir Path to download OpenTofu/Terraform modules into. The default is `.terragrunt-cache`. Type: string Environment Variables: TG_DOWNLOAD_DIR Previous exec Next migrate",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.920Z"
  },
  {
    "title": "migrate",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/backend/migrate/",
    "content": "migrate Usage Migrate OpenTofu/Terraform state from one unit to another. Examples Migrate backend state from `unit` to `unit-renamed`. backend migrate old-unit-name new-unit-nameForce state migration, even if the bucket doesn't have versioning enabled. backend migrate --force old-unit-name new-unit-name This command will migrate the OpenTofu/Terraform state backend from one unit to another. You will typically want to use this command if you are using a key attribute for your remote_state block that uses the path_relative_to_include function, and you want to rename the unit. For example, given the following filesystem structure: Directoryold-unit-name terragrunt.hclroot.hcl remote_state { backend = \"s3\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} You couldn’t simply rename the old-unit-name directory to new-unit-name and run terragrunt apply in new-unit-name, because the change in the evaluated value for path_relative_to_include() would result in a new state key for the new-unit-name unit. Instead, you can use the backend migrate command to migrate the backend state from the old-unit-name unit to the new-unit-name unit. cp -R old-unit-name new-unit-nameterragrunt backend migrate old-unit-name new-unit-namerm -rf old-unit-name This will migrate the backend state from the old-unit-name unit to the new-unit-name unit, and then delete the old-unit-name unit. Terragrunt performs migrations in one of two ways, depending on the level of support for the backends being migrated, and the state of configuration between the two units. If the backend source for both the source and destination units are the same (both are S3 or GCS), Terragrunt will use the AWS/GCP SDK to move state between the two units transparently without interacting with OpenTofu/Terraform. This is the preferred method, when possible. If either backend source isn’t supported by Terragrunt, or the state of configuration between the two units is different, Terragrunt will instead use the OpenTofu/Terraform CLI to move the state between the two units. This is the fallback method, and will generally be slower. Terragrunt also won’t be able to delete the existing state from the source unit in this case, so you’ll need to handle that yourself. Flags --config Path to the Terragrunt configuration file to use to migrate the resources. Note that this path is relative to the directory of each of the source and destination units, not the current working directory. Type: string Environment Variables: TG_CONFIG --download-dir Path to download OpenTofu/Terraform modules into. The default is `.terragrunt-cache`. Note that this path is relative to the directory of each of the source and destination units, not the current working directory. Type: string Environment Variables: TG_DOWNLOAD_DIR --force When this flag is set, Terragrunt will force the migration of the backend state, even if the bucket containing it has versioning disabled. Danger This flag is dangerous and should be used with caution.Gruntwork recommends always enabling versioning on your backend state resources. Migrating backend state without versioning enabled can result in irreversible data loss. Type: bool Environment Variables: TG_FORCE Previous bootstrap Next delete",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.960Z"
  },
  {
    "title": "delete",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/backend/delete/",
    "content": "delete Usage Delete backend state used by a unit. Examples Delete backend state for the current unit. terragrunt backend deleteDelete backend state for the current unit without confirmation. terragrunt backend delete --non-interactiveDelete backend state for the current unit, even if it doesn't have versioning enabled. terragrunt backend delete --force Delete State Using this command deletes the backend state file for the current Terragrunt unit. For example, given the following remote_state block: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" encrypt = true dynamodb_table = \"tf-lock\" accesslogging_bucket_name = \"mybucket-logs\" }} Running terragrunt backend delete will delete the backend state file located at path/to/my/key in the mybucket bucket. Flags --all When this flag is set Terragrunt will delete the backend state for all units discovered in the current working directory. Type: bool Environment Variables: TG_ALL --config Path to the Terragrunt configuration file to use to delete the resources. Type: string Environment Variables: TG_CONFIG --download-dir Path to download OpenTofu/Terraform modules into. The default is `.terragrunt-cache`. Type: string Environment Variables: TG_DOWNLOAD_DIR --force When this flag is set, Terragrunt will delete the backend state regardless of whether the bucket containing it has versioning enabled. Danger This flag is dangerous and should be used with caution.Gruntwork recommends always enabling versioning on your backend state resources. Deleting backend state without versioning enabled can result in irreversible data loss. Type: bool Environment Variables: TG_FORCE Previous migrate Next generate",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:58.997Z"
  },
  {
    "title": "generate",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/stack/generate/",
    "content": "generate Usage Generate a stack of units based on configurations in a terragrunt.stack.hcl file. Examples Generate a stack of units using the configurations in a terragrunt.stack.hcl file. terragrunt stack generate Generating a stack unit \"mother\" { source = \"units/chicken\" path = \"mother\"} unit \"father\" { source = \"./units/chicken\" path = \"father\"} unit \"chick_1\" { source = \"./units/chick\" path = \"chicks/chick-1\"} unit \"chick_2\" { source = \"units/chick\" path = \"chicks/chick-2\"} Running the following: terragrunt stack generate Generates the following stack: terragrunt.stack.hclDirectory.terragrunt-stack Directorymother terragrunt.hclDirectoryfather terragrunt.hclDirectorychicks Directorychick-1 terragrunt.hclDirectorychick-2 terragrunt.hcl Note Parallel Execution: Stack generation runs concurrently to improve performance. The number of parallel tasks is determined by the GOMAXPROCS environment variable and can be explicitly controlled using the --parallelism flag:terragrunt stack generate --parallelism 4Automatic Discovery: The command automatically discovers all terragrunt.stack.hcl files within the directory structure and generates them in parallel.Validation of Units and Stacks: During the stack generation, the system will validate that each unit and stack’s target directory contains the appropriate configuration file (terragrunt.hcl for units and terragrunt.stack.hcl for stacks). This ensures the directories are correctly structured before proceeding with the stack generation. To skip this validation, you can use the --no-stack-validate flag:terragrunt stack generate --no-stack-validate Caution Path Restrictions: If an absolute path is provided as an argument, generate will throw an error. Only relative paths within the working directory are supported. Caution Recreating the stack directory and cleaning stale files: By default, terragrunt stack generate does not delete files in .terragrunt-stack that are no longer produced by the current terragrunt.stack.hcl. If you removed or changed values or units and still see old files (e.g., a lingering terragrunt.values.hcl), regenerate from a clean state by either refreshing sources or cleaning the directory first:# Refresh sources and regenerate from a clean copyterragrunt stack generate --source-update # Or explicitly remove the generated directory firstterragrunt stack clean && terragrunt stack generateYou can also pass --source-update when running commands via stack run:terragrunt stack run plan --source-update Previous delete Next run",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.026Z"
  },
  {
    "title": "run",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/stack/run/",
    "content": "run Usage Run OpenTofu/Terraform commands against a stack of units. Examples Run a plan on each unit. terragrunt stack run planApply changes for each unit. terragrunt stack run applyDestroy all units. terragrunt stack run destroy The stack run * command allows users to execute IaC commands across all units defined in a terragrunt.stack.hcl file. This feature facilitates efficient orchestration of operations on multiple units, simplifying workflows for managing complex infrastructure stacks. Automatic stack generation Before executing the specified command, the terragrunt stack run * command will automatically generate the stack by creating the .terragrunt-stack directory using the terragrunt.stack.hcl configuration file. This ensures that all units are up-to-date before running the requested operation. Note Refreshing sources and cleaning stale files: If you change or remove units/values and find stale files remain in .terragrunt-stack, pass --source-update to refresh sources before generation, or clean the directory first:terragrunt stack run plan --source-update # orterragrunt stack clean && terragrunt stack run plan Flags --no-stack-generate Disable automatic stack regeneration before running the stack commands. When enabled, Terragrunt will skip automatic stack regeneration before executing the command. This is useful when you want to run operations using the existing .terragrunt-stack directory without regenerating it, improving execution speed and avoiding unnecessary changes. Type: bool Environment Variables: TG_NO_STACK_GENERATE Previous generate Next output",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.059Z"
  },
  {
    "title": "clean",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/stack/clean/",
    "content": "clean Usage Running `terragrunt stack clean` removes the `.terragrunt-stack` directory, which is generated by the `terragrunt stack generate` or `terragrunt stack run` commands.This can be useful when you need to remove generated configurations or troubleshoot issues. Examples Remove all auto-generated `.terragrunt-stack` directories created by `stack` commands. terragrunt stack clean Previous output Next catalog",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.100Z"
  },
  {
    "title": "output",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/stack/output/",
    "content": "output Usage The `terragrunt stack output` command allows users to retrieve and interact with outputs from multiple units within a Terragrunt stack.This feature simplifies handling infrastructure outputs by consolidating them into a single view. Examples Get outputs from a stack of units. terragrunt stack outputGet outputs from a stack of units in JSON format. terragrunt stack output --format jsonGet an output from a stack of units in raw format. terragrunt stack output --format raw app.id Executing terragrunt stack output in a stack directory produces an aggregated output from all units within the stack: $ terragrunt stack outputservice.output1 = \"output1\"service.output2 = \"output2\"db.output1 = \"output1\"db.output2 = \"output2\" Indexing outputs To retrieve outputs for a specific unit, specify the unit name: $ terragrunt stack output project1_app1project1_app1 = { complex = { delta = 0.02 id = 2 name = \"name1\" timestamp = \"2025-02-07T21:05:51Z\" } complex_list = [{ delta = 0.02 id = 10 name = \"name1\" timestamp = \"2025-02-07T21:05:51Z\" }, { delta = 0.03 id = 20 name = \"name10\" timestamp = \"2025-02-07T21:05:51Z\" }] custom_value1 = \"value1\" data = \"app1\" list = [\"1\", \"2\", \"3\"]} You can also retrieve a specific output from a unit: $ terragrunt stack output project1_app1.custom_value1project1_app1.custom_value1 = \"value1\" Output formats Terragrunt provides multiple output formats for easier parsing and integration with other tools. The desired format can be specified using the --format CLI flag. FormatDescriptiondefaultFormat output as HCL.jsonFormat output as JSON. This can be useful for integrations with other tools.rawFormat output as a simple raw string. Useful for integration into bash scripts. To retrieve outputs in structured JSON format: $ terragrunt stack output --format json project1_app2{ \"project1_app2\": { \"complex\": { \"delta\": 0.02, \"id\": 2, \"name\": \"name2\", \"timestamp\": \"2025-02-07T21:05:51Z\" }, \"complex_list\": [ { \"delta\": 0.02, \"id\": 2, \"name\": \"name2\", \"timestamp\": \"2025-02-07T21:05:51Z\" }, { \"delta\": 0.03, \"id\": 2, \"name\": \"name3\", \"timestamp\": \"2025-02-07T21:05:51Z\" } ], \"custom_value2\": \"value2\", \"data\": \"app2\", \"list\": [ \"a\", \"b\", \"c\" ] }} json format Accessing a specific list inside JSON format: $ terragrunt stack output --format json project1_app2.complex_list{ \"project1_app2.complex_list\": [ { \"delta\": 0.02, \"id\": 2, \"name\": \"name2\", \"timestamp\": \"2025-02-07T21:05:51Z\" }, { \"delta\": 0.03, \"id\": 2, \"name\": \"name3\", \"timestamp\": \"2025-02-07T21:05:51Z\" } ]} raw format The raw format returns outputs as plain values without additional structure. When accessing lists or structured outputs, indexes are required to extract values. Retrieving a simple value: $ terragrunt stack output --format raw project1_app2.dataapp2 Flags --format Format stack output. (json, raw). Specifies the output format for stack outputs. Available formats are: default - Format output as HCL (default) json - Format output as JSON for machine readability raw - Format output as raw string for shell script integration Example: # JSON formatterragrunt stack output --format json # Raw formatterragrunt stack output --format raw project1_app1.custom_value1 Type: string Environment Variables: TG_FORMAT --json Format stack output as JSON. Alias for --format json. A convenience flag that formats the stack output as JSON. This is equivalent to using --format json. Example: terragrunt stack output --json Type: bool Environment Variables: TG_JSON --raw Format stack output as raw string. Alias for --format raw. A convenience flag that formats the stack output as a raw string. This is equivalent to using --format raw. This is particularly useful when you need to use the output in shell scripts or other automation. Example: # Store the output in a variableAPP_ID=$(terragrunt stack output --raw app.id) Caution When using --stack-output-raw, you must use index-based access. Attempting to access nested structures directly may result in unexpected output or errors. Type: bool Environment Variables: TG_RAW --no-stack-generate Disable automatic stack regeneration before running the stack commands. When enabled, Terragrunt will skip automatic stack regeneration before executing the command. This is useful when you want to run operations using the existing .terragrunt-stack directory without regenerating it, improving execution speed and avoiding unnecessary changes. Type: bool Environment Variables: TG_NO_STACK_GENERATE Previous run Next clean",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.139Z"
  },
  {
    "title": "catalog",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/catalog/",
    "content": "catalog Usage Launch a Terminal User Interface (TUI) to browse and use OpenTofu/Terraform modules. Examples Start up a catalog using the configurations discovered in a parent Terragrunt configuration. terragrunt catalogExplicitly indicate the name of the root configuration being discovered. terragrunt catalog --root-file-name root.hcl terragrunt catalog [repo-url] [options] For more information on how the catalog works, see the dedicated catalog documentation. Flags --no-include-root Do not include the root configuration file in any generated terragrunt.hcl file during scaffolding. When enabled, Terragrunt will not automatically include the root configuration file in generated terragrunt.hcl files during scaffolding operations in the catalog. This is useful when you want more control over which configurations are included in newly scaffolded modules. Examples: terragrunt catalog --no-include-root Type: bool Environment Variables: TG_NO_INCLUDE_ROOT --root-file-name The name of the root configuration file to include in any generated terragrunt.hcl during scaffolding. Specifies the name of the root configuration file that should be: Included in generated terragrunt.hcl files during scaffolding Used when searching for Catalog URLs This is particularly useful when your root configuration uses a different naming convention than the default. Example: terragrunt catalog --root-file-name root.hcl Type: string Environment Variables: TG_ROOT_FILE_NAME Previous clean Next scaffold",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.179Z"
  },
  {
    "title": "scaffold",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/scaffold/",
    "content": "scaffold Usage Generate Terragrunt configuration files from a catalog. Examples Scaffold a standard MySQL database module as a new unit. terragrunt scaffold github.com/gruntwork-io/terragrunt-infrastructure-modules-example//modules/mysql terragrunt scaffold <MODULE_URL> [TEMPLATE_URL] [--var] [--var-file] [--no-include-root] [--root-file-name] For more information on how scaffolding works, see the dedicated scaffold documentation. Flags --no-include-root Disable inclusion of the root module in the generated `terragrunt.hcl` file (equivalent to using `--var=EnableRootInclude=false`, and will be overridden if the corresponding `var` value is set). When enabled, Terragrunt will not automatically include the root configuration file in generated terragrunt.hcl files during scaffolding operations. This is equivalent to setting --var=EnableRootInclude=false and will be overridden if that variable is explicitly set. Example: terragrunt scaffold github.com/org/repo//modules/mysql --no-include-root Type: bool Environment Variables: TG_NO_INCLUDE_ROOT --root-file-name Set the name of the root configuration file to include in the generated `terragrunt.hcl` file. Specifies the name of the root configuration file to include in generated terragrunt.hcl files during scaffolding. This is equivalent to using --var=RootFileName=<name>, and will be overridden if the corresponding var value is set. Example: terragrunt scaffold github.com/org/repo//modules/mysql --root-file-name root.hcl Type: string Environment Variables: TG_ROOT_FILE_NAME --var Variable to use for the boilerplate template. Sets variables to be used in the boilerplate template during scaffolding. You can specify multiple variables by using the flag multiple times. Example: terragrunt scaffold github.com/org/repo//modules/mysql \\ --var=\"environment=prod\" \\ --var=\"region=us-east-1\" Type: string Environment Variables: TG_VAR --var-file Variable file to use for the boilerplate template. Specifies a file containing variables to be used in the boilerplate template during scaffolding. The file should contain variable definitions in HCL format. Example: # vars.hclenvironment = \"prod\"region = \"us-east-1\" # Commandterragrunt scaffold github.com/org/repo//modules/mysql --var-file=vars.hcl Type: string Environment Variables: TG_VAR_FILE Previous catalog Next find",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.217Z"
  },
  {
    "title": "find",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/find/",
    "content": "find Usage The `find` command helps you discover Terragrunt configurations in your codebase.It recursively searches for `terragrunt.hcl` and `terragrunt.stack.hcl` files and displays them in formatted output. Examples Find all configurations (units and stacks) in the current directory. terragrunt findFind all configurations in a different directory. terragrunt find --working-dir /path/to/working/dirDisable color output. terragrunt find --no-colorFind all configurations in the current directory and emit them as a JSON string. terragrunt find --format 'json'Find all configurations and output them in JSON format (alias for --format=json). terragrunt find --jsonSort configurations based on their dependencies using DAG mode. terragrunt find --dagSort configurations based on dependency graph as if running plan command. $ terragrunt find --queue-construct-as=planstacks/live/devstacks/live/produnits/live/dev/vpcunits/live/prod/vpcunits/live/dev/dbunits/live/prod/dbunits/live/dev/ec2units/live/prod/ec2Sort configurations based on dependency graph as if running destroy command. $ terragrunt find --queue-construct-as=destroystacks/live/devstacks/live/produnits/live/dev/ec2units/live/prod/ec2units/live/dev/dbunits/live/prod/dbunits/live/dev/vpcunits/live/prod/vpcInclude dependency information in the output. terragrunt find --dependencies --format 'json'Include exclude configuration in the output. terragrunt find --exclude --format 'json'Include external dependencies in the output. terragrunt find --dependencies --external --format 'json'Include the list of files read by each component in the output. terragrunt find --reading --format 'json' Color Output When used without any flags, all units and stacks discovered in the current working directory are displayed in colorful text format. Color Coding Discovered configurations are color coded to help you identify them at a glance: Units are displayed in blue Stacks are displayed in green Output Formats The find command supports two output formats: Text Format (Default) The default text format displays each configuration on a new line, with color coding for different types. JSON Format You can output the results in JSON format using either: terragrunt find --format=json or the shorter alias: terragrunt find --json The JSON output includes additional metadata about each configuration, such as its type (unit or stack) and path. DAG Mode The find command supports DAG mode to sort output based on dependencies using the --dag flag. When using DAG mode, configurations with no dependencies appear first, followed by configurations that depend on them, maintaining the correct dependency order: terragrunt find --dagunitA # no dependenciesunitB # no dependenciesunitC # depends on unitAunitD # depends on unitC If multiple configurations share common dependencies, they will be sorted in lexical order. Queue Construct As The find command supports the --queue-construct-as flag (or its shorter alias --as) to sort output based on the dependency graph, as if a particular command was run. For example, when using the plan command: terragrunt find --queue-construct-as=planstacks/live/devstacks/live/produnits/live/dev/vpcunits/live/prod/vpcunits/live/dev/dbunits/live/prod/dbunits/live/dev/ec2units/live/prod/ec2 This will sort the output based on the dependency graph, as if the plan command was run. All dependent units will appear after the units they depend on. When using the destroy command: terragrunt find --as=destroystacks/live/devstacks/live/produnits/live/dev/ec2units/live/prod/ec2units/live/dev/dbunits/live/prod/dbunits/live/dev/vpcunits/live/prod/vpc This will sort the output based on the dependency graph, as if the destroy command was run. All dependent units will appear before the units they depend on. Note: The --queue-construct-as flag implies the --dag flag. Dependencies You can include dependency information in the output using the --dependencies flag. When enabled, the JSON output will include the dependency relationships between configurations: terragrunt find --dependencies --format=json[ { \"type\": \"unit\", \"path\": \"unitA\", \"dependencies\": [] }, { \"type\": \"unit\", \"path\": \"unitB\", \"dependencies\": [\"../unitA\", \"../../external/unitC\"] }] Exclude Configuration You can include exclude configuration in the output using the --exclude flag. When enabled, the JSON output will include the configurations of the exclude block in the discovered units: terragrunt find --exclude --format=json[ { \"type\": \"unit\", \"path\": \"action/exclude-apply\", \"exclude\": { \"exclude_dependencies\": true, \"actions\": [ \"apply\" ], \"if\": true } }] You can combine this with the --queue-construct-as flag to dry-run behavior relevant to excludes: terragrunt find --exclude --queue-construct-as=plan --format=json find will remove any units that would match the exclude configuration. Reading Files You can include information about which files are read by each component using the --reading flag. This is particularly useful for understanding configuration dependencies and tracking which shared files are consumed by your Terragrunt configurations. When enabled, the JSON output will include a reading field that lists all files that were read during the parsing of each component: $ terragrunt find --reading --format=json[ { \"type\": \"unit\", \"path\": \"app\", \"reading\": [ \"path/to/shared.hcl\", \"path/to/shared.tfvars\", ] }] This includes files read by Terragrunt helper functions such as: read_terragrunt_config() - Reading other Terragrunt configuration files read_tfvars_file() - Reading Terraform variable files sops_decrypt_file() - Reading encrypted files via SOPS mark_as_read() - Explicitly marking a file as read External Dependencies By default, external dependencies (those outside the working directory) are not part of the overall results (although, they will be mentioned in the dependency section of the JSON output). Use the --external flag to include them as top-level results: terragrunt find --dependencies --external --format=json[ { \"type\": \"unit\", \"path\": \"internal/unitA\", \"dependencies\": [] }, { \"type\": \"unit\", \"path\": \"internal/unitB\", \"dependencies\": [\"../unitA\", \"../../external/unitC\"] }, { \"type\": \"unit\", \"path\": \"external/unitC\", \"dependencies\": [] }] Hidden Configurations By default, hidden directories (those starting with .) are excluded from the search. Use the --hidden flag to include them: terragrunt find --hidden Disabling Color Output You can disable color output by using the global --no-color flag: terragrunt find --no-color When stdout is redirected, color output is disabled automatically to prevent undesired interference with other tools. Working Directory You can change the working directory for find by using the global --working-dir flag: terragrunt find --working-dir=/path/to/working/dir Filtering Results Experimental Feature This feature is currently experimental and not yet complete. See the filter-flag experiment documentation for details on what is and isn’t supported.Usage of the filter flag requires usage of the filter-flag experiment, like so:terragrunt find --experiment filter-flag --filter 'foo'Examples below will omit the --experiment filter-flag flag for brevity. The find command supports the --filter flag to target specific configurations using a flexible query language. This is particularly useful for discovering configurations that match specific criteria before running operations on them. Basic Filtering Examples # Filter by name using glob patternsterragrunt find --filter 'app*' # Filter by pathterragrunt find --filter './prod/**' # Filter by typeterragrunt find --filter 'type=unit' # Combine filters with intersectionterragrunt find --filter './prod/** | type=unit' Advanced Filtering The filter syntax supports negation, multiple filters, and complex queries: # Exclude specific configurationsterragrunt find --experiment filter-flag --filter '!./test/**' # Multiple filters (OR logic)terragrunt find --experiment filter-flag --filter 'app1' --filter 'app2' # Complex queries with chainingterragrunt find --experiment filter-flag --filter './dev/** | type=unit | !name=unit1' Learn More About Filtering For comprehensive examples and advanced usage patterns, see the Filters feature documentation. Flags --format Format the results as specified. Supported values (text, json). Default: text. This is particularly useful when you need to process the results programmatically or integrate with other tools. Example: $ terragrunt find --format=json | jq '.[:3]'[ { \"type\": \"stack\", \"path\": \"basic\" }, { \"type\": \"unit\", \"path\": \"basic/units/chick\" }, { \"type\": \"unit\", \"path\": \"basic/units/chicken\" }] Type: string Environment Variables: TG_FORMAT --json Output results in JSON format. This is equivalent to using `--format=json`. This flag is a convenient shorthand for --format=json. It’s particularly useful when you need to process the results programmatically or integrate with other tools. Example: $ terragrunt find --json | jq '.[:3]'[ { \"type\": \"stack\", \"path\": \"basic\" }, { \"type\": \"unit\", \"path\": \"basic/units/chick\" }, { \"type\": \"unit\", \"path\": \"basic/units/chicken\" }] Equivalent Flags The --json flag is equivalent to --format=json. You can use either one to get the same JSON output. Type: bool Environment Variables: TG_JSON --dag Output in DAG mode. Outputs configurations in DAG mode, which sorts configurations by dependency order by relationship in the dependency graph. By default, configurations are sorted alphabetically: $ terragrunt findlive/dev/dblive/dev/ec2live/dev/vpclive/prod/dblive/prod/ec2live/prod/vpc When the --dag flag is used, configurations are sorted by dependency order (dependencies before their dependents): $ terragrunt find --daglive/dev/vpclive/prod/vpclive/dev/dblive/prod/dblive/dev/ec2live/prod/ec2 When not used in the JSON format: $ terragrunt find --json --dependencies[ { \"type\": \"unit\", \"path\": \"live/dev/db\", \"dependencies\": [ \"live/dev/vpc\" ] }, { \"type\": \"unit\", \"path\": \"live/dev/ec2\", \"dependencies\": [ \"live/dev/vpc\", \"live/dev/db\" ] }, { \"type\": \"unit\", \"path\": \"live/dev/vpc\" }, { \"type\": \"unit\", \"path\": \"live/prod/db\", \"dependencies\": [ \"live/prod/vpc\" ] }, { \"type\": \"unit\", \"path\": \"live/prod/ec2\", \"dependencies\": [ \"live/prod/vpc\", \"live/prod/db\" ] }, { \"type\": \"unit\", \"path\": \"live/prod/vpc\" }] Results are sorted by path. When combined with the JSON format: $ terragrunt find --json --dependencies --dag[ { \"type\": \"unit\", \"path\": \"live/dev/vpc\" }, { \"type\": \"unit\", \"path\": \"live/prod/vpc\" }, { \"type\": \"unit\", \"path\": \"live/dev/db\", \"dependencies\": [ \"live/dev/vpc\" ] }, { \"type\": \"unit\", \"path\": \"live/prod/db\", \"dependencies\": [ \"live/prod/vpc\" ] }, { \"type\": \"unit\", \"path\": \"live/dev/ec2\", \"dependencies\": [ \"live/dev/vpc\", \"live/dev/db\" ] }, { \"type\": \"unit\", \"path\": \"live/prod/ec2\", \"dependencies\": [ \"live/prod/vpc\", \"live/prod/db\" ] }] Type: boolean Environment Variables: TG_DAG --hidden Find configurations in hidden directories. Example: terragrunt find --hidden Type: bool Environment Variables: TG_HIDDEN --dependencies Include dependency information in the output. When enabled, the output will include information about dependencies between configurations. This is particularly useful when combined with JSON output format to understand the dependency relationships in your codebase. Example: terragrunt find --dependencies --format json[ { \"type\": \"unit\", \"path\": \"unitA\", \"dependencies\": [] }, { \"type\": \"unit\", \"path\": \"unitB\", \"dependencies\": [\"../unitA\"] }] Type: bool Environment Variables: TG_DEPENDENCIES --exclude Include exclude configuration in the output Include exclude configuration in the output. When enabled, the JSON output will include the configurations of the exclude block in the discovered units. Usage --exclude Examples Show exclude configurations in JSON format: terragrunt find --exclude --format=json Show exclude configurations with queue construct simulation: terragrunt find --exclude --queue-construct-as=plan --format=json Behavior When enabled, the JSON output will include any exclude block configurations found in the units: $ terragrunt find --exclude --format=json | jq[ { \"type\": \"unit\", \"path\": \"action/exclude-apply\", \"exclude\": { \"exclude_dependencies\": true, \"actions\": [ \"apply\" ], \"if\": true } }, { \"type\": \"unit\", \"path\": \"action/exclude-plan\", \"exclude\": { \"exclude_dependencies\": true, \"actions\": [ \"plan\" ], \"if\": true } }, { \"type\": \"unit\", \"path\": \"all-except-output/app1\", \"exclude\": { \"exclude_dependencies\": true, \"actions\": [ \"all_except_output\" ], \"if\": true } }] Note that you can combine this with the --queue-construct-as flag to dry-run behavior relevant to excludes. $ terragrunt find --exclude --queue-construct-as=plan --format=json | jq[ { \"type\": \"unit\", \"path\": \"action/exclude-apply\", \"exclude\": { \"exclude_dependencies\": true, \"actions\": [ \"apply\" ], \"if\": true } }] Type: boolean Environment Variables: TG_EXCLUDE --include Include `include` configuration in the output. When enabled, JSON output will include the configurations of the include block of discovered units. $ terragrunt find --include --format=json | jq[ { \"type\": \"unit\", \"path\": \"bar\", \"include\": { \"cloud\": \"cloud.hcl\" } }, { \"type\": \"unit\", \"path\": \"foo\" }] You can use tools like jq to filter the output and get all the units that include a specific configuration. $ terragrunt find --include --format=json | jq '[.[] | select(.include.cloud == \"cloud.hcl\")]'[ { \"type\": \"unit\", \"path\": \"bar\", \"include\": { \"cloud\": \"cloud.hcl\" } }] Type: boolean Environment Variables: TG_INCLUDE --reading Include the list of files that are read by components in the output. When enabled, JSON output will include a list of files that were read during the parsing of each component. This is useful for understanding configuration dependencies and tracking which shared files are consumed by your Terragrunt configurations. $ terragrunt find --reading --format=json | jq[ { \"type\": \"unit\", \"path\": \"app\", \"reading\": [ \"shared.hcl\", \"shared.tfvars\", \"common/variables.hcl\" ] }] The reading field includes files read by Terragrunt helper functions such as: read_terragrunt_config() - Reading other Terragrunt configuration files read_tfvars_file() - Reading Terraform variable files sops_decrypt_file() - Reading encrypted files via SOPS mark_as_read() - Explicitly marking files as read You can use tools like jq to analyze which components read specific files: $ terragrunt find --reading --format=json | jq '[.[] | select(.reading[]? | contains(\"shared.hcl\"))]'[ { \"type\": \"unit\", \"path\": \"app\", \"reading\": [ \"shared.hcl\", \"shared.tfvars\" ] }] Type: boolean Environment Variables: TG_READING --external Include external dependencies in the output. When enabled, units outside the working directory can be included as part of the output, if any unit depends on them. This is useful when you need to understand all dependency relationships, including those that don’t exist in the current directory. Example: terragrunt find --dependencies --external --format json[ { \"type\": \"unit\", \"path\": \"internal/unitA\", \"dependencies\": [] }, { \"type\": \"unit\", \"path\": \"internal/unitB\", \"dependencies\": [\"../unitA\", \"../../external/unitC\"] }, { \"type\": \"unit\", \"path\": \"external/unitC\", \"dependencies\": [] }] Type: bool Environment Variables: TG_EXTERNAL --queue-construct-as Sort output based on the dependency graph, as if a particular command was run. This flag implies the `--dag` flag. The flag has a shorter alias `--as`. Sort output based on the dependency graph, as if a particular command was run. This flag implies the --dag flag. The flag has a shorter alias --as. Usage --queue-construct-as=COMMAND--as=COMMAND Where COMMAND is the command to simulate (e.g., plan, destroy). Examples Sort output as if running plan command (dependencies after dependents): terragrunt find --queue-construct-as=plan Sort output as if running destroy command (dependencies before dependents): terragrunt find --queue-construct-as=destroy Behavior When using plan command, all dependent units will appear after the units they depend on. When using destroy command, all dependent units will appear before the units they depend on. Note: This flag implies the --dag flag. Type: string Aliases: --as Environment Variables: TG_QUEUE_CONSTRUCT_AS --filter Filter configurations using a flexible query language Experimental Feature This feature is currently experimental and not yet complete. See the filter-flag experiment documentation for details on what is and isn’t supported.Usage of the filter flag requires usage of the filter-flag experiment, like so:terragrunt find --experiment filter-flag --filter 'foo'Examples below will omit the --experiment filter-flag flag for brevity.Currently, the --filter flag is only available in the find and list commands. Support for the run command is planned for a future release. The --filter flag provides a sophisticated querying syntax for targeting specific units and stacks in Terragrunt commands. Usage terragrunt find --filter 'app*'terragrunt list --filter './prod/** | type=unit' Name-Based Filtering Match configurations by their name using exact matches or glob patterns: # Exact matchterragrunt find --filter app1 # Glob patternterragrunt find --filter 'app*' Path-Based Filtering Match configurations by their file system path: # Relative paths with globsterragrunt find --filter './envs/prod/**' # Absolute pathsterragrunt find --filter '/absolute/path/to/envs/dev/apps/*' Attribute-Based Filtering Match configurations by their configuration attributes: # Filter by typeterragrunt find --filter 'type=unit'terragrunt find --filter 'type=stack' # Filter by external dependency statusterragrunt find --filter 'external=false' # Filter by files readterragrunt find --filter 'reading=shared.hcl'terragrunt find --filter 'reading=common/*.hcl' # Globs supported!terragrunt find --filter 'reading=config/**' # Double-wildcard globs are required filtering on files nested in subdirectories.terragrunt find --filter 'reading=config/vars.tfvars' What does 'Reading' mean? The reading attribute filters configurations based on which files they themselves read. This is useful for finding all infrastructure that could potentially be impacted by shared configuration, and is particularly useful when a commonly included configuration (like root.hcl) is updated.See the mark_as_read function documentation for more information on how Terragrunt tracks files that are read during parsing. Negation Exclude configurations using the ! prefix: # Exclude by nameterragrunt find --filter '!app1' # Exclude by pathterragrunt find --filter '!./prod/**' Intersection (Refinement) Use the | operator to refine results: # Find all units in prod directoryterragrunt find --filter './prod/** | type=unit' # Chain multiple filtersterragrunt find --filter './dev/** | type=unit | !name=unit1' Union (Multiple Filters) Specify multiple --filter flags to combine results using OR logic: # Find components named 'unit1' OR 'stack1'terragrunt find --filter unit1 --filter stack1 Supported Commands Currently supported in: find list Planned for future releases: run hcl fmt hcl validate Learn More For comprehensive examples and advanced usage patterns, see the Filters feature documentation. Type: list(string) Previous scaffold Next list",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.280Z"
  },
  {
    "title": "list",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/list/",
    "content": "list Usage The list command helps you discover and display Terragrunt configurations in your codebase. It provides various output formats and options to help you understand the structure and dependencies of your Terragrunt configurations. Examples List all units in a typical multi-environment infrastructure setup. $ terragrunt listlive/dev/db live/dev/ec2 live/dev/vpclive/prod/db live/prod/ec2 live/prod/vpcList all units in long format, showing unit types and paths. $ terragrunt list -lType Pathunit live/dev/dbunit live/dev/ec2unit live/dev/vpcunit live/prod/dbunit live/prod/ec2unit live/prod/vpcList all units in tree format to visualize the infrastructure hierarchy. $ terragrunt list -T.╰── live ├── dev │ ├── db │ ├── ec2 │ ╰── vpc ╰── prod ├── db ├── ec2 ╰── vpcList all units with their dependencies to understand infrastructure relationships. $ terragrunt list -l --dependenciesType Path Dependenciesunit live/dev/db live/dev/vpcunit live/dev/ec2 live/dev/db, live/dev/vpcunit live/dev/vpcunit live/prod/db live/prod/vpcunit live/prod/ec2 live/prod/db, live/prod/vpcunit live/prod/vpcList all units in dependency order (DAG) to understand deployment sequence. $ terragrunt list -l --dag --dependenciesType Path Dependenciesunit b-dependencyunit a-dependent b-dependencyList all units in dependency order as if running plan command. $ terragrunt list --queue-construct-as=planstacks/live/dev stacks/live/prod units/live/dev/vpcunits/live/prod/vpc units/live/dev/db units/live/prod/dbunits/live/dev/ec2 units/live/prod/ec2List all units in dependency order as if running destroy command. $ terragrunt list --queue-construct-as=destroystacks/live/dev stacks/live/prod units/live/dev/ec2units/live/prod/ec2 units/live/dev/db units/live/prod/dbunits/live/dev/vpc units/live/prod/vpc Output Formats The list command supports multiple output formats to help you visualize your Terragrunt configurations in different ways: Text Format (Default) The default text format provides a simple, space-separated list of configurations. It will display all configurations that fit in the width of your terminal. When configurations exceed the width of your terminal, it will wrap to the next line. Long Format The long format provides additional details about each configuration, including its type: Tree Format The tree format provides a hierarchical view of your configurations: By default, configurations in tree format are displayed ordered by name and grouped by directory: .╰── live ├── dev │ ├── db │ ├── ec2 │ ╰── vpc ╰── prod ├── db ├── ec2 ╰── vpc DAG Mode The list command supports DAG mode to sort and group output based on dependencies using the --dag flag. When using DAG mode, configurations with no dependencies appear first, followed by configurations that depend on them, maintaining the correct dependency order. For example, in default text format: # Default alphabetical order$ terragrunt lista-dependent b-dependency # DAG mode order$ terragrunt list --dagb-dependency a-dependent When using --dag with the tree format, configurations are sorted by dependency order and grouped by relationship in the dependency graph: $ terragrunt list --tree --dag.├── live/dev/vpc│ ├── live/dev/db│ │ ╰── live/dev/ec2│ ╰── live/dev/ec2╰── live/prod/vpc ├── live/prod/db │ ╰── live/prod/ec2 ╰── live/prod/ec2 Queue Construct As The list command supports the --queue-construct-as flag (or its shorter alias --as) to sort output based on the dependency graph, as if a particular command was run. For example, when using the plan command: $ terragrunt list --queue-construct-as=planstacks/live/dev stacks/live/prod units/live/dev/vpcunits/live/prod/vpc units/live/dev/db units/live/prod/dbunits/live/dev/ec2 units/live/prod/ec2 This will sort the output based on the dependency graph, as if the plan command was run. All dependent units will appear after the units they depend on. When using the destroy command: $ terragrunt list --queue-construct-as=destroystacks/live/dev stacks/live/prod units/live/dev/ec2units/live/prod/ec2 units/live/dev/db units/live/prod/dbunits/live/dev/vpc units/live/prod/vpc This will sort the output based on the dependency graph, as if the destroy command was run. All dependent units will appear before the units they depend on. Note: The --queue-construct-as flag implies the --dag flag. Dependencies and Discovery Dependencies Include dependency information in the output using the --dependencies flag. When combined with different grouping options, this provides powerful ways to visualize your infrastructure’s dependency structure. External Dependencies Use the --external flag to discover and include dependencies that exist outside your current working directory. This is particularly useful when working with shared modules or cross-repository dependencies. Hidden Configurations By default, Terragrunt excludes configurations in hidden directories (those starting with a dot). Use the --hidden flag to include these configurations in the output. Working Directory You can change the working directory for list by using the global --working-dir flag: terragrunt list --working-dir=/path/to/working/dir Color Output When used without any flags, all units and stacks discovered in the current working directory are displayed in colorful text format. Color Coding Discovered configurations are color coded to help you identify them at a glance: Units are displayed in blue Stacks are displayed in green You can disable color output by using the global --no-color flag. Filtering Results Experimental Feature This feature is currently experimental and not yet complete. See the filter-flag experiment documentation for details on what is and isn’t supported.Usage of the filter flag requires usage of the filter-flag experiment, like so:terragrunt list --experiment filter-flag --filter 'foo' The list command supports the --filter flag to target specific configurations using a flexible query language. This is particularly useful for listing configurations that match specific criteria before running operations on them. Basic Filtering Examples # Filter by name using glob patternsterragrunt list --experiment filter-flag --filter 'app*' # Filter by pathterragrunt list --experiment filter-flag --filter './prod/**' # Filter by typeterragrunt list --experiment filter-flag --filter 'type=unit' # Combine filters with intersectionterragrunt list --experiment filter-flag --filter './prod/** | type=unit' Advanced Filtering The filter syntax supports negation, multiple filters, and complex queries: # Exclude specific configurationsterragrunt list --experiment filter-flag --filter '!./test/**' # Multiple filters (OR logic)terragrunt list --experiment filter-flag --filter 'app1' --filter 'app2' # Complex queries with chainingterragrunt list --experiment filter-flag --filter './dev/** | type=unit | !name=unit1' Learn More About Filtering For comprehensive examples and advanced usage patterns, see the Filters feature documentation. Flags --format Format the results as specified. Supported values (text, long, tree). Default: text. Controls how the list results are displayed: text (default): Simple space-separated list of configurations. Best for quick overview and scripting. long: Detailed view showing type (unit/stack), path, and module information. Useful for auditing and documentation. tree: Hierarchical view showing directory structure. Perfect for understanding infrastructure organization. These values all have shortcuts as standalone flags: --long / -l for long --tree / -T for tree Examples: # Default text format - Great for quick overview$ terragrunt listlive/dev/db live/dev/ec2 live/dev/vpclive/prod/db live/prod/ec2 live/prod/vpc # Long format - Useful for reading structured information quickly$ terragrunt list -lType Path Dependenciesunit live/dev/db live/dev/vpcunit live/dev/ec2 live/dev/db, live/dev/vpcunit live/dev/vpcunit live/prod/db live/prod/vpcunit live/prod/ec2 live/prod/db, live/prod/vpcunit live/prod/vpc # Tree format - Optimal for visualizing structure$ terragrunt list -T.╰── live ├── dev │ ├── db │ ├── ec2 │ ╰── vpc ╰── prod ├── db ├── ec2 ╰── vpc The examples above demonstrate a typical multi-environment infrastructure setup with networking, compute, and data layers. Each format provides a different perspective on the same infrastructure, making it easier to understand and manage your Terragrunt configurations. Type: string Environment Variables: TG_FORMAT --hidden Include hidden directories in list results. Controls whether hidden directories (those starting with a dot) are included in the list results. By default, Terragrunt excludes hidden directories from the list output to reduce noise and focus on your primary infrastructure components. However, there may be cases where you want to see all configurations, including those in hidden directories. Example: $ terragrunt list -lType Pathstack stackunit unit $ terragrunt list -l --hiddenType Pathunit .hide/unitstack stackunit unit Type: boolean Environment Variables: TG_HIDDEN --dependencies Include dependencies in list results. Controls whether dependency information is included in the list output. When enabled, Terragrunt will analyze and display the dependency relationships between configurations. This flag is particularly powerful when combined with different output formats and sorting options: With --format=long: Shows dependencies in a tabular format With --format=json: Includes full dependency ancestry in JSON output Tip Combine --dependencies with --sort=dag and --group-by=dag (or just use --dag) to get a complete picture of your infrastructure’s deployment sequence. Type: boolean Environment Variables: TG_DEPENDENCIES --external Discover external dependencies from initial results. Controls whether Terragrunt should discover and include external dependencies in the list results. External dependencies are Terragrunt configurations that are referenced by your configurations but exist outside the current working directory. This flag is most useful when: Investigating the complete dependency graph of your infrastructure Determining the full blast radius of a change Example: $ terragrunt list -l --dependenciesType Path Dependenciesunit a-dependent b-dependencyunit b-dependency $ terragrunt list -l --dependencies --externalType Path Dependenciesunit ../external/c-dependencyunit a-dependent ../external/c-dependency, b-dependencyunit b-dependency Note The --external flag is typically used in combination with --dependencies to show why the external dependency was discovered. Type: boolean Environment Variables: TG_EXTERNAL --tree Output in tree format. A shorthand flag that sets --format=tree. Displays the list results in a hierarchical tree structure that shows the relationships between your Terragrunt configurations. Type: boolean Aliases: -T Environment Variables: TG_TREE --long Output in long format. A shorthand flag that sets --format=long. Displays the list results in a detailed tabular format that includes additional information about each configuration. Type: boolean Aliases: -l Environment Variables: TG_LONG --dag Output in DAG mode. Outputs configurations in DAG mode, which sorts configurations by dependency order and groups them by relationship in the dependency graph. Examples: By default, configurations are sorted alphabetically: $ terragrunt listlive/dev/db live/dev/ec2 live/dev/vpclive/prod/db live/prod/ec2 live/prod/vpc When the --dag flag is used, configurations are sorted by dependency order (dependencies before their dependents): $ terragrunt list --daglive/dev/vpc live/prod/vpc live/dev/dblive/prod/db live/dev/ec2 live/prod/ec2 When not used in the long format: $ terragrunt list -l --dependenciesType Path Dependenciesunit live/dev/db live/dev/vpcunit live/dev/ec2 live/dev/db, live/dev/vpcunit live/dev/vpcunit live/prod/db live/prod/vpcunit live/prod/ec2 live/prod/db, live/prod/vpcunit live/prod/vpc Results are sorted by name. When combined with the long format: $ terragrunt list -l --dependencies --dagType Path Dependenciesunit live/dev/vpcunit live/prod/vpcunit live/dev/db live/dev/vpcunit live/prod/db live/prod/vpcunit live/dev/ec2 live/dev/db, live/dev/vpcunit live/prod/ec2 live/prod/db, live/prod/vpc When not used in the tree format: $ terragrunt list -T.╰── live ├── dev │ ├── db │ ├── ec2 │ ╰── vpc ╰── prod ├── db ├── ec2 ╰── vpc When combined with the tree format: $ terragrunt list -T --dag.├── live/dev/vpc│ ├── live/dev/db│ │ ╰── live/dev/ec2│ ╰── live/dev/ec2╰── live/prod/vpc ├── live/prod/db │ ╰── live/prod/ec2 ╰── live/prod/ec2 Type: boolean Environment Variables: TG_DAG --queue-construct-as Sort output based on the dependency graph, as if a particular command was run. This flag implies the `--dag` flag. The flag has a shorter alias `--as`. Sort output based on the dependency graph, as if a particular command was run. This flag implies the --dag flag. The flag has a shorter alias --as. Usage --queue-construct-as=COMMAND--as=COMMAND Where COMMAND is the command to simulate (e.g., plan, destroy). Examples Sort output as if running plan command (dependencies after dependents): terragrunt find --queue-construct-as=plan Sort output as if running destroy command (dependencies before dependents): terragrunt find --queue-construct-as=destroy Behavior When using plan command, all dependent units will appear after the units they depend on. When using destroy command, all dependent units will appear before the units they depend on. Note: This flag implies the --dag flag. Type: string Aliases: --as Environment Variables: TG_QUEUE_CONSTRUCT_AS --filter Filter configurations using a flexible query language Experimental Feature This feature is currently experimental and not yet complete. See the filter-flag experiment documentation for details on what is and isn’t supported.Usage of the filter flag requires usage of the filter-flag experiment, like so:terragrunt find --experiment filter-flag --filter 'foo'Examples below will omit the --experiment filter-flag flag for brevity.Currently, the --filter flag is only available in the find and list commands. Support for the run command is planned for a future release. The --filter flag provides a sophisticated querying syntax for targeting specific units and stacks in Terragrunt commands. Usage terragrunt find --filter 'app*'terragrunt list --filter './prod/** | type=unit' Name-Based Filtering Match configurations by their name using exact matches or glob patterns: # Exact matchterragrunt find --filter app1 # Glob patternterragrunt find --filter 'app*' Path-Based Filtering Match configurations by their file system path: # Relative paths with globsterragrunt find --filter './envs/prod/**' # Absolute pathsterragrunt find --filter '/absolute/path/to/envs/dev/apps/*' Attribute-Based Filtering Match configurations by their configuration attributes: # Filter by typeterragrunt find --filter 'type=unit'terragrunt find --filter 'type=stack' # Filter by external dependency statusterragrunt find --filter 'external=false' # Filter by files readterragrunt find --filter 'reading=shared.hcl'terragrunt find --filter 'reading=common/*.hcl' # Globs supported!terragrunt find --filter 'reading=config/**' # Double-wildcard globs are required filtering on files nested in subdirectories.terragrunt find --filter 'reading=config/vars.tfvars' What does 'Reading' mean? The reading attribute filters configurations based on which files they themselves read. This is useful for finding all infrastructure that could potentially be impacted by shared configuration, and is particularly useful when a commonly included configuration (like root.hcl) is updated.See the mark_as_read function documentation for more information on how Terragrunt tracks files that are read during parsing. Negation Exclude configurations using the ! prefix: # Exclude by nameterragrunt find --filter '!app1' # Exclude by pathterragrunt find --filter '!./prod/**' Intersection (Refinement) Use the | operator to refine results: # Find all units in prod directoryterragrunt find --filter './prod/** | type=unit' # Chain multiple filtersterragrunt find --filter './dev/** | type=unit | !name=unit1' Union (Multiple Filters) Specify multiple --filter flags to combine results using OR logic: # Find components named 'unit1' OR 'stack1'terragrunt find --filter unit1 --filter stack1 Supported Commands Currently supported in: find list Planned for future releases: run hcl fmt hcl validate Learn More For comprehensive examples and advanced usage patterns, see the Filters feature documentation. Type: list(string) Previous find Next fmt",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.331Z"
  },
  {
    "title": "fmt",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/hcl/fmt/",
    "content": "fmt Usage Recursively find HashiCorp Configuration Language (HCL) files and rewrite them into a canonical format. Examples Recursively format all HCL files in the current directory. terragrunt hcl fmt Flags --check Enable check mode in the hclfmt command. When enabled, Terragrunt will check if HCL files are properly formatted without making any changes. This is useful in CI/CD pipelines to ensure consistent formatting. The command will: Exit with status code 0 if files are properly formatted Exit with status code 1 if any files need formatting Example: terragrunt hcl fmt --check Type: bool Environment Variables: TG_CHECK --diff Print diff between original and modified file versions when running with 'hclfmt'. When enabled, Terragrunt will show the differences between the original and formatted versions of HCL files. This helps you understand what changes the formatter would make. Example: terragrunt hcl fmt --diff Type: bool Environment Variables: TG_DIFF --exclude-dir Skip HCL formatting in given directories. Specifies directories to exclude from HCL formatting. This is useful when you want to skip formatting certain parts of your codebase. Example: terragrunt hcl fmt --exclude-dir=vendor --exclude-dir=.terragrunt-cache Type: string Environment Variables: TG_EXCLUDE_DIR --file The path to a single hcl file that the hclfmt command should run on. Specifies a single HCL file to format instead of recursively searching for files. This is useful when you want to format just one specific file. Example: terragrunt hcl fmt --file=./environments/prod/terragrunt.hcl Type: string Environment Variables: TG_FILE --stdin Format HCL from stdin and print result to stdout. When enabled, Terragrunt will read HCL content from standard input, format it, and write the result to standard output. This is useful for integrating with text editors or other tools. Example: echo 'locals { foo=\"bar\" }' | terragrunt hcl fmt --stdin Type: bool Environment Variables: TG_STDIN --queue-exclude-dir Unix-style glob of directories to exclude from the queue of Units to run. Specifies directories to exclude when running commands with all. This flag can be specified multiple times to exclude multiple directories. When using the TG_QUEUE_EXCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note When using --queue-exclude-dir, only the specified units are excluded, not their dependencies.Dependencies of excluded units will still be run unless they are also explicitly excluded. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_EXCLUDE_DIR --queue-exclude-external Exclude external dependencies from the queue of Units to run. When enabled, Terragrunt will exclude external dependencies (dependencies outside the current working directory) from the queue when running commands with all. Note that an external dependency is a dependency that is outside the current Terragrunt working directory and is not within any directories specified by queue-include-dir. This flag is useful when you want to limit the scope of execution to only units within your current working directory structure. Type: bool Environment Variables: TG_QUEUE_EXCLUDE_EXTERNAL --queue-excludes-file Path to a file with a list of directories that need to be excluded when running `run --all` commands. Units in these directories will be excluded during execution of the commands. If a relative path is specified, it should be relative from —working-dir. This will only exclude the module, not its dependencies. This flag has been designed to integrate nicely with the hcl validate command, which can return a list of invalid files delimited by newlines when passed the --show-config-path flag. To integrate the two, you can run something like the following using bash process substitution: terragrunt run --all plan --queue-excludes-file <(terragrunt hcl validate --show-config-path || true) Type: string Default: .terragrunt-excludes Environment Variables: TG_QUEUE_EXCLUDES_FILE --queue-ignore-dag-order Ignore DAG order for --all commands. When enabled, Terragrunt will ignore the dependency order when running commands with --all. This means units will be executed in arbitrary order, which can be useful for read-only operations like plan. Note that this can lead to errors if used with commands that modify state, as dependencies might be processed in the wrong order. To learn more about how to use this flag, see the Stacks feature documentation. Type: bool Environment Variables: TG_QUEUE_IGNORE_DAG_ORDER --queue-ignore-errors Continue processing Units even if a dependency fails. When enabled, Terragrunt will continue processing remaining units even if one fails. This can be useful when you want to see all potential errors in your configuration, rather than stopping at the first failure. Note that this may lead to incomplete or inconsistent states if used with commands that modify infrastructure. To learn more about how to use this flag, see the Stacks feature documentation. Danger Using --queue-ignore-errors can lead to inconsistent state if dependencies fail but dependent modules continue running.Use this flag with caution, especially with apply or destroy commands.For more fine-grained error handling, see the errors block in the HCL reference. Type: bool Environment Variables: TG_QUEUE_IGNORE_ERRORS --queue-include-dir Unix-style glob of directories to include in the queue of Units to run. Specifies directories to include when running commands with --all. This flag can be specified multiple times to include multiple directories. When using the TG_QUEUE_INCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_INCLUDE_DIR --queue-include-external Include external dependencies in the queue of Units to run. When enabled, Terragrunt will include external dependencies (dependencies discovered outside the current working directory) in the queue when running commands with --all or --graph. Type: bool Environment Variables: TG_QUEUE_INCLUDE_EXTERNAL --queue-include-units-reading If flag is set, 'run --all' will only run the command against Terragrunt units that read the specified file via an HCL function or include. This flag works very similarly to the --queue-units-that-include flag, but instead of looking only for included configurations, it also looks for configurations that read a given file. When passed in, the --all command will include all units (modules) that read a given file into the queue. This is useful when you want to trigger an update on all units that read or include a given file using HCL functions in their configurations. Consider the following folder structure: Directoryreading-shared-hcl terragrunt.hclDirectoryalso-reading-shared-hcl terragrunt.hclDirectorynot-reading-shared-hcl terragrunt.hclshared.hcl Suppose that reading-shared-hcl and also-reading-shared-hcl both read shared.hcl in their configurations, like so: locals { shared = read_terragrunt_config(find_in_parent_folders(\"shared.hcl\"))} If you run the command run --all init --queue-include-units-reading shared.hcl from the root folder, both reading-shared-hcl and also-reading-shared-hcl will be run; not not-reading-shared-hcl. This is because the read_terragrunt_config HCL function has a special hook that allows Terragrunt to track that it has read the file shared.hcl. This hook is used by all native HCL functions that Terragrunt supports which read files. Note, however, that there are certain scenarios where Terragrunt may not be able to track that a file has been read this way. For example, you may be using a bash script to read a file via run_cmd, or reading the file via OpenTofu code. To support these use-cases, the mark_as_read function can be used to manually mark a file as read. That would look something like this: locals { filename = mark_as_read(\"file-read-by-tofu.txt\")} inputs = { filename = local.filename} Caution Due to how Terragrunt parses configurations during run --all, functions will only properly mark files as read if they are used in the locals block.Reading a file directly in the inputs block will not mark the file as read, as the inputs block is not evaluated until after the queue has already been populated. Type: string Environment Variables: TG_QUEUE_INCLUDE_UNITS_READING --queue-strict-include Only process the directories matched by --queue-include-dir. When enabled, Terragrunt will only process directories that match the patterns specified by --queue-include-dir. For example, with the following directory structure: Directoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectorystaging Directoryapp terragrunt.hclDirectorydb terragrunt.hcl Running terragrunt run --all plan --queue-include-dir \"prod/*\" would process all directories, but the --all flag includes by default when no excludes are provided, so the stage stack would also be included by default. Running terragrunt run --all plan --queue-include-dir \"prod/*\" --queue-strict-include tells Terragrunt to exclude by default, so it only include units prod/app and prod/db. Danger When using --queue-strict-include, all dependencies of included directories will be excluded if they are not explicitly included. This can lead to unexpected behavior if you’re not careful with your include patterns.Always verify the modules that will be included before running destructive commands. Type: bool Environment Variables: TG_QUEUE_STRICT_INCLUDE Previous list Next validate",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.373Z"
  },
  {
    "title": "validate",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/hcl/validate/",
    "content": "validate Usage Recursively find HashiCorp Configuration Language (HCL) files and validate them. Examples Discover all HCL files in the current directory, and validate them. terragrunt hcl validate Flags --json Output the result in JSON format. When enabled, Terragrunt will output the validation results in JSON format, making it easier to parse and process programmatically. Example: terragrunt hcl validate --json Type: bool Environment Variables: TG_JSON --show-config-path Show a list of files with invalid configuration. When enabled, Terragrunt will display the full paths of files that contain invalid HCL configurations. This is particularly useful when validating multiple files to quickly identify which files need attention. Example: terragrunt hcl validate --show-config-path Type: bool Environment Variables: TG_SHOW_CONFIG_PATH --inputs Validate that all variables are set by the module a unit provisions. When enabled, Terragrunt will validate that all variables are set by the module a unit provisions. Example: terragrunt hcl validate --inputs Type: bool Environment Variables: TG_INPUTS --strict Throw an error if any inputs are set that are not defined in the module that a unit provisions. When enabled, Terragrunt will throw an error if any inputs are set that are not defined in the module that a unit provisions. Example: terragrunt hcl validate --inputs --strict Type: bool Environment Variables: TG_STRICT --queue-exclude-dir Unix-style glob of directories to exclude from the queue of Units to run. Specifies directories to exclude when running commands with all. This flag can be specified multiple times to exclude multiple directories. When using the TG_QUEUE_EXCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note When using --queue-exclude-dir, only the specified units are excluded, not their dependencies.Dependencies of excluded units will still be run unless they are also explicitly excluded. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_EXCLUDE_DIR --queue-exclude-external Exclude external dependencies from the queue of Units to run. When enabled, Terragrunt will exclude external dependencies (dependencies outside the current working directory) from the queue when running commands with all. Note that an external dependency is a dependency that is outside the current Terragrunt working directory and is not within any directories specified by queue-include-dir. This flag is useful when you want to limit the scope of execution to only units within your current working directory structure. Type: bool Environment Variables: TG_QUEUE_EXCLUDE_EXTERNAL --queue-excludes-file Path to a file with a list of directories that need to be excluded when running `run --all` commands. Units in these directories will be excluded during execution of the commands. If a relative path is specified, it should be relative from —working-dir. This will only exclude the module, not its dependencies. This flag has been designed to integrate nicely with the hcl validate command, which can return a list of invalid files delimited by newlines when passed the --show-config-path flag. To integrate the two, you can run something like the following using bash process substitution: terragrunt run --all plan --queue-excludes-file <(terragrunt hcl validate --show-config-path || true) Type: string Default: .terragrunt-excludes Environment Variables: TG_QUEUE_EXCLUDES_FILE --queue-ignore-dag-order Ignore DAG order for --all commands. When enabled, Terragrunt will ignore the dependency order when running commands with --all. This means units will be executed in arbitrary order, which can be useful for read-only operations like plan. Note that this can lead to errors if used with commands that modify state, as dependencies might be processed in the wrong order. To learn more about how to use this flag, see the Stacks feature documentation. Type: bool Environment Variables: TG_QUEUE_IGNORE_DAG_ORDER --queue-ignore-errors Continue processing Units even if a dependency fails. When enabled, Terragrunt will continue processing remaining units even if one fails. This can be useful when you want to see all potential errors in your configuration, rather than stopping at the first failure. Note that this may lead to incomplete or inconsistent states if used with commands that modify infrastructure. To learn more about how to use this flag, see the Stacks feature documentation. Danger Using --queue-ignore-errors can lead to inconsistent state if dependencies fail but dependent modules continue running.Use this flag with caution, especially with apply or destroy commands.For more fine-grained error handling, see the errors block in the HCL reference. Type: bool Environment Variables: TG_QUEUE_IGNORE_ERRORS --queue-include-dir Unix-style glob of directories to include in the queue of Units to run. Specifies directories to include when running commands with --all. This flag can be specified multiple times to include multiple directories. When using the TG_QUEUE_INCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_INCLUDE_DIR --queue-include-external Include external dependencies in the queue of Units to run. When enabled, Terragrunt will include external dependencies (dependencies discovered outside the current working directory) in the queue when running commands with --all or --graph. Type: bool Environment Variables: TG_QUEUE_INCLUDE_EXTERNAL --queue-include-units-reading If flag is set, 'run --all' will only run the command against Terragrunt units that read the specified file via an HCL function or include. This flag works very similarly to the --queue-units-that-include flag, but instead of looking only for included configurations, it also looks for configurations that read a given file. When passed in, the --all command will include all units (modules) that read a given file into the queue. This is useful when you want to trigger an update on all units that read or include a given file using HCL functions in their configurations. Consider the following folder structure: Directoryreading-shared-hcl terragrunt.hclDirectoryalso-reading-shared-hcl terragrunt.hclDirectorynot-reading-shared-hcl terragrunt.hclshared.hcl Suppose that reading-shared-hcl and also-reading-shared-hcl both read shared.hcl in their configurations, like so: locals { shared = read_terragrunt_config(find_in_parent_folders(\"shared.hcl\"))} If you run the command run --all init --queue-include-units-reading shared.hcl from the root folder, both reading-shared-hcl and also-reading-shared-hcl will be run; not not-reading-shared-hcl. This is because the read_terragrunt_config HCL function has a special hook that allows Terragrunt to track that it has read the file shared.hcl. This hook is used by all native HCL functions that Terragrunt supports which read files. Note, however, that there are certain scenarios where Terragrunt may not be able to track that a file has been read this way. For example, you may be using a bash script to read a file via run_cmd, or reading the file via OpenTofu code. To support these use-cases, the mark_as_read function can be used to manually mark a file as read. That would look something like this: locals { filename = mark_as_read(\"file-read-by-tofu.txt\")} inputs = { filename = local.filename} Caution Due to how Terragrunt parses configurations during run --all, functions will only properly mark files as read if they are used in the locals block.Reading a file directly in the inputs block will not mark the file as read, as the inputs block is not evaluated until after the queue has already been populated. Type: string Environment Variables: TG_QUEUE_INCLUDE_UNITS_READING --queue-strict-include Only process the directories matched by --queue-include-dir. When enabled, Terragrunt will only process directories that match the patterns specified by --queue-include-dir. For example, with the following directory structure: Directoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectorystaging Directoryapp terragrunt.hclDirectorydb terragrunt.hcl Running terragrunt run --all plan --queue-include-dir \"prod/*\" would process all directories, but the --all flag includes by default when no excludes are provided, so the stage stack would also be included by default. Running terragrunt run --all plan --queue-include-dir \"prod/*\" --queue-strict-include tells Terragrunt to exclude by default, so it only include units prod/app and prod/db. Danger When using --queue-strict-include, all dependencies of included directories will be excluded if they are not explicitly included. This can lead to unexpected behavior if you’re not careful with your include patterns.Always verify the modules that will be included before running destructive commands. Type: bool Environment Variables: TG_QUEUE_STRICT_INCLUDE Previous fmt Next graph",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.411Z"
  },
  {
    "title": "graph",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/dag/graph/",
    "content": "graph Usage Print a visual representation of the Terragrunt dependency graph in DOT language format.This command analyzes your Terragrunt configuration and outputs a directed acyclic graph (DAG) showing the relationships and dependencies between your Terraform modules. Examples Graph all dependencies in the graph as a DotViz graph. $ terragrunt dag graphdigraph { \"alb\" ; \"ecs\" ; \"ecs\" -> \"alb\";}Graph all dependencies in visual diagram. $ terragrunt dag graph | dot -Tpng > graph.png Previous validate Next render",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.457Z"
  },
  {
    "title": "render",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/render/",
    "content": "render Usage Generate a simplified version of the Terragrunt configuration with all includes and dependencies resolved. Examples Render the configurations for the current unit in JSON format. terragrunt render --format=json Render the Terragrunt configuration in the current working directory, with as much work done as possible beforehand (that is, with all includes merged, dependencies resolved/interpolated, function calls executed, etc). The only supported format at the moment is JSON, but support for HCL will be added in a future version. Example: The following terragrunt.hcl: locals { aws_region = \"us-east-1\"} inputs = { aws_region = local.aws_region} Renders to the following HCL by default: $ terragrunt renderlocals { aws_region = \"us-east-1\"}inputs = { aws_region = \"us-east-1\"} Note the resolution of the aws_region local, making it easier to read the final evaluated configuration at a glance. Renders to the following JSON when the --format json flag is used: $ terragrunt render --format json{ \"locals\": { \"aws_region\": \"us-east-1\" }, \"inputs\": { \"aws_region\": \"us-east-1\" } // NOTE: other attributes are omitted for brevity} You can also use the --write flag to write the rendered configuration to a canonically named file in the same working directory as the terragrunt.hcl file. Example: # Note the use of the `--json` shortcut flag.terragrunt render --json --write This will write the rendered configuration to terragrunt.rendered.json in the current working directory. This can be useful when rendering many configurations in a given directory, and you want to keep the rendered configurations in the same directory as the original configurations, without leveraging external tools or scripts. This is also useful when combined with the --all flag, which will render all configurations discovered from the current working directory. # Note the use of the `-w` alias for the `--write` flag.terragrunt render --all --json -w This will render all configurations discovered from the current working directory and write the rendered configurations to terragrunt.rendered.json files adjacent to the configurations they are derived from. Flags --format Use the specified format for the rendered configuration. Supported values (hcl, json). default hcl. Controls the output format of the rendered Terragrunt configuration. Supports: hcl - Output in HCL format (default) json - Output in JSON format The HCL format is human-readable and matches the original configuration style, while JSON format is useful for programmatic processing. Example: # Render as HCL (default)terragrunt render # Render as JSONterragrunt render --format json Type: string Environment Variables: TG_FORMAT --write Write the rendered configuration to a file (terragrunt.rendered.hcl or terragrunt.rendered.json by default). Example: terragrunt render --write --json This will write the rendered configuration to terragrunt.rendered.json in the current working directory. Type: boolean Aliases: w Environment Variables: TG_WRITE --all Render all configurations discovered from the current working directory. Example: terragrunt render --all --json This will render all configurations discovered from the current working directory and write the rendered configurations to terragrunt.rendered.json files adjacent to the configurations they are derived from. Combining this with the --write flag will write the rendered configurations to terragrunt.rendered.json files adjacent to the configurations they are derived from. terragrunt render -a --json --write Type: boolean Aliases: a Environment Variables: TG_ALL Previous graph Next print",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.487Z"
  },
  {
    "title": "print",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/info/print/",
    "content": "print Usage Outputs a JSON object with contextual information such as the configuration file path, current working directory, cache download location, IAM role (if any), the binary used to run Terraform or Tofu, and the command being executed. Examples Print out information about the current context of Terragrunt. $ terragrunt info print{ \"config_path\": \"/example/path/terragrunt.hcl\", \"download_dir\": \"/example/path/.terragrunt-cache\", \"iam_role\": \"\", \"terraform_binary\": \"tofu\", \"terraform_command\": \"print\", \"working_dir\": \"/example/path\"} Previous render Next Global Flags",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.518Z"
  },
  {
    "title": "Global Flags",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/global-flags/",
    "content": "Global Flags The Terragrunt CLI supports the following global flags: ExperimentSection titled “Experiment” --experiment Enables a specific experiment. For a list of available experiments, see the experiments documentation. Type: string Environment Variables: TG_EXPERIMENT Experiment ModeSection titled “Experiment Mode” --experiment-mode Enables experiment mode for Terragrunt. For more information, see the experiments documentation. Type: boolean Environment Variables: TG_EXPERIMENT_MODE Log Custom FormatSection titled “Log Custom Format” --log-custom-format Set the custom log formatting. For more information, see the log formatting documentation. Type: string Environment Variables: TG_LOG_CUSTOM_FORMAT Log DisableSection titled “Log Disable” --log-disable Disable logging. When enabled, Terragrunt will disable all logging output. This is useful when you want to see only the OpenTofu/Terraform output or when using Terragrunt in scripts where logging isn’t needed. Note that this automatically enables the --tf-forward-stdout flag to ensure OpenTofu/Terraform output is still visible. Type: bool Environment Variables: TG_LOG_DISABLE Log FormatSection titled “Log Format” --log-format Set the format for Terragrunt's log output. For a list of available formats and their descriptions, see the log formatting documentation. Type: string Environment Variables: TG_LOG_FORMAT Log LevelSection titled “Log Level” --log-level Sets the logging level for Terragrunt. Controls the verbosity of Terragrunt’s logging output. For more information about available log levels and their meanings, see the log levels documentation. Type: string Environment Variables: TG_LOG_LEVEL Show Absolute PathsSection titled “Show Absolute Paths” --log-show-abs-paths Show absolute paths in logs. When enabled, Terragrunt will show absolute paths in log messages instead of relative paths. For more information, see the log formatting documentation. Type: bool Environment Variables: TG_LOG_SHOW_ABS_PATHS No ColorSection titled “No Color” --no-color Disable color output for both Terragrunt and OpenTofu/Terraform. When enabled, Terragrunt will disable colored output in both its own logs and in OpenTofu/Terraform output. This is useful when running in environments where color codes might cause issues, such as CI/CD pipelines or when redirecting output to files. Type: bool Environment Variables: TG_NO_COLOR Non-InteractiveSection titled “Non-Interactive” --non-interactive Assume \"yes\" for all prompts. When enabled, Terragrunt will automatically answer “yes” to any prompts that would normally require user input. This is particularly useful in automated environments or CI/CD pipelines where user interaction isn’t possible. Caution When using --non-interactive, Terragrunt will automatically answer “yes” to all prompts except for external dependency inclusion prompts.These will default to “no” for safety. Use --queue-include-external to explicitly include external dependencies. Type: bool Environment Variables: TG_NON_INTERACTIVE Strict ControlSection titled “Strict Control” --strict-control Enables specific strict controls. Enables specific strict controls in Terragrunt. For a list of available controls and their descriptions, run terragrunt info strict. This flag provides finer-grained control compared to --strict-mode, allowing you to enable specific controls rather than all of them. For more information, see the strict controls documentation. Caution When accessing complex data structures using the raw format, you must use index-based access.Attempting to access nested structures directly may result in unexpected output or errors. Type: string Environment Variables: TG_STRICT_CONTROL Strict ModeSection titled “Strict Mode” --strict-mode Enables strict mode for Terragrunt. When enabled, Terragrunt will operate in strict mode, which opts in the user to all future breaking changes. For more information about what strict mode enables, run terragrunt info strict. For more information about strict mode, see the strict controls documentation. Type: bool Environment Variables: TG_STRICT_MODE Working DirectorySection titled “Working Directory” --working-dir The path where all commands should be run. Default is current directory. Specifies the working directory where Terragrunt should execute its commands. By default, Terragrunt uses the current directory. Example: terragrunt run plan --working-dir /path/to/infrastructure/prod Note The --working-dir flag behaves differently for run --all commands versus single unit commands.For run --all, Terragrunt will execute in all subdirectories of the working directory, while for run commands it will execute only in the specified directory. Type: string Environment Variables: TG_WORKING_DIR HelpSection titled “Help” --help Show help information. Displays help information about Terragrunt commands and their usage. Can be used with specific commands to get detailed help about that command. Type: bool VersionSection titled “Version” --version Show terragrunt version. Displays the current version of Terragrunt. Type: bool Edit page Previous print Next Rules",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.560Z"
  },
  {
    "title": "Rules",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/rules/",
    "content": "Rules These are the rules that Terragrunt maintainers endeavor to follow when working on the CLI. Whenever we break these rules, that is a bug and should be reported. The maintainers will either fix the behavior, or update the rules to reflect the reason for the discrepancy. In addition, if you find that a certain pattern that’s reliably followed in the CLI is not documented here, please let us know so we can update this document to encourage consistency. The final argument to a Terragrunt command will always be a verb. All the arguments preceding the final argument will usually be a noun. ExceptionThe exceptions to this rule are commands like terragrunt run, as these will frequently have two verbs in sequence (e.g. terragrunt run plan).This is an exception to the rule because it is exceptional behavior. The end of Terragrunt’s responsibility (from a CLI design perspective) is to process the run command, so what follows is not subject to the rules dictated here. All flags will usually start with a noun. ExceptionThe exception to this rule will be for negation as the flag will start with no/ non , as discussed below. If the flag is controlling a single configuration for Terragrunt, that configuration will be the name of the flag. Example--working-dir: Set the working directory configuration for Terragrunt. If a Terragrunt system can be controlled entirely by referencing the name of the system, or the flag can control high level behavior of Terragrunt on its own, that will be the name of the flag. Example--provider-cache: The system is the provider cache server. The Provider Cache Server be enabled if this flag is set.For brevity, prefer this to flags like --provider-cache-enable. If a configuration is being set for a system, another noun will follow the name of the system after a dash. The flag will accept a parameter that sets the configuration for that system. Example --log-level: The system is log, the level is the configuration. --provider-cache-dir: The system is the provider cache server. The directory is the configuration. If an operation will be performed on a system, a verb will follow. If necessary, a noun will follow the verb to indicate what the parameter for that flag corresponds to, or the setting it controls for the operation. Example--queue-include-unit - The system is the runner queue. The operation being performed to that system is include. The parameter of unit indicates that the parameter to the flag will be a unit being included in the queue. Behavior on the same systems will always share the same stem. ExampleAll flags that have to do with the Terragrunt Provider Cache Server will start with provider-cache.A user looking through the flags available in Terragrunt sorted in alphabetical order will find them right next to each other. All boolean flags will accept an optional parameter of true or false . true will usually correspond to the default behavior of the flag, and false will correspond to the inverse. ExceptionThe exception to this rule is when the default behavior of a flag changes.For example, the terragrunt-include-module-prefix flag was previously opt-in, but users were better served with the behavior being opt-out. To preserve backwards compatibility until the next release, the flag remained, but to use it, users had to set it via --terragrunt-include-module-prefix=false.In this scenario, whenever applicable, a different flag will be made available that does obey this rule (like tf-forward-stdout). When a flag prevents something from happening that Terragrunt would do by default, it will be proceeded by the prefix no/ non. Example--no-color has a default value of true, and setting it to false will make it so that the behavior of not setting the flag is active (Terragrunt will emit colorful output).The alternative would be to have a --color flag, and using that flag to disable color would require that they do something like --color=false.This would violate the rule that the default behavior of the flag wouldn’t change anything, as Terragrunt emits color by default. Commands and flags will always be backwards compatible until the next major release of Terragrunt. This includes instances where behavior violates one of the other rules listed here. If naming a command or flag following these rules would make it harder to understand or longer than it needs to be, the exception will be allowed and documented. Flags that specifically control the behavior of OpenTofu/Terraform will be prefixed tf. Example--tf-path controls the path to the OpenTofu/Terraform binary. Every flag will have at least one corresponding environment variable that is exactly the same text as the flag, but converted to SHOUTY_SNAKE_CASE instead of kebab-case, and prefixed with TG_. When more than one environment variable controls a flag, it will be to support backwards compatibility. Exampleiam-assume-role-duration —> TG_IAM_ASSUME_ROLE_DURATION Edit page Previous Global Flags Next Strict Controls",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.586Z"
  },
  {
    "title": "Strict Controls",
    "url": "https://terragrunt.gruntwork.io/docs/reference/strict-controls/",
    "content": "Strict Controls Terragrunt supports operating in a mode referred to as “Strict Mode”. Strict Mode is a set of controls that can be enabled to ensure that your Terragrunt usage is future-proof by making deprecated features throw errors instead of warnings. This can be useful when you want to ensure that your Terragrunt code is up-to-date with the latest conventions to avoid breaking changes in future versions of Terragrunt. Whenever possible, Terragrunt will initially provide you with a warning when you use a deprecated feature, without throwing an error. However, in Strict Mode, these warnings will be converted to errors, which will cause the Terragrunt command to fail. A good practice for using strict controls is to enable Strict Mode in your CI/CD pipelines for lower environments to catch any deprecated features early on. This allows you to fix them before they become a problem in production in a future Terragrunt release. If you are unsure about the impact of enabling strict controls, you can enable them for specific controls to gradually increase your confidence in the future compatibility of your Terragrunt usage. Controlling Strict ModeSection titled “Controlling Strict Mode” The simplest way to enable strict mode is to set the strict-mode flag. This will enable strict mode for all Terragrunt commands, for all strict mode controls. $ terragrunt run-all plan15:26:08.585 WARN The `run-all plan` command is deprecated and will be removed in a future version. Use `terragrunt run --all plan` instead. $ terragrunt --strict-mode run-all plan15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead. You can also use the environment variable, which can be more useful in CI/CD pipelines: $ TG_STRICT_MODE='true' terragrunt run-all plan15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead. Instead of enabling strict mode like this, you can also enable specific strict controls by setting the strict-control flag to a value that’s specific to a particular strict control. This can allow you to gradually increase your confidence in the future compatibility of your Terragrunt usage. $ terragrunt run-all plan --strict-control cli-redesign15:26:08.585 WARN The `run-all plan` command is deprecated and will be removed in a future version. Use `terragrunt run --all plan` instead. $ terragrunt run-all plan --strict-control cli-redesign15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead. Again, you can also use the environment variable, which might be more useful in CI/CD pipelines: $ TG_STRICT_CONTROL='cli-redesign' terragrunt run-all plan15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead. You can enable multiple strict controls at once: $ terragrunt run-all plan --strict-control cli-redesign --strict-control default-command15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead.15:26:46.521 ERROR Unable to determine underlying exit code, so Terragrunt will exit with error code 1 $ terragrunt run-all apply --strict-control cli-redesign --strict-control default-command15:26:46.564 ERROR The `run-all apply` command is no longer supported. Use `terragrunt run --all apply` instead.15:26:46.564 ERROR Unable to determine underlying exit code, so Terragrunt will exit with error code 1 You can also enable multiple strict controls at once when using the environment variable by using a comma-delimited list. $ TG_STRICT_CONTROL='cli-redesign,default-command' bash -c 'terragrunt run-all plan; terragrunt run-all apply'15:26:46.521 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead.15:26:46.521 ERROR Unable to determine underlying exit code, so Terragrunt will exit with error code 115:26:46.564 ERROR The `run-all apply` command is no longer supported. Use `terragrunt run --all apply` instead.15:26:46.564 ERROR Unable to determine underlying exit code, so Terragrunt will exit with error code 1 You can also use control categories to enable certain categories of strict controls. $ terragrunt run-all plan --strict-control deprecated-commands15:26:23.685 ERROR The `run-all plan` command is no longer supported. Use `terragrunt run --all plan` instead. Strict Mode ControlsSection titled “Strict Mode Controls” The following strict mode controls are available: skip-dependencies-inputsSection titled “skip-dependencies-inputs” Disable reading of dependency inputs to enhance dependency resolution performance by preventing recursively parsing Terragrunt inputs from dependencies. Tip Skipping dependency inputs is a performance optimization. For more details on performance optimizations, their tradeoffs, and other performance tips, read the dedicated Performance documentation. Reason: Enabling the skip-dependencies-inputs option prevents the recursive parsing of Terragrunt inputs, leading to optimized performance during dependency resolution. root-terragrunt-hclSection titled “root-terragrunt-hcl” Throw an error when users try to reference a root terragrunt.hcl file using find_in_parent_folders. This control will also try to find other scenarios where users may be using terragrunt.hcl as the root configuration, including when using commands like scaffold and catalog, which can generate a terragrunt.hcl file expecting a terragrunt.hcl file at the root of the project. Enabling this flag adjusts the defaults for those commands so that they expect a recommended root.hcl file by default, and will throw an error if a terragrunt.hcl file is explicitly set. Reason: Using a root terragrunt.hcl file was previously the recommended pattern to use with Terragrunt, but that is no longer the case. For more information see Migrating from root terragrunt.hcl. terragrunt-prefix-flagsSection titled “terragrunt-prefix-flags” Throw an error when using the --terragrunt- prefix for flags. Reason: This is no longer necessary, due to the work in RFC #3445. Example: The --terragrunt-non-interactive flag is deprecated and will be removed in a future version. Use --non-interactive instead. terragrunt-prefix-env-varsSection titled “terragrunt-prefix-env-vars” Throw an error when using the TERRAGRUNT_ prefix for environment variables. Reason: This prefix has been renamed to TG_ to shorten the prefix, due to the work in RFC #3445. Example: The TERRAGRUNT_LOG_LEVEL env var is deprecated and will be removed in a future version. Use TG_LOG_LEVEL=info instead. default-commandSection titled “default-command” Throw an error when using the Terragrunt default command. Reason: Terragrunt now supports a special run command that can be used to explicitly forward commands to OpenTofu/Terraform when no shortcut exists in the Terragrunt CLI. Example: The default command is deprecated and will be removed in a future version. Use terragrunt run instead. cli-redesignSection titled “cli-redesign” Throw an error when using commands that were deprecated as part of the CLI redesign. Commands: run-all graph graph-dependencies hclfmt hclvalidate output-module-groups render-json terragrunt-info validate-inputs Reason: These commands have been deprecated in favor of more consistent and intuitive commands as part of the CLI redesign. For more information, see the CLI Redesign Migration Guide. bare-includeSection titled “bare-include” Throw an error when using a bare include. Reason: Backwards compatibility for supporting bare includes results in a performance penalty for Terragrunt, and deprecating support provides a significant performance improvement. For more information, see the Bare Include Migration Guide. double-starSection titled “double-star” Opts in to a breaking change to ** and **/* behavior in queue-exclude-dir and queue-include-dir. Without the strict control enabled, to match units in all subdirectories a glob pattern must end in **/*. With the strict control enabled, the pattern **/* will match all subdirectories with a depth of at least one as indicated by the / character. The glob pattern ** will now match all subdirectories regardless of depth. With the following directory layout, glob behavior for * characters with the strict control enabled is: └── units ├── subdir1 │ └── subdir2 │ └── unit-c ├── unit-a └── unit-b The * wildcard indicates any character except / e.g. units/* matches unit-a and unit-b. The ** double wildcard indicates any character including /, this allows matching multiple directory depths e.g. units/** matches unit-a, unit-b, and unit-c. Control CategoriesSection titled “Control Categories” Certain strict controls are grouped into categories to make it easier to enable multiple strict controls at once. These categories change over time, so you might want to use the specific strict controls if you want to ensure that only certain controls are enabled. deprecated-commandsSection titled “deprecated-commands” Throw an error when using the deprecated commands. Controls: default-command cli-redesign Note: The individual *-all commands (plan-all, apply-all, destroy-all, output-all, validate-all, spin-up, tear-down) have been removed from Terragrunt and are no longer available as strict controls. Use terragrunt run --all for the modern equivalent. deprecated-flagsSection titled “deprecated-flags” Throw an error when using the deprecated flags. Controls: terragrunt-prefix-flags deprecated-env-varsSection titled “deprecated-env-vars” Throw an error when using the deprecated environment variables. Controls: terragrunt-prefix-env-vars deprecated-configsSection titled “deprecated-configs” Throw an error when using the deprecated Terragrunt configuration. Controls: skip-dependencies-inputs Completed ControlsSection titled “Completed Controls” The following strict controls have been completed and are no longer needed: legacy-all spin-up tear-down plan-all apply-all destroy-all output-all validate-all require-explicit-bootstrapSection titled “require-explicit-bootstrap” Status: Completed - Backend provisioning is no longer performed automatically by default. Terragrunt now requires explicit opt-in to bootstrap backend infrastructure. Use terragrunt backend bootstrap or pass --backend-bootstrap to a run command (e.g., terragrunt run apply --backend-bootstrap) to provision or update backend resources referenced by the remote_state block. This strict control is no longer necessary because the default behavior already requires explicit bootstrapping. legacy-allSection titled “legacy-all” Status: Completed - The legacy *-all commands have been removed from Terragrunt. This control was previously used to throw an error when using any of the legacy commands that were replaced by run-all. These commands have now been completely removed from Terragrunt as part of the deprecation schedule. Previously controlled commands (now removed): plan-all - Use terragrunt run --all plan instead apply-all - Use terragrunt run --all apply instead destroy-all - Use terragrunt run --all destroy instead output-all - Use terragrunt run --all output instead validate-all - Use terragrunt run --all validate instead spin-up - Use terragrunt run --all apply instead tear-down - Use terragrunt run --all destroy instead spin-upSection titled “spin-up” Status: Completed - This command has been completely removed from Terragrunt. Reason: The spin-up command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all apply instead. tear-downSection titled “tear-down” Status: Completed - This command has been completely removed from Terragrunt. Reason: The tear-down command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all destroy instead. plan-allSection titled “plan-all” Status: Completed - This command has been completely removed from Terragrunt. Reason: The plan-all command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all plan instead. apply-allSection titled “apply-all” Status: Completed - This command has been completely removed from Terragrunt. Reason: The apply-all command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all apply instead. destroy-allSection titled “destroy-all” Status: Completed - This command has been completely removed from Terragrunt. Reason: The destroy-all command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all destroy instead. output-allSection titled “output-all” Status: Completed - This command has been completely removed from Terragrunt. Reason: The output-all command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all output instead. validate-allSection titled “validate-all” Status: Completed - This command has been completely removed from Terragrunt. Reason: The validate-all command was deprecated and has now been removed as part of the deprecation schedule. Use terragrunt run --all validate instead. Edit page Previous Rules Next Experiments",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.621Z"
  },
  {
    "title": "Experiments",
    "url": "https://terragrunt.gruntwork.io/docs/reference/experiments/",
    "content": "Experiments Terragrunt supports operating in a mode referred to as “Experiment Mode”. Experiment Mode is a set of controls that can be enabled to opt in to experimental features before they’re stable. These features are subject to change and may be removed or altered at any time. They generally provide early access to new features or changes that are being considered for inclusion in future releases. Those experiments will be documented here so that you know the following: What the experiment is. What the experiment does. How to provide feedback on the experiment. What criteria must be met for the experiment to be considered stable. Sometimes, the criteria for an experiment to be considered stable is unknown, as there may not be a clear path to stabilization. In that case, this will be noted in the experiment documentation, and collaboration with the community will be encouraged to help determine the future of the experiment. Controlling Experiment ModeSection titled “Controlling Experiment Mode” The simplest way to enable experiment mode is to set the experiment-mode flag. This will enable experiment mode for all Terragrunt commands, for all experiments (note that this isn’t generally recommended, unless you are following Terragrunt development closely and are prepared for the possibility of breaking changes). terragrunt plan --experiment-mode You can also use the environment variable, which can be more useful in CI/CD pipelines: TG_EXPERIMENT_MODE='true' terragrunt plan Instead of enabling experiment mode, you can also enable specific experiments by setting the experiment flag to a value that’s specific to an experiment. This can allow you to experiment with a specific unstable feature that you think might be useful to you. terragrunt plan --experiment symlinks Again, you can also use the environment variable, which can be more useful in CI/CD pipelines: TG_EXPERIMENT='symlinks' terragrunt plan You can also enable multiple experiments at once. terragrunt --experiment symlinks plan Including the environment variable: TG_EXPERIMENT='symlinks,stacks' terragrunt plan Active ExperimentsSection titled “Active Experiments” The following experiments are available: symlinks cas filter-flag symlinksSection titled “symlinks” Support symlink resolution for Terragrunt units. symlinks - What it doesSection titled “symlinks - What it does” By default, Terragrunt will ignore symlinks when determining which units it should run. By enabling this experiment, Terragrunt will resolve symlinks and add them to the list of units being run. symlinks - How to provide feedbackSection titled “symlinks - How to provide feedback” Provide your feedback on the Experiment: Symlinks discussion. symlinks - Criteria for stabilizationSection titled “symlinks - Criteria for stabilization” To stabilize this feature, the following need to be resolved, at a minimum: Ensure that symlink support continues to work for users referencing symlinks in flags. See #3622. Add integration tests for all filesystem flags to confirm support with symlinks (or document the fact that they cannot be supported). Ensure that MacOS integration tests still work. See #3616. Add integration tests for MacOS in CI. casSection titled “cas” Support for Terragrunt Content Addressable Storage (CAS). cas - What it doesSection titled “cas - What it does” Allow Terragrunt to store and retrieve Git repositories from a Content Addressable Storage (CAS) system. The CAS is used to speed up both catalog cloning and OpenTofu/Terraform source cloning by avoiding redundant downloads of Git repositories. cas - How to provide feedbackSection titled “cas - How to provide feedback” Share your experience with this feature in the CAS Feedback GitHub Discussion. Feedback is crucial for ensuring the feature meets real-world use cases. Please include: Any bugs or issues encountered (including logs or stack traces if possible). Suggestions for additional improvements or enhancements. cas - Criteria for stabilizationSection titled “cas - Criteria for stabilization” To transition the cas feature to a stable release, the following must be addressed: Add support for storing and retrieving catalog repositories from the CAS. Add support for storing and retrieving OpenTofu/Terraform modules from the CAS. Add support for storing and retrieving Unit/Stack configurations from the CAS. filter-flagSection titled “filter-flag” Support for sophisticated unit and stack filtering using the --filter flag. filter-flag - What it doesSection titled “filter-flag - What it does” The --filter flag provides a sophisticated querying syntax for targeting units and stacks in Terragrunt commands. This unified approach replaces the need for multiple queue control flags and offers powerful filtering capabilities. Current Support Status: ✅ Available in find and list commands ❌ Not yet available in run command - this is planned for future implementation Supported Filtering Types: Name-based filtering: Target units/stacks by their directory name (exact match or glob patterns) Path-based filtering: Target units/stacks by their file system path (relative, absolute, or glob patterns) Attribute-based filtering: Target units by configuration attributes: type=unit or type=stack - Filter by component type external=true or external=false - Filter by whether the unit/stack is an external dependency (outside the current working directory) name=pattern - Filter by name using glob patterns Negation filters: Exclude units using the ! prefix Filter intersection: Combine filters using the | operator for results pruning Multiple filters: Specify multiple --filter flags to union results Not Yet Implemented: Git-based filtering ([ref...ref] syntax) Dependency/dependent traversal (... syntax) Integration with run command filter-flag - How to provide feedbackSection titled “filter-flag - How to provide feedback” Provide your feedback on the Filter Flag RFC GitHub issue. filter-flag - Criteria for stabilizationSection titled “filter-flag - Criteria for stabilization” To transition the filter-flag feature to a stable release, the following must be addressed, at a minimum: Add support for name-based filtering Add support for path-based filtering (relative, absolute, glob) Add support for attribute-based filtering (type, external, name) Add support for negation filters (!) Add support for filter intersection (|) Add support for multiple filters (union/OR semantics) Integrate with the find command Integrate with the list command Integrate with the run command Add support for git-based filtering ([ref…ref] syntax) Add support for dependency/dependent traversal (… syntax) Add support for --filters-file flag Add support for --filter-allow-destroy flag Add support for --filter-affected shorthand Comprehensive integration testing across all commands Deprecate legacy queue control flags (queue-exclude-dir, queue-include-dir, etc.) Future Deprecations: When this experiment stabilizes, the following queue control flags will be deprecated in favor of the unified --filter flag: --queue-exclude-dir --queue-excludes-file --queue-exclude-external --queue-include-dir --queue-include-external --queue-include-units-including --queue-strict-include The current plan is to continue to support the flags as aliases for particular --filter patterns. Completed ExperimentsSection titled “Completed Experiments” cli-redesign stacks runner-pool report auto-provider-cache-dir cli-redesignSection titled “cli-redesign” Support for the new Terragrunt CLI design. cli-redesign - What it doesSection titled “cli-redesign - What it does” Enabled features from the CLI Redesign RFC. This experiment flag is no longer needed, as the CLI Redesign is now the default. cli-redesign - How to provide feedbackSection titled “cli-redesign - How to provide feedback” Now that the CLI Redesign experiment is complete, please provide feedback in the form of standard GitHub issues. cli-redesign - Criteria for stabilizationSection titled “cli-redesign - Criteria for stabilization” To transition cli-redesign features to a stable the following have been completed: Add support for run command. Add support for basic usage of the run command (e.g., terragrunt run plan, terragrunt run -- plan -no-color). Add support for the --all flag. Add support for the --graph flag. Add support for exec command. Rename legacy --terragrunt- prefixed flags so that they no longer need the prefix. Add the hcl command, replacing commands like hclfmt, hclvalidate and validate-inputs. Add OpenTofu commands as explicit shortcuts in the CLI instead of forwarding all unknown commands to OpenTofu/Terraform. Add support for the backend command. Add support for the render command. Add support for the info command. Add support for the dag command. Add support for the find command. Add support for find without flags. Add support for find with colorful output. Add support for find with --format=json flag. Add support for find with stdout redirection detection. Add support for find with --hidden flag. Add support for find with --sort=alpha flag. Add support for find with --sort=dag flag. Add support for find with the exclude block used to exclude units from the search. Add integration with symlinks experiment to support finding units/stacks via symlinks. Add handling of broken configurations or configurations requiring authentication. Add integration test for find with --sort=dag flag on all the fixtures in the test/fixtures directory. Add support for the list command. Add support for list without flags. Add support for list with colorful output. Add support for list with --format=tree flag. Add support for list with --format=long flag. Add support for list with stdout redirection detection. Add support for list with --hidden flag. Add support for list with --sort=alpha flag. Add support for list with --sort=dag flag. Add support for list with --group-by=fs flag. Add support for list with --group-by=dag flag. Add support for list with the exclude block used to exclude units from the search. Add integration with symlinks experiment to support listing units/stacks via symlinks. Add handling of broken configurations or configurations requiring authentication. Add integration test for list with --sort=dag flag on all the fixtures in the test/fixtures directory. stacksSection titled “stacks” Support for Terragrunt stacks. What it doesSection titled “What it does” Enable stack command to manage Terragrunt stacks. stacks - Criteria for stabilizationSection titled “stacks - Criteria for stabilization” To transition the stacks feature to a stable release, the following must be addressed: Add support for stack run * command Add support for stack output commands to extend stack-level operations. Integration testing for recursive stack handling across typical workflows, ensuring smooth transitions during plan, apply, and destroy operations. Confirm compatibility with parallelism flags (e.g., --parallel), especially for stacks with dependencies. Ensure that error handling and failure recovery strategies work as intended across large and nested stacks. runner-poolSection titled “runner-pool” Proposes replacing Terragrunt’s group-based execution with a dynamic runner pool that schedules Units as soon as dependencies are resolved. This improves efficiency, reduces bottlenecks, and limits the impact of individual failures. runner-pool - What it doesSection titled “runner-pool - What it does” Allow usage of experimental runner pool implementation for units execution. runner-pool - How to provide feedbackSection titled “runner-pool - How to provide feedback” Provide your feedback on the Runner Pool. runner-pool - Criteria for stabilizationSection titled “runner-pool - Criteria for stabilization” To transition the runner-pool feature to a stable release, the following must be addressed: Use new discovery and queue packages to discover units. Add support for including/excluding external units in the discovery process. Add runner pool implementation to execute discovered units. Add integration tests to track that the runner pool works in the same way as the current implementation. Add performance tests to track that the runner pool implementation is faster than the current implementation. Add support for fail fast behavior in the runner pool. Improve the UI to queue to apply. Add OpenTelemetry support to the runner pool. reportSection titled “report” Support for Terragrunt Run Reports and Summaries. report - What it doesSection titled “report - What it does” Allows generation of run reports and summary displays. This experiment flag is no longer needed, as the report feature is now stable and available by default. report - How to provide feedbackSection titled “report - How to provide feedback” Now that the report experiment is complete, please provide feedback in the form of standard GitHub issues. report - Criteria for stabilizationSection titled “report - Criteria for stabilization” To transition the report feature to stable, the following have been completed: Add support for generating reports (in CSV format by default). Add support for displaying summaries of runs. Add ability to disable summary display. Add support for generating reports in JSON format. Add comprehensive integration tests for the report experiment. Finalize the design of run summaries and reports. auto-provider-cache-dirSection titled “auto-provider-cache-dir” Enable native OpenTofu provider caching by setting TF_PLUGIN_CACHE_DIR instead of using Terragrunt’s internal provider cache server. auto-provider-cache-dir - What it doesSection titled “auto-provider-cache-dir - What it does” This experiment automatically configures OpenTofu to use its built-in provider caching mechanism by setting the TF_PLUGIN_CACHE_DIR environment variable. This approach leverages OpenTofu’s native provider caching capabilities, which are more robust for concurrent operations in OpenTofu 1.10+. This experiment flag is no longer needed, as the auto-provider-cache-dir feature is now enabled by default when using OpenTofu >= 1.10. Requirements: OpenTofu version >= 1.10 is required Only works when using OpenTofu (not Terraform) Disabling the feature: You can disable the auto-provider-cache-dir feature using the --no-auto-provider-cache-dir flag: terragrunt run --all apply --no-auto-provider-cache-dir auto-provider-cache-dir - How to provide feedbackSection titled “auto-provider-cache-dir - How to provide feedback” Now that the auto-provider-cache-dir experiment is complete, please provide feedback in the form of standard GitHub issues. auto-provider-cache-dir - Criteria for stabilizationSection titled “auto-provider-cache-dir - Criteria for stabilization” To transition the auto-provider-cache-dir feature to stable, the following have been completed: Comprehensive testing to confirm the safety of concurrent runs using the same provider cache directory. Performance comparison with the existing provider cache server approach. Documentation and examples of best practices for usage. Community feedback on real-world usage and any edge cases discovered. Edit page Previous Strict Controls Next Supported Versions",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.660Z"
  },
  {
    "title": "Supported Versions",
    "url": "https://terragrunt.gruntwork.io/docs/reference/supported-versions/",
    "content": "OpenTofu and Terraform Version Compatibility Table Supported OpenTofu VersionsSection titled “Supported OpenTofu Versions” The officially supported versions are: OpenTofu VersionTerragrunt Version1.10.x>= 0.82.01.9.x>= 0.72.01.8.x>= 0.66.01.7.x>= 0.58.01.6.x>= 0.52.0 Supported Terraform VersionsSection titled “Supported Terraform Versions” The officially supported versions are: Terraform VersionTerragrunt Version1.13.x>= 0.86.01.12.x>= 0.80.01.11.x>= 0.75.01.10.x>= 0.74.01.9.x>= 0.60.01.8.x>= 0.57.01.7.x>= 0.56.01.6.x>= 0.53.01.5.x>= 0.48.01.4.x>= 0.45.01.3.x>= 0.40.01.2.x>= 0.38.01.1.x>= 0.36.01.0.x>= 0.31.00.15.x>= 0.29.00.14.x>= 0.27.00.13.x>= 0.25.00.12.x0.19.0 - 0.24.40.11.x0.14.0 - 0.18.7 Note 1: Terragrunt lists support for BSL versions of Terraform (>= 1.6.x) and core IaC functionality will work as expected. However, support for BSL Terraform-specific features is not guaranteed even if that version is in this table. Note 2: This table lists versions that are officially tested in the CI process. In practice, the version compatibility is more relaxed than documented above. For example, we’ve found that Terraform 0.13 works with any version above 0.19.0, and we’ve also found that terraform 0.11 works with any version above 0.19.18 as well. If you wish to use Terragrunt against an untested Terraform version, you can use the terraform_version_constraint (introduced in Terragrunt v0.19.18) attribute to relax the version constraint. Edit page Previous Experiments Next Lock Files",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.685Z"
  },
  {
    "title": "Lock Files",
    "url": "https://terragrunt.gruntwork.io/docs/reference/lock-files/",
    "content": "Lock File Handling How to use lock files with TerragruntSection titled “How to use lock files with Terragrunt” To use OpenTofu/Terraform lock files with Terragrunt, you need to: Run Terragrunt as usual (e.g., run terragrunt plan, terragrunt apply, etc.). Check the .terraform.lock.hcl file into version control. Everything else with OpenTofu/Terraform and Terragrunt should work as expected. To learn the details of how this works, read on! How Terragrunt handles lock filesSection titled “How Terragrunt handles lock files” What’s a lock file?Section titled “What’s a lock file?” Terraform 0.14 added support for a lock file which gets created or updated every time you run tofu init/terraform init. The file is typically generated into your working directory (i.e., the folder in which you ran tofu init/terraform init) and is called .terraform.lock.hcl. It captures the versions of all the OpenTofu/Terraform providers you’re using. Normally, you want to check this file into version control so that when your team members run OpenTofu/Terraform, they get the identical provider versions. The problem with mixing remote OpenTofu/Terraform configurations in Terragrunt and lock filesSection titled “The problem with mixing remote OpenTofu/Terraform configurations in Terragrunt and lock files” Let’s say you are using Terragrunt with remote OpenTofu/Terraform configurations and you have the following folder structure: Directorylive Directoryprod Directoryvpc terragrunt.hclDirectorystage Directoryvpc terragrunt.hcl Imagine that in live/stage/vpc/terragrunt.hcl, you have the following contents: terraform { source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"} If you ran terragrunt apply in the /live/stage/vpc folder, Terragrunt will: git clone the VPC module in the source URL into a temp folder in .terragrunt-cache/xxx/vpc, where xxx is dynamically determined based on the URL. Run tofu apply/terraform apply in the .terragrunt-cache/xxx/vpc temp folder. As a result, the .terraform.lock.hcl file will be generated in the .terragrunt-cache/xxx/vpc temp folder, rather than in /live/stage/vpc. How Terragrunt solves this problemSection titled “How Terragrunt solves this problem” To solve this problem, since version v0.27.0, Terragrunt implements the following logic for lock files: If Terragrunt finds a .terraform.lock.hcl file in your working directory (e.g., in /live/stage/vpc), before running OpenTofu/Terraform, Terragrunt will copy that lock file into the temp folder it uses when running your OpenTofu/Terraform code (e.g., .terragrunt-cache/xxx/vpc). This way, if you had a lock file checked into version control, Terragrunt will respect and use it with your OpenTofu/Terraform code as you’d expect. After running OpenTofu/Terraform, if Terragrunt finds a .terraform.lock.hcl in the temp folder (e.g., .terragrunt-cache/xxx/vpc), it will copy that lock file back to your working directory (e.g., to /live/stage/vpc). That way, you can commit the lock file (or the changes to the lock file) to version control as usual. <!— markdownlint-disable MD026 —> Check the lock file in!Section titled “Check the lock file in!” After running Terragrunt on each of your modules, you should check your lock files in! That means your folder structure should end up looking something like this: Directorylive Directoryprod Directoryvpc .terraform.lock.hclterragrunt.hclDirectorystage Directoryvpc .terraform.lock.hclterragrunt.hcl Also, any time you change the providers you’re using, and re-run init, the lock file will be updated, so make sure to check the updates into version control too. Disabling the copy of the generated lock fileSection titled “Disabling the copy of the generated lock file” In certain use cases, like when using a remote module containing a lock file within it, you probably don’t want Terragrunt to also copy the lock file into your working directory. In these scenarios, you can opt out of copying the .terraform.lock.hcl file by using copy_terraform_lock_file = false in the terraform configuration block as follows: terraform { ... copy_terraform_lock_file = false} Edit page Previous Supported Versions Next Overview",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.714Z"
  },
  {
    "title": "Overview",
    "url": "https://terragrunt.gruntwork.io/docs/reference/logging/",
    "content": "Overview Terragrunt logs messages as it runs to help you understand what it’s doing. Given that Terragrunt is an IaC orchestrator, this can result in messages that are surprising if you don’t understand what Terragrunt is doing behind the scenes. Log LevelsSection titled “Log Levels” To start with, Terragrunt has the following log levels: STDERR STDOUT ERROR WARN INFO DEBUG TRACE The STDOUT and STDERR log levels are non-standard, and exist due to Terragrunt’s special responsibility as an IaC orchestrator. For the most part, whenever you use Terragrunt to run something using another tool (like OpenTofu or Terraform), Terragrunt will capture the stdout and stderr terminal output from that tool, enrich it with additional information, then log it as STDOUT or STDERR respectively. The exception to this is when Terragrunt is running a process in “Headless Mode”, where it will instead emit stdout and stderr terminal output to INFO and ERROR log levels respectively. All other log levels are standard, and are used by Terragrunt to log its own messages. For example: $ terragrunt --log-level debug plan14:20:38.431 DEBUG Terragrunt Version: 0.0.014:20:38.431 DEBUG Did not find any locals block: skipping evaluation.14:20:38.431 DEBUG Running command: tofu --version14:20:38.431 DEBUG Engine is not enabled, running command directly in .14:20:38.451 DEBUG tofu version: 1.8.514:20:38.451 DEBUG Reading Terragrunt config file at ./terragrunt.hcl14:20:38.451 DEBUG Did not find any locals block: skipping evaluation.14:20:38.451 DEBUG Did not find any locals block: skipping evaluation.14:20:38.452 DEBUG Running command: tofu init14:20:38.452 DEBUG Engine is not enabled, running command directly in .14:20:38.469 INFO tofu: Initializing the backend...14:20:38.470 INFO tofu: Initializing provider plugins...14:20:38.470 INFO tofu: OpenTofu has been successfully initialized!14:20:38.470 INFO tofu:14:20:38.470 INFO tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see14:20:38.470 INFO tofu: any changes that are required for your infrastructure. All OpenTofu commands14:20:38.470 INFO tofu: should now work.14:20:38.470 INFO tofu: If you ever set or change modules or backend configuration for OpenTofu,14:20:38.470 INFO tofu: rerun this command to reinitialize your working directory. If you forget, other14:20:38.470 INFO tofu: commands will detect it and remind you to do so if necessary.14:20:38.470 DEBUG Running command: tofu plan14:20:38.470 DEBUG Engine is not enabled, running command directly in .14:20:38.490 STDOUT tofu: No changes. Your infrastructure matches the configuration.14:20:38.490 STDOUT tofu: OpenTofu has compared your real infrastructure against your configuration and14:20:38.490 STDOUT tofu: found no differences, so no changes are needed. Here, we have three types of log messages: DEBUG messages from Terragrunt itself. By default, Terragrunt’s log level is INFO, but we’ve set it to DEBUG using the --log-level flag. STDOUT messages from OpenTofu. These are messages that OpenTofu would normally print directly to the terminal, but instead, Terragrunt captures them and logs them as STDOUT log messages, along with timestamps and other metadata. INFO messages from Terragrunt auto-init. These were initially emitted by OpenTofu. However, the user did not specifically ask for them, so Terragrunt logs them as INFO messages. EnrichmentSection titled “Enrichment” The reason Terragrunt enriches stdout/stderr from the processes is that it is often very useful to have this extra metadata. For example: $ terragrunt run --all plan14:27:45.359 INFO The stack at . will be processed in the following order for command plan:Group 1- Module ./unit-1- Module ./unit-2 14:27:45.399 INFO [unit-2] tofu: Initializing the backend...14:27:45.399 INFO [unit-1] tofu: Initializing the backend...14:27:45.400 INFO [unit-2] tofu: Initializing provider plugins...14:27:45.400 INFO [unit-2] tofu: OpenTofu has been successfully initialized!14:27:45.400 INFO [unit-2] tofu:14:27:45.400 INFO [unit-2] tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see14:27:45.400 INFO [unit-2] tofu: any changes that are required for your infrastructure. All OpenTofu commands14:27:45.400 INFO [unit-2] tofu: should now work.14:27:45.400 INFO [unit-2] tofu: If you ever set or change modules or backend configuration for OpenTofu,14:27:45.400 INFO [unit-2] tofu: rerun this command to reinitialize your working directory. If you forget, other14:27:45.400 INFO [unit-2] tofu: commands will detect it and remind you to do so if necessary.14:27:45.400 INFO [unit-1] tofu: Initializing provider plugins...14:27:45.400 INFO [unit-1] tofu: OpenTofu has been successfully initialized!14:27:45.400 INFO [unit-1] tofu:14:27:45.400 INFO [unit-1] tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see14:27:45.400 INFO [unit-1] tofu: any changes that are required for your infrastructure. All OpenTofu commands14:27:45.400 INFO [unit-1] tofu: should now work.14:27:45.400 INFO [unit-1] tofu: If you ever set or change modules or backend configuration for OpenTofu,14:27:45.400 INFO [unit-1] tofu: rerun this command to reinitialize your working directory. If you forget, other14:27:45.400 INFO [unit-1] tofu: commands will detect it and remind you to do so if necessary.14:27:45.422 STDOUT [unit-2] tofu: No changes. Your infrastructure matches the configuration.14:27:45.423 STDOUT [unit-2] tofu: OpenTofu has compared your real infrastructure against your configuration and14:27:45.423 STDOUT [unit-2] tofu: found no differences, so no changes are needed.14:27:45.423 STDOUT [unit-1] tofu: No changes. Your infrastructure matches the configuration.14:27:45.423 STDOUT [unit-1] tofu: OpenTofu has compared your real infrastructure against your configuration and14:27:45.423 STDOUT [unit-1] tofu: found no differences, so no changes are needed. Here you see two different units being run by Terragrunt concurrently, and stdout/stderr for each being emitted in real time. This is really helpful when managing IaC at scale, as it lets you know exactly what each unit in your stack is doing, and how long it is taking. It’s easier to see the impact of this enrichment if we turn it off, so let’s use the bare preset described in Log Formatting. $ terragrunt run --all --log-format bare planINFO[0000] The stack at /Users/yousif/tmp/testing-stdout-stderr-split will be processed in the following order for command plan:Group 1- Module /Users/yousif/tmp/testing-stdout-stderr-split/unit-1- Module /Users/yousif/tmp/testing-stdout-stderr-split/unit-2 Initializing the backend... Initializing provider plugins... OpenTofu has been successfully initialized! You may now begin working with OpenTofu. Try running \"tofu plan\" to seeany changes that are required for your infrastructure. All OpenTofu commandsshould now work. If you ever set or change modules or backend configuration for OpenTofu,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary. No changes. Your infrastructure matches the configuration. OpenTofu has compared your real infrastructure against your configuration andfound no differences, so no changes are needed. Initializing the backend... Initializing provider plugins... OpenTofu has been successfully initialized! You may now begin working with OpenTofu. Try running \"tofu plan\" to seeany changes that are required for your infrastructure. All OpenTofu commandsshould now work. If you ever set or change modules or backend configuration for OpenTofu,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary. No changes. Your infrastructure matches the configuration. OpenTofu has compared your real infrastructure against your configuration andfound no differences, so no changes are needed. This tells Terragrunt to log messages from OpenTofu/Terraform without any enrichment. As you can see, it’s not as easy to disambiguate the messages from the two units, so it helps to use Terragrunt’s default log format when managing IaC at scale. Exceptions to enrichmentSection titled “Exceptions to enrichment” There are exceptions to the general rule that Terragrunt logs stdout/stderr from the processes it runs as STDOUT and STDERR respectively when not in Headless Mode. Because Terragrunt is an IaC orchestrator, it uses its awareness of OpenTofu/Terraform usage to recognize certain circumstances when a user is likely to want stdout/stderr to be emitted exactly as it would when running a process directly. An example of this is terragrunt output: $ terragrunt output -json15:20:07.759 INFO tofu: Initializing the backend...15:20:07.759 INFO tofu: Initializing provider plugins...15:20:07.759 INFO tofu: OpenTofu has been successfully initialized!15:20:07.759 INFO tofu:15:20:07.759 INFO tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see15:20:07.759 INFO tofu: any changes that are required for your infrastructure. All OpenTofu commands15:20:07.759 INFO tofu: should now work.15:20:07.759 INFO tofu: If you ever set or change modules or backend configuration for OpenTofu,15:20:07.759 INFO tofu: rerun this command to reinitialize your working directory. If you forget, other15:20:07.759 INFO tofu: commands will detect it and remind you to do so if necessary.{ \"something\": { \"sensitive\": false, \"type\": \"string\", \"value\": \"Hello, World!\" }, \"something_else\": { \"sensitive\": false, \"type\": \"string\", \"value\": \"Goodbye, World!\" }} As you can see, the output from OpenTofu here isn’t being enriched, even though the user explicitly asked Terragrunt to run output -json. This is because the user is pretty likely to want to programmatically interact with the output of output, and so Terragrunt doesn’t enrich it. If, for example, you wanted to use a tool like jq to parse the output of terragrunt output -json, you could do that without having to worry about Terragrunt’s metadata getting in the way, or disabling anything with an extra flag. $ terragrunt output -json | jq '.something'15:24:40.310 INFO tofu: Initializing the backend...15:24:40.311 INFO tofu: Initializing provider plugins...15:24:40.311 INFO tofu: OpenTofu has been successfully initialized!15:24:40.311 INFO tofu:15:24:40.311 INFO tofu: You may now begin working with OpenTofu. Try running \"tofu plan\" to see15:24:40.311 INFO tofu: any changes that are required for your infrastructure. All OpenTofu commands15:24:40.311 INFO tofu: should now work.15:24:40.311 INFO tofu: If you ever set or change modules or backend configuration for OpenTofu,15:24:40.311 INFO tofu: rerun this command to reinitialize your working directory. If you forget, other15:24:40.311 INFO tofu: commands will detect it and remind you to do so if necessary.{ \"sensitive\": false, \"type\": \"string\", \"value\": \"Hello, World!\"} Streaming and bufferingSection titled “Streaming and buffering” While Terragrunt logs stdout from OpenTofu/Terraform in real time, it buffers each line of stdout before logging it. This is because Terragrunt needs to be able to buffer stdout to prevent different units from interleaving their log messages. Depending on what you’re doing with Terragrunt, this might occasionally result in issues when multiple units are running concurrently, and they are each producing multi-line output that is more convenient to be read independently. In these cases, you can do some post-processing on the logs to read the units in isolation. For example: $ terragrunt run --all apply --no-color --non-interactive > logs16:01:51.164 INFO The stack at . will be processed in the following order for command apply:Group 1- Module ./unit1- Module ./unit2 $ grep '\\[unit1\\]' < logs16:01:51.272 STDOUT [unit1] tofu: null_resource.empty: Refreshing state... [id=3335573617542340690]16:01:51.279 STDOUT [unit1] tofu: OpenTofu used the selected providers to generate the following execution16:01:51.279 STDOUT [unit1] tofu: plan. Resource actions are indicated with the following symbols:16:01:51.279 STDOUT [unit1] tofu: -/+ destroy and then create replacement16:01:51.279 STDOUT [unit1] tofu: OpenTofu will perform the following actions:16:01:51.279 STDOUT [unit1] tofu: # null_resource.empty must be replaced16:01:51.279 STDOUT [unit1] tofu: -/+ resource \"null_resource\" \"empty\" {16:01:51.279 STDOUT [unit1] tofu: ~ id = \"3335573617542340690\" -> (known after apply)16:01:51.279 STDOUT [unit1] tofu: ~ triggers = { # forces replacement16:01:51.280 STDOUT [unit1] tofu: ~ \"always_run\" = \"2025-01-09T21:01:17Z\" -> (known after apply)16:01:51.280 STDOUT [unit1] tofu: }16:01:51.280 STDOUT [unit1] tofu: }16:01:51.280 STDOUT [unit1] tofu: Plan: 1 to add, 0 to change, 1 to destroy.16:01:51.280 STDOUT [unit1] tofu:16:01:51.297 STDOUT [unit1] tofu: null_resource.empty: Destroying... [id=3335573617542340690]16:01:51.297 STDOUT [unit1] tofu: null_resource.empty: Destruction complete after 0s16:01:51.300 STDOUT [unit1] tofu: null_resource.empty: Creating...16:01:51.301 STDOUT [unit1] tofu: null_resource.empty: Provisioning with 'local-exec'...16:01:51.301 STDOUT [unit1] tofu: null_resource.empty (local-exec): Executing: [\"/bin/sh\" \"-c\" \"echo 'sleeping...'; sleep 1; echo 'done sleeping'\"]16:01:51.304 STDOUT [unit1] tofu: null_resource.empty (local-exec): sleeping...16:01:52.311 STDOUT [unit1] tofu: null_resource.empty (local-exec): done sleeping16:01:52.312 STDOUT [unit1] tofu: null_resource.empty: Creation complete after 1s [id=4749136145104485309]16:01:52.322 STDOUT [unit1] tofu:16:01:52.322 STDOUT [unit1] tofu: Apply complete! Resources: 1 added, 0 changed, 1 destroyed.16:01:52.322 STDOUT [unit1] tofu: $ grep '\\[unit2\\]' < logs16:01:51.273 STDOUT [unit2] tofu: null_resource.empty: Refreshing state... [id=7532622543468447677]16:01:51.280 STDOUT [unit2] tofu: OpenTofu used the selected providers to generate the following execution16:01:51.280 STDOUT [unit2] tofu: plan. Resource actions are indicated with the following symbols:16:01:51.280 STDOUT [unit2] tofu: -/+ destroy and then create replacement16:01:51.280 STDOUT [unit2] tofu: OpenTofu will perform the following actions:16:01:51.280 STDOUT [unit2] tofu: # null_resource.empty must be replaced16:01:51.280 STDOUT [unit2] tofu: -/+ resource \"null_resource\" \"empty\" {16:01:51.280 STDOUT [unit2] tofu: ~ id = \"7532622543468447677\" -> (known after apply)16:01:51.280 STDOUT [unit2] tofu: ~ triggers = { # forces replacement16:01:51.280 STDOUT [unit2] tofu: ~ \"always_run\" = \"2025-01-09T21:01:17Z\" -> (known after apply)16:01:51.280 STDOUT [unit2] tofu: }16:01:51.280 STDOUT [unit2] tofu: }16:01:51.280 STDOUT [unit2] tofu: Plan: 1 to add, 0 to change, 1 to destroy.16:01:51.280 STDOUT [unit2] tofu:16:01:51.297 STDOUT [unit2] tofu: null_resource.empty: Destroying... [id=7532622543468447677]16:01:51.297 STDOUT [unit2] tofu: null_resource.empty: Destruction complete after 0s16:01:51.300 STDOUT [unit2] tofu: null_resource.empty: Creating...16:01:51.301 STDOUT [unit2] tofu: null_resource.empty: Provisioning with 'local-exec'...16:01:51.301 STDOUT [unit2] tofu: null_resource.empty (local-exec): Executing: [\"/bin/sh\" \"-c\" \"echo 'sleeping...'; sleep 1; echo 'done sleeping'\"]16:01:51.303 STDOUT [unit2] tofu: null_resource.empty (local-exec): sleeping...16:01:52.311 STDOUT [unit2] tofu: null_resource.empty (local-exec): done sleeping16:01:52.312 STDOUT [unit2] tofu: null_resource.empty: Creation complete after 1s [id=6569505210291935319]16:01:52.322 STDOUT [unit2] tofu:16:01:52.322 STDOUT [unit2] tofu: Apply complete! Resources: 1 added, 0 changed, 1 destroyed.16:01:52.322 STDOUT [unit2] tofu: Disabling logsSection titled “Disabling logs” Finally, you can also disable logs entirely, like so: $ terragrunt --log-disable plan Initializing the backend... Initializing provider plugins... OpenTofu has been successfully initialized! You may now begin working with OpenTofu. Try running \"tofu plan\" to seeany changes that are required for your infrastructure. All OpenTofu commandsshould now work. If you ever set or change modules or backend configuration for OpenTofu,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary. No changes. Your infrastructure matches the configuration. OpenTofu has compared your real infrastructure against your configuration andfound no differences, so no changes are needed. This will give you the closest experience to using OpenTofu/Terraform directly, with Terragrunt doing all of its work in the background. Edit page Previous Lock Files Next Formatting",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.764Z"
  },
  {
    "title": "Formatting",
    "url": "https://terragrunt.gruntwork.io/docs/reference/logging/formatting/",
    "content": "Formatting Using the --log-custom-format <format> flag you can customize the way Terragrunt logs with total control over the logging format. The argument passed to this flag is a Terragrunt native format string that has special syntax, as described below. PlaceholdersSection titled “Placeholders” The format string consists of placeholders and text. Placeholders start with the % sign. e.g. --log-custom-format \"%time %level %msg\" Output: 10:09:19.809 debug Running command: tofu --version To escape the % character, use %%. e.g. --log-custom-format \"%time %level %%msg\" Output: 10:09:19.809 debug %msg Placeholders have preset names: %time - Current time. %interval - Seconds elapsed since Terragrunt started. %level - Log level. %prefix - Path to the working directory were Terragrunt is running. %msg - Log message. %tf-path - Path to the OpenTofu/Terraform executable (as defined by tf-path). %tf-command - Executed OpenTofu/Terraform command, e.g. apply. %tf-command-args - Arguments of the executed OpenTofu/Terraform command, e.g. apply -auto-approve. %t - Indent. %n - Newline. Any other text is considered plain text. The parser always tries to find the longest name. For example, tofu command “apply -auto-approve” with format “%tf-command-args” will be replaced with “apply -auto-approve”, but not “apply-args”. If you need to replace it with “apply-args”, use empty brackets “%tf-command()-args”. More examples: “%tf-path” will be replaced with “tofu”, %t()-path will be replaced with ” -path”. e.g. --log-custom-format \"time=%time level=%level message=%msg\" Output: time=00:10:44.716 level=debug message=Running command: tofu --version Using the placeholder as shown above will display the value simply. If you would like to format the value, you can pass options to the placeholder. Placeholder formatting uses the following syntax: %placeholder-name(option-name=option-value, option-name=option-value,...) e.g. --log-custom-format \"%time(format='Y-m-d H:i:sv') %level(format=short,case=upper) %msg\" Output: 2024-11-12 11:52:20.214 DEB Running command: tofu --version In this example, the timestamp (as referenced by the %time placeholder) has been formatted with the format string Y-m-d H:i:sv. Similarly, the log level (as referenced by the %level placeholder), has been formatted to use the short format, and upper case. Even if you don’t pass options, the empty parenthesis are added implicitly. Thus %time is equivalent to %time(). Parenthesis are considered part of the syntax for specifying parameters to placeholders by default. Any parenthesis following a placeholder will be interpreted as specifying the parameters for the placeholder function. e.g. --log-custom-format \"%time(plain-text)\" Output: invalid option name \"plain-text\" for placeholder \"time\" If you would like to escape parentheses so that they appear as plain text in logs, make sure to use empty parentheses after a placeholder so that the following parentheses are not evaluated as specifying parameters for the placeholder function. e.g. --log-custom-format \"%time()(plain-text)\" Output: 12:33:08.513(plain-text) You can format plain text as well by using an unnamed placeholder. e.g. --log-custom-format \"%(content='time=',color=magenta)%time %(content='level=',color=light-blue)%level %(content='msg=',color=green)%msg\" Output: time=12:33:08.513 level=debug msg=Running command: tofu --version Unfortunately, it is not possible to display color in a Markdown document, but in the above output, time= is colored magenta, level= is colored light blue and msg= is colored green. OptionsSection titled “Options” Options can be divided into common ones, which can be passed to any placeholder, and specific ones for each placeholder. Common options: content=<text> - Sets a placeholder value, typically used to set the initial value of an unnamed placeholder. case=[upper|lower|capitalize] - Sets the case of the text. width=<number> - Sets the column width. align=[left|center|right] - Aligns content relative to the edges of the column, used in conjunction with width. prefix=<text> - Prepends the prefix to the content. If the content of the placeholder is empty, the prefix will not be prepended. suffix=<text>- Appends the suffix to the content. If the content of the placeholder is empty, the suffix will not be appended. escape=[json] - Escapes content for use as a value in a JSON string. color=[red|white|yellow|green|cayn|magenta|blue|...] - Sets the color for the content. 1..255 - Specifies a color using a number, 1 to 255 red|white|yellow|green|cyan|magenta|blue|light-blue|light-black|light-red|light-green|light-yellow|light-magenta|light-cyan|light-white - Specifies a color using a word gradient - Specifies to use a different color each time the placeholder content changes. preset - Specifies to use preset colors. For example, each log level name has its own preset color. disable - Disables color, also removes colors set in terraform/tofu output. Specific options for placeholders: %level format=[full|short|tiny] - Specifies the format for log level names. full - stdout, stderr, error, warn, info, debug, trace short - std, err, wrn, inf, deb, trc tiny - s, e, w, i, d, t %time format=<time-format> - Sets the time format. Preset formats: date-time - e.g. 2006-01-02 15:04:05 date-only - e.g. 2006-01-02 time-only - e.g. 15:04:05 rfc3339 - e.g. 2006-01-02T15:04:05Z07:00 rfc3339-nano - e.g. 2006-01-02T15:04:05.999999999Z07:00 Custom format string characters: H - 24-hour format of an hour with leading zeros, 00 to 23 h - 12-hour format of an hour with leading zeros, 01 to 12 g - 12-hour format of an hour without leading zeros, 1 to 12 i - Minutes with leading zeros, 00 to 59 s - Seconds with leading zeros, 00 to 59 v - Milliseconds. e.g. .654 u - Microseconds, e.g. .654321 Y - A full numeric representation of a year, e.g. 1999, 2003 y - A two digit representation of a year, e.g. 99, 03 m - Numeric representation of a month, with leading zeros, 01 to 12 n - Numeric representation of a month, without leading zeros, 1 to 12 M - A short textual representation of a month, three letters, Jan to Dec d - Day of the month, 2 digits with leading zeros, 01 to 31 j - Day of the month without leading zeros, 1 to 31 D - A textual representation of a day, three letters, Mon to Sun A - Uppercase Ante meridiem and Post meridiem, AM or PM a - Lowercase Ante meridiem and Post meridiem, am or pm T - Timezone abbreviation, e.g. EST, MDT P - Difference to Greenwich time (GMT) with colon between hours and minutes, e.g. +02:00 O - Difference to Greenwich time (GMT) without colon between hours and minutes, e.g. +0200 %prefix path=[relative|short-relative|short] relative - Outputs a relative path to the working directory. short-relative - Outputs a relative path to the working directory, trims the leading slash ./ and hides the working directory path . short - Outputs an absolute path, but hides the working directory path. %tf-path path=[filename|dir] filename - Outputs the name of the executable. dir - Outputs the directory name of the executable. %msg path=[relative] relative - Converts all absolute paths to paths relative to the working directory. PresetsSection titled “Presets” The examples below replicate the preset formats specified with --log-format. They can be useful if you need to change existing formats to suit your needs. PrettySection titled “Pretty” --log-format pretty --log-custom-format \"%time(color=light-black) %level(case=upper,width=6,color=preset) %prefix(path=short-relative,color=gradient,suffix=' ')%tf-path(color=cyan,suffix=': ')%msg(path=relative)\" BareSection titled “Bare” --log-format bare --tf-forward-stdout --log-custom-format \"%level(case=upper,width=4)[%interval] %msg %prefix(path=short,prefix='prefix=[',suffix=']')\" Key-valueSection titled “Key-value” --log-format key-value --log-custom-format \"time=%time(format=rfc3339) level=%level prefix=%prefix(path=short-relative) tf-path=%tf-path(path=filename) msg=%msg(path=relative,color=disable)\" JSONSection titled “JSON” --log-format json --log-custom-format '{\"time\":\"%time(format=rfc3339,escape=json)\", \"level\":\"%level(escape=json)\", \"prefix\":\"%prefix(path=short-relative,escape=json)\", \"tf-path\":\"%tf-path(path=filename,escape=json)\", \"msg\":\"%msg(path=relative,escape=json,color=disable)\"}' Edit page Previous Overview Next Terragrunt Cache",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.804Z"
  },
  {
    "title": "Terragrunt Cache",
    "url": "https://terragrunt.gruntwork.io/docs/reference/terragrunt-cache/",
    "content": "Terragrunt Cache Terragrunt uses a cache directory (.terragrunt-cache) to store downloaded modules when using the source attribute in the terraform block. This cache directory is created whenever Terragrunt downloads a module from a remote source, and where it runs the OpenTofu/Terraform commands. It also stores any modules and providers that are downloaded as part of these commands by default. Clearing the Terragrunt cacheSection titled “Clearing the Terragrunt cache” Terragrunt creates a .terragrunt-cache folder in the current working directory as its scratch directory. It downloads your remote OpenTofu/Terraform configurations into this folder, runs your OpenTofu/Terraform commands in this folder, and any modules and providers those commands download also get stored in this folder. You can safely delete this folder any time, and Terragrunt will recreate it as necessary. If you need to clean up a lot of these folders (e.g., after terragrunt run --all apply), you can use the following commands on Mac and Linux: Recursively find all the .terragrunt-cache folders that are children of the current folder: find . -type d -name \".terragrunt-cache\" If you are SURE you want to delete all the folders that come up in the previous command, you can recursively delete all of them as follows: find . -type d -name \".terragrunt-cache\" -prune -exec rm -rf {} \\; Also consider setting the TG_DOWNLOAD_DIR environment variable if you wish to place the cache directories somewhere else. If the reason you are clearing out your Terragrunt cache is that you are struggling with running out of disk space, consider using the Provider Cache feature to store OpenTofu/Terraform provider plugins in a shared location, as those are typically the largest files stored in the .terragrunt-cache directory. Edit page Previous Formatting Next Contributing",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:40:59.834Z"
  },
  {
    "title": "Contributing",
    "url": "https://terragrunt.gruntwork.io/docs/community/contributing/",
    "content": "Contributing Contribution GuidelinesSection titled “Contribution Guidelines” Contributions to Terragrunt are very welcome! We follow a fairly standard pull request process for contributions, subject to the following guidelines: File a GitHub issue or write an RFCSection titled “File a GitHub issue or write an RFC” Before starting any work, we recommend filing a GitHub issue in this repo. This is your chance to ask questions and get feedback from the maintainers and the community before you sink a lot of time into writing (possibly the wrong) code. If there is anything you’re unsure about, just ask! Sometimes, the scope of the feature proposal is large enough that it requires major updates to the code base to implement. In these situations, a maintainer may suggest writing up an RFC that describes the feature in more details than what can be reasonably captured in an enhancement. To write an RFC, click the RFC button in the issues tab. This will present you a template you can fill out to describe the feature you want to propose. Update the documentationSection titled “Update the documentation” We recommend updating the documentation before updating any code (see Readme Driven Development). This ensures the documentation stays up to date and allows you to think through the problem at a high level before you get lost in the weeds of coding. The documentation is built with Starlight and hosted on Vercel from the docs-starlight folder on main branch. Read this README.md to learn more about making updates to the docs. Update the testsSection titled “Update the tests” We also recommend updating the automated tests before updating any code (see Test Driven Development). That means you add or update a test case, verify that it’s failing with a clear error message, and then make the code changes to get that test to pass. This ensures the tests stay up to date and verify all the functionality in this Module, including whatever new functionality you’re adding in your contribution. Check out Developing Terragrunt for instructions on running the automated tests. Update the codeSection titled “Update the code” At this point, make your code changes and use your new test case to verify that everything is working. Check out Developing Terragrunt for instructions on how to build and run Terragrunt locally. We have a style guide for the Go programming language, in which we documented some best practices for writing Go code. Please ensure your code adheres to the guidelines outlined in the guide. Create a pull requestSection titled “Create a pull request” Create a pull request with your changes. Please make sure to include the following: A description of the change, including a link to your GitHub issue. The output of your automated test run, preferably in a GitHub Gist. We cannot run automated tests for pull requests automatically due to security concerns, so we need you to manually provide this test output so we can verify that everything is working. Any notes on backwards incompatibility. Merge and releaseSection titled “Merge and release” The maintainers for this repo will review your code and provide feedback. If everything looks good, they will merge the code and release a new version, which you’ll be able to find in the releases page. Developing TerragruntSection titled “Developing Terragrunt” Running locallySection titled “Running locally” To run Terragrunt locally, use the go run command: go run main.go plan Testing on WindowsSection titled “Testing on Windows” Running tests on Windows is currently limited. Not all tests pass reliably, and additional configuration is required for proper functionality. Specifically: Long file paths must be enabled on Windows. A Bash shell (e.g., via Git Bash or WSL) must be available in the environment. For setup instructions and requirements, .github/scripts/windows-setup.ps1. DependenciesSection titled “Dependencies” Terragrunt uses go modules (read more about the modules system in the official wiki). This means that dependencies are automatically installed when you use any go command that compiles the code (build, run, test, etc.). LintingSection titled “Linting” Terragrunt uses golangci-lint to lint the golang code in the codebase. This is a helpful form of static analysis that can catch common bugs and issues related to performance, style and maintainability. We use the linter as a guide to learn about how we can improve the Terragrunt codebase. We do not enforce 100% compliance with the linter. If you believe that an error thrown by the linter is irrelevant, use the documentation on false-positives to suppress that error, along with an explanation of why you believe the error is a false positive. If you feel like the linter is missing a check that would be useful for improving the code quality of Terragrunt, please open an issue to discuss it, then open a pull request to add the check. There are two lint configurations currently in use: Default linter This is the default configuration that is used when running golangci-lint run. The configuration for this lint is defined in the .golangci.yml file. These lints must pass before any code is merged into the main branch. Strict linter This is the more strict configuration that is used to check for additional issues in pull requests. This configuration is defined in the .strict.golanci.yml file. These lints do not have to pass before code is merged into the main branch, but the results are useful to look at to improve code quality. Default linterSection titled “Default linter” Before any tests run in our continuous integration suite, they must pass the default linter. This is to ensure an acceptable floor for code quality in the codebase. To run the default linter directly, use: golangci-lint run There’s also a Makefile recipe that runs the default linter: make run-lint If possible, you are advised to integrate the linter into your code editor to get immediate feedback as edit Terragrunt code. Strict linterSection titled “Strict linter” To run the strict linter, use: golangci-lint run -c .strict.golangci.yml It’s generally not practical to run the strict linter on the entire codebase, as it’s very strict and will likely produce a lot of errors. Instead, you can run it on the files that have changed with respect to the main branch. You can do that like so: golangci-lint run -c .strict.golangci.yml --new-from-rev origin/main ./... This is basically what the run-strict-lint Makefile recipe does: make run-strict-lint In our continuous integration suite, we run the strict linter on the files that have changed with respect to the main branch, and very intentionally do not enforce that the lints pass. We pay more attention to the results when the lint fails for a pull request that are changing few small files, as those are much more likely to pass. In those cases, maintainers will review the results, and may suggest changes that are necessary to improve code quality if they believe the cost of implementing the changes is low. MarkdownlintSection titled “Markdownlint” In addition to the golang linter, we also use markdownlint to lint the markdown files in the codebase. This is to ensure that the documentation is consistent and easy to read. You’ll want to check that the markdown files are linted correctly before submitting a pull request to update the docs. You can do this by running: markdownlint \\ --disable MD013 MD024 \\ -- \\ docs Running testsSection titled “Running tests” There are multiple different kinds of tests in the Terragrunt codebase, and each serves a different purpose. Unit testsSection titled “Unit tests” These are tests that test individual functions in the codebase. They are located in the same package as the code they are testing and are suffixed *_test.go. They use a package directive that is suffixed _test of the package they test to force them to only test exported functions of that package, while residing in the same directory. The idea behind this practice is to keep the tests close to the code they are testing, and to force them to only test the public API of the package. This allows implementation details of particular functions to change without breaking tests, as long as the public API behaves the same. In general, if you are editing Terragrunt code, and there isn’t a unit test that covers the code you are updating, it’s probably a good idea to add one. If there is a unit test for the code you are updating, you should make sure that you run that test after any update to ensure that you haven’t broken anything. When possible, introduce new tests for code before you start making changes. This is a practice known as Test Driven Development. You can run the unit tests for a particular package by running: go test ./path/to/package To specifically run a single test, you can use the -run flag: go test -run TestFunctionName ./path/to/package There are many ways to customize the go test command, including using flags like -v to get more verbose output. To learn more about go testing, read the official documentation. Integration testsSection titled “Integration tests” These are tests that test integrations between multiple parts of the Terragrunt codebase, and external services. They generally invoke Terragrunt as if you were using it from the command line. These tests are located in the test directory, and are suffixed *_test.go. Often, these tests run against test fixtures, which are small Terragrunt configurations that emulate specific real-world scenarios. These test fixtures are located in the test/fixtures directory. To run the integration tests, you can use the go test command: go test ./test Note that integration tests can be slow, as they often involve running full Terragrunt commands, and that frequently involves spawning new processes. As a result, you may want to run only a subset of the tests while developing. You can do this by using the -run flag: go test -run 'TestBeginningOfFunctionName*' ./test This will run all tests that start with TestBeginningOfFunctionName. Note that some tests may require that you opt-in for them to be tested. This is because they may require access to external services that you need to authenticate with or use a specific external tool that you might not have installed. In these cases, we use golang build tags to conditionally compile the tests. You can run these tests by setting the appropriate build tag before testing. For example, AWS tests are tagged using the aws build tag. To run these tests, you can use the -tags flag set in the GOFLAGS environment variable like so: GOFLAGS='-tags=aws' go test -run 'TestAwsInitHookNoSourceWithBackend' . Depending on how you’ve configured your editor, you may need to make sure that your editor has the GOFLAGS environment variable set before starting for the best development experience: export GOFLAGS='-tags=aws'neovim . In general, we try to make sure that any test that requires a build tag is also consistently prefixed a certain way so that they can be tested independently. For example, all AWS tests are prefixed with TestAws*. Terragrunt also includes integration tests for Google Cloud Platform (GCP). These tests are prefixed with TestGcp* and are tagged with the gcp build tag. To run these tests, you can use the -tags flag set in the GOFLAGS environment variable, similar to AWS tests: GOFLAGS='-tags=gcp' go test -run 'TestGcp*' . To successfully run the GCP tests, you must set the following environment variables: GCLOUD_SERVICE_KEY: The service account JSON key used for authentication. GOOGLE_CLOUD_PROJECT or GOOGLE_PROJECT_ID: The GCP project name. GOOGLE_COMPUTE_ZONE: The compute zone name. GOOGLE_IDENTITY_EMAIL: The service account identity email. GCLOUD_SERVICE_KEY_IMPERSONATOR: (Optional) An additional service account key used in impersonation tests. Make sure these environment variables are set in your shell before running the tests. For example: export GCLOUD_SERVICE_KEY=\"/path/to/service-account.json\"export GOOGLE_CLOUD_PROJECT=\"your-gcp-project\"export GOOGLE_COMPUTE_ZONE=\"us-central1-a\"export GOOGLE_IDENTITY_EMAIL=\"service-account@your-gcp-project.iam.gserviceaccount.com\"export GCLOUD_SERVICE_KEY_IMPERSONATOR=\"/path/to/impersonator-service-account.json\" The service account used for GCP tests must have the following IAM roles in your GCP project: roles/storage.admin roles/iam.serviceAccountTokenCreator You can assign these roles using the following gcloud commands: gcloud projects add-iam-policy-binding <gcp-project> \\ --member=\"<service-account>\" \\ --role=\"roles/storage.admin\" gcloud projects add-iam-policy-binding <gcp-project> \\ --member=\"<service-account>\" \\ --role=\"roles/iam.serviceAccountTokenCreator\" Race testsSection titled “Race tests” Given that Terragrunt is a tool that frequently involves concurrently running multiple things at once, there’s always a risk for race conditions to occur. As such, there are dedicated tests that are run with the -race flag in CI to use golang’s built-in tooling for identifying race conditions. In general, when encountering a bug caused by a race condition in the wild, we endeavor to write a test for it, and add it to the ./test/race_test.go file to avoid regressions in the future. If you want to make sure that new code you are writing doesn’t introduce a race condition, add a test for it in the race_test.go file. We can do a better job of finding candidates for additional testing here, so if you are interested in helping out, please open an issue to discuss it. The convention we use for race tests is to prefix them with WithRacing. The Terragrunt Continuous Integration workflow will run these tests with the -race flag as part of the test suite. Benchmark testsSection titled “Benchmark tests” Benchmark tests are tests that are run with the -bench flag to the go test command. They are used to measure the performance of a particular function or set of functions. You can find them by looking for tests that start with Benchmark* instead of Test* in the codebase. For more information on Terragrunt performance, read the dedicated Performance documentation. In general, we have inadequate benchmark testing in the Terragrunt codebase, and want to improve this. If you are interested in helping out, please open an issue to discuss it. Prior to the release of Terragrunt 1.0, we will have a concerted effort to improve the benchmark testing in the codebase. Continuous IntegrationSection titled “Continuous Integration” Currently, all of the testing mentioned above is run automatically as part of our continuous integration suite in CircleCI. This has caused some issues in the past, as many contributors don’t have CircleCI accounts, and it can be frustrating to be unable to replicate the automated tests that we run for contributors when they submit a pull request. We acknowledge this issue, and are working on a solution to make it easier for contributors to run the tests for this project themselves. This will involve moving the tests to GitHub Actions, which will allow contributors to run the tests on their own fork of the project, without needing to sign up for a new account. This work will be completed prior to the release of Terragrunt 1.0. Debug loggingSection titled “Debug logging” If you set the TG_DEBUG_INPUTS environment variable to “true”, the stack trace for any error will be printed to stdout when you run the app. Additionally, newer features introduced in v0.19.0 (such as locals and dependency blocks) can output more verbose logging if you set the TG_LOG environment variable to debug. Error handlingSection titled “Error handling” In this project, we try to ensure that: Every error has a stacktrace. This makes debugging easier. Every error generated by our own code (as opposed to errors from Go built-in functions or errors from 3rd party libraries) has a custom type. This makes error handling more precise, as we can decide to handle different types of errors differently. To accomplish these two goals, we have created an errors package that has several helper methods, such as errors.New(err error), which wraps the given error in an Error object that contains a stacktrace. Under the hood, the errors package is using the go-errors library, but this may change in the future, so the rest of the code should not depend on go-errors directly. Here is how the errors package should be used: Any time you want to create your own error, create a custom type for it, and when instantiating that type, wrap it with a call to errors.New. That way, any time you call a method defined in the Terragrunt code, you know the error it returns already has a stacktrace and you don’t have to wrap it yourself. Any time you get back an error object from a function built into Go or a 3rd party library, immediately wrap it with errors.New. This gives us a stacktrace as close to the source as possible. If you need to get back the underlying error, you can use the errors.IsError and errors.Unwrap functions. FormattingSection titled “Formatting” Every source file in this project should be formatted with go fmt. There are few helper scripts and targets in the Makefile that can help with this (mostly taken from the terraform repo when it was MPL licensed): make fmtcheck Checks to see if all source files are formatted. Exits 1 if there are unformatted files. make fmt Formats all source files with gofmt. make install-pre-commit-hook Installs a git pre-commit hook that will run all of the source files through gofmt. To ensure that your changes get properly formatted, please install the git pre-commit hook with make install-pre-commit-hook. Development ContainersSection titled “Development Containers” Development Containers enable you to capture an entire development environment within a container. They can specify the required binaries, languages, extensions, and settings for a project. They can even define commands to run when entering the container. The Dev Container spec is met by a number of supporting tools and editors, but here we demonstrate a Visual Studio Code example for contributing to the Terragrunt project. Install the Dev Containers VSCode extension. Create a .devcontainer.json file at the project root. The example below: Launches a container configured with the appropriate version of Go. Integrates golangci-lint (the standard Golang linter) into the editor. Installs the markdownlint extension with specific rules disabled: MD013 MD024 Includes Node.js and OpenTofu. Starts the Astro Starlight docs upon container startup. { \"name\": \"Terragrunt Contributing IDE\", \"image\": \"mcr.microsoft.com/devcontainers/go:1-1.23-bookworm\", \"runArgs\": [\"--network=host\"], \"customizations\": { \"vscode\": { \"settings\": { \"go.lintTool\": \"golangci-lint\", \"go.lintFlags\": [ \"--fast\" ], \"markdownlint.config\": { \"MD013\": false, \"MD024\": false } }, \"extensions\": [ \"davidanson.vscode-markdownlint\" ] } }, \"features\": { \"ghcr.io/devcontainers/features/node:1\": {}, \"ghcr.io/robbert229/devcontainer-features/opentofu:1\": {} }, \"postCreateCommand\": \"cd docs-starlight && npm install && npm run dev\"} Open the project as a VSCode workspace and select Reopen in Container when prompted. Tip If you miss the prompt, just open the command palette and run:Dev Containers: Rebuild and Reopen in Container Terragrunt ReleasesSection titled “Terragrunt Releases” Terragrunt releases follow semantic versioning guidelines (semver). Note that as of 2024/10/17, Terragrunt is still pre-1.0, so breaking changes may still be introduced in minor releases. We will try to minimize these changes as much as possible, but they may still happen. Once 1.0 is released, Terragrunt backwards compatibility will be guaranteed for all minor releases. This documentation should be updated at that time to reflect the new policy. If it has not, please file a bug report. When to Cut a New ReleaseSection titled “When to Cut a New Release” While Terragrunt is still pre-1.0, maintainers will cut a new release whenever a new feature is added or a bug is fixed. Maintainers will exercise their best judgment to determine when a new release is necessary, and bias towards cutting a new release as frequently as possible when in doubt. Post-1.0, maintainers will slow down the release cadence using a different release cadence. This documentation will be updated at that time to reflect the new policy. How to Create a New ReleaseSection titled “How to Create a New Release” To release a new version of Terragrunt, go to the Releases Page and cut a new release off the main branch. Ensure that the new release uses the Set as a pre-release checkbox initially. The CircleCI job for this repo has been configured to: Automatically detect new tags. Build binaries for every OS using that tag as a version number. Upload the binaries to the release in GitHub. See .circleci/config.yml for details. Follow the CircleCI job to ensure that the binaries are uploaded correctly. Once the job is successful, go back to the release, uncheck the Set as a pre-release checkbox and check the Set as the latest release checkbox. Pre-releasesSection titled “Pre-releases” Occasionally, Terragrunt maintainers will cut a pre-release to get feedback on the UI/UX for a new feature or to test it in the wild before making it generally available. These releases are generally cut off a feature branch, in order to keep the main branch stable and releasable at all times. Pre-releases are tagged with a pre-release name that looks like the following: alpha2025022501, etc. with the following information: Channel: e.g. alpha (indicating the stability of the release) The alpha channel has the following meaning in Terragrunt: alpha: This release is recommended for testing in non-production environments only. It is intended for testing out new features with stakeholders external to Gruntwork before a general release. At the moment, this is really the only channel we need. In the future, we might adjust this to include more channels, such as beta, etc. Date: e.g. 20250225 (indicating the date the release was cut without dashes or slashes) Incremental number: e.g. 01 (indicating the number of pre-releases cut on that day) This pre-release system is subject to change, and maintainers will update this documentation to reflect any changes. The current plan for how maintainers are going to handle pre-releases after 1.0 is that: Pre-releases for the alpha channel will continue to be cut from feature branches, and use the same naming convention as before. Pre-releases for the rc channel will be cut from the main branch, and use a naming convention that looks like v1.0.0-rc2025022501, etc. Release candidates in the rc channel will undergo more thorough testing, both automated and manual. Maintainers will have a workflow to promote release candidates to general availability, and this documentation will be updated to reflect that workflow at the time of the 1.0 release. Edit page Previous Terragrunt Cache Next Support",
    "section": "community",
    "lastUpdated": "2025-10-23T16:40:59.877Z"
  },
  {
    "title": "Support",
    "url": "https://terragrunt.gruntwork.io/docs/community/support/",
    "content": "Support Github DiscussionsSection titled “Github Discussions” Search Terragrunt GitHub Discussions for existing questions or ask your own. Gruntwork GitHub Discussions is a good place for general discussions and questions about Gruntwork tools. Join the Discord CommunitySection titled “Join the Discord Community” Join the Gruntwork Discord Community to chat with maintainers and members of the community. Github IssuesSection titled “Github Issues” Read through existing issues or post a new one. Github issues is a good place to: Report a bug Ask for improvements Propose a change to how Terragrunt works Start contributing by solving issues Commercial supportSection titled “Commercial support” Does your company rely on Terragrunt in production? If so, you can get commercial support directly from Gruntwork, the creators of Terragrunt! Check out the Gruntwork Support Page for more details. Edit page Previous Contributing Next License",
    "section": "community",
    "lastUpdated": "2025-10-23T16:40:59.930Z"
  },
  {
    "title": "License",
    "url": "https://terragrunt.gruntwork.io/docs/community/license/",
    "content": "License This code is released under the MIT License. See LICENSE.txt. Edit page Previous Support Next Debugging",
    "section": "community",
    "lastUpdated": "2025-10-23T16:40:59.977Z"
  },
  {
    "title": "Debugging",
    "url": "https://terragrunt.gruntwork.io/docs/troubleshooting/debugging/",
    "content": "Debugging Terragrunt and OpenTofu/Terraform usually play well together in helping you write DRY, reusable infrastructure. But how do we figure out what went wrong in the rare case that they don’t play well? Terragrunt provides a way to configure logging level through the --log-level command flag. Additionally, Terragrunt provides --inputs-debug, that can be used to generate terragrunt-debug.tfvars.json. For example you could use it like this to debug an apply that’s producing unexpected output: terragrunt apply --log-level debug --inputs-debug Running this command will do two things for you: Output a file named terragrunt-debug.tfvars.json to your terragrunt working directory (the same one containing your terragrunt.hcl) Print instructions on how to invoke tofu/terraform against the generated file to reproduce exactly the same tofu/terraform output as you saw when invoking terragrunt. This will help you to determine where the problem’s root cause lies. Using those features is helpful when you want determine which of these three major areas is the root cause of your problem: Misconfiguration of your infrastructure code. An error in terragrunt. An error in tofu/terraform. Let’s run through a few use-cases. Debugging OpenTofu/Terraform BehaviorSection titled “Debugging OpenTofu/Terraform Behavior” Consider this file structure for a fictional production environment where we have configured an application to deploy as many tasks as there are minimum number of machines in some cluster. Directorylive Directoryprod Directoryapp vars.tfmain.tfoutputs.tfterragrunt.hclDirectoryecs-cluster outputs.tf The files contain this text (app/main.tf and ecs-cluster/outputs.tf omitted for brevity): variable \"image_id\" { type = string}variable \"num_tasks\" { type = number}# app/outputs.tfoutput \"task_ids\" { value = module.app_infra_module.task_ids}# app/terragrunt.hcllocals { image_id = \"acme/myapp:1\"}dependency \"cluster\" { config_path = \"../ecs-cluster\"}inputs = { image_id = locals.image_id num_tasks = dependency.cluster.outputs.cluster_min_size} You perform a terragrunt apply, and find that outputs.task_ids has 7 elements, but you know that the cluster only has 4 VMs in it! What’s happening? Let’s figure it out. Run this: terragrunt apply --log-level debug --inputs-debug After applying, you will see this output on standard error [terragrunt] Variables passed to tofu/terraform are located in \"~/live/prod/app/terragrunt-debug.tfvars\"[terragrunt] Run this command to replicate how tofu/terraform was invoked:[terragrunt] tofu/terraform apply -var-file=\"~/live/prod/app/terragrunt-debug.tfvars.json\" \"~/live/prod/app\" Well we may have to do all that, but first let’s just take a look at terragrunt-debug.tfvars.json { \"image_id\": \"acme/myapp:1\", \"num_tasks\": 7} So this gives us the clue — we expected num_tasks to be 4, not 7! Looking into ecs-cluster/outputs.tf we see this text: output \"cluster_min_size\" { value = module.my_cluster_module.cluster_max_size} Oops! It says max when it should be min. If we fix ecs-cluster/outputs.tf we should be golden! We fix the problem in time to take a nice afternoon walk in the sun. In this example we’ve seen how debug options can help us root cause issues in dependency and local variable resolution. Additional OpenTofu/Terraform DebuggingSection titled “Additional OpenTofu/Terraform Debugging” If you’re still having trouble, you may want to try adjusting the TF_LOG environment variables to instruct OpenTofu/Terraform to emit more detailed logs. You can learn more about OpenTofu/Terraform debug logging here. Edit page Previous License Next OpenTelemetry",
    "section": "troubleshooting",
    "lastUpdated": "2025-10-23T16:41:00.040Z"
  },
  {
    "title": "OpenTelemetry",
    "url": "https://terragrunt.gruntwork.io/docs/troubleshooting/open-telemetry/",
    "content": "OpenTelemetry Terragrunt can be configured to emit telemetry in OpenTelemetry format, traces and metrics. OpenTelemetry tracing is frequently used in Terragrunt to collect traces to analyze performance. For more details on analyzing and optimizing performance, read the dedicated Performance documentation. High-level overviewSection titled “High-level overview” The following are concepts and technologies that are important to be aware of when using OpenTelemetry with Terragrunt. OpenTelemetry Traces Metrics Jaeger Tracing configuration: TG_TELEMETRY_TRACE_EXPORTER - traces exporter type to be used. Currently supported values are: none - no trace exporting, default value. console - to export traces to console as JSON otlpHttp - to export traces to an OpenTelemetry collector over HTTP otlptracehttp otlpGrpc - to export traces over gRPC otlptracegrpc http - to export traces to a custom HTTP endpoint using otlptracehttp TG_TELEMETRY_TRACE_EXPORTER_HTTP_ENDPOINT - in case of http exporter, this is the endpoint to which traces will be sent. TG_TELEMETRY_TRACE_EXPORTER_INSECURE_ENDPOINT - if set to true, the exporter will not validate the server’s certificate, helpful for local traces collection. TRACEPARENT - if set, the value will be used as a parent trace context, format TRACEPARENT=00-<hex_trace_id>-<hex_span_id>-<trace_flags>, example: TRACEPARENT=00-xxx-yyy-01 Metrics configuration: TG_TELEMETRY_METRIC_EXPORTER - metrics exporter type to be used. Currently supported values are: none - no metric exporting, default value. console - write metrics to console as JSONs. otlpHttp - export metrics to an OpenTelemetry collector over HTTP otlpmetrichttp grpcHttp - export metrics to an OpenTelemetry collector over gRPC otlpmetricgrpc TG_TELEMETRY_METRIC_EXPORTER_INSECURE_ENDPOINT - if set to true, the exporter will not validate the server’s certificate, helpful for local metrics collection. Example configurations for trace collectionSection titled “Example configurations for trace collection” Collection of examples how to configure Terragrunt to emit traces and metrics in OpenTelemetry format. Example traces collection with JaegerSection titled “Example traces collection with Jaeger” Start a Jaeger instance with docker: docker run --rm --name jaeger -e COLLECTOR_OTLP_ENABLED=true -p 16686:16686 -p 4317:4317 -p 4318:4318 jaegertracing/all-in-one:1.54.0 Verify that UI is available at http://localhost:16686/ Define environment variables for Terragrunt to report traces to Jaeger: export TG_TELEMETRY_TRACE_EXPORTER=httpexport TG_TELEMETRY_TRACE_EXPORTER_HTTP_ENDPOINT=localhost:4318export TG_TELEMETRY_TRACE_EXPORTER_INSECURE_ENDPOINT=true Run terragrunt Verify that traces are available in Jaeger UI Configurations to collect traces in Grafana TempoSection titled “Configurations to collect traces in Grafana Tempo” Start a Grafana Tempo instance example Define environment variables for Terragrunt to report traces to Tempo: export TG_TELEMETRY_TRACE_EXPORTER=otlpHttp# Replace with your tempo instanceexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318export TG_TELEMETRY_TRACE_EXPORTER_INSECURE_ENDPOINT=true Run terragrunt Check for traces in Tempo UI for service “terragrunt” Example traces collection in consoleSection titled “Example traces collection in console” Set env variable to enable telemetry: export TG_TELEMETRY_TRACE_EXPORTER=console Run terragrunt Check produced traces in console, like: {\"Name\":\"run_bash\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"f91587247524593b\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:30.564217484Z\",\"EndTime\":\"2024-02-08T12:32:31.570666395Z\",\"Attributes\":[{\"Key\":\"command\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"bash\"}},{\"Key\":\"args\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"[-c sleep 1]\"}},{\"Key\":\"dir\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod2\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":0,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}}{\"Name\":\"parse_config_file\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"d2823047fb469bdf\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:30.380054129Z\",\"EndTime\":\"2024-02-08T12:32:31.570899286Z\",\"Attributes\":[{\"Key\":\"config_path\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod2/terragrunt.hcl\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":0,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}}{\"Name\":\"run_terraform\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"152d873a18559f07\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:31.57161757Z\",\"EndTime\":\"2024-02-08T12:32:31.688157882Z\",\"Attributes\":[{\"Key\":\"command\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"tofu\"}},{\"Key\":\"args\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"[init]\"}},{\"Key\":\"dir\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod2\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":0,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}}{\"Name\":\"run_terraform\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"29341bdb65f66b1e\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:31.688240673Z\",\"EndTime\":\"2024-02-08T12:32:31.793377642Z\",\"Attributes\":[{\"Key\":\"command\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"tofu\"}},{\"Key\":\"args\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"[apply -auto-approve -input=false]\"}},{\"Key\":\"dir\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod2\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":0,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}}{\"Name\":\"run_module\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"8a01522bc65e0f1b\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:30.290680776Z\",\"EndTime\":\"2024-02-08T12:32:31.793392803Z\",\"Attributes\":[{\"Key\":\"path\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod2\"}},{\"Key\":\"terraformCommand\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"apply\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":0,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}}{\"Name\":\"run --all apply\",\"SpanContext\":{\"TraceID\":\"bdf3cb9078706b7f0b4f1d92428eedc0\",\"SpanID\":\"b0b007770f852066\",\"TraceFlags\":\"01\",\"TraceState\":\"\",\"Remote\":false},\"Parent\":{\"TraceID\":\"00000000000000000000000000000000\",\"SpanID\":\"0000000000000000\",\"TraceFlags\":\"00\",\"TraceState\":\"\",\"Remote\":false},\"SpanKind\":1,\"StartTime\":\"2024-02-08T12:32:26.388519019Z\",\"EndTime\":\"2024-02-08T12:32:31.793405603Z\",\"Attributes\":[{\"Key\":\"terraformCommand\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"apply\"}},{\"Key\":\"args\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"[apply]\"}},{\"Key\":\"dir\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test\"}}],\"Events\":null,\"Links\":null,\"Status\":{\"Code\":\"Unset\",\"Description\":\"\"},\"DroppedAttributes\":0,\"DroppedEvents\":0,\"DroppedLinks\":0,\"ChildSpanCount\":28,\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-29-g66bfa07b756e-dirty\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.0\"}}],\"InstrumentationLibrary\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"}} Collection of metrics with OpenTelemetry collector and PrometheusSection titled “Collection of metrics with OpenTelemetry collector and Prometheus” Start OpenTelemetry collector with Prometheus receiver. Example setup through docker-compose.yml: version: '3'services: otel-collector: image: otel/opentelemetry-collector:0.94.0 volumes: - ./otel-collector-config.yaml:/etc/otelcol/config.yaml ports: - \"4317:4317\" # OTLP gRPC receiver - \"4318:4318\" # OTLP HTTP receiver - \"8889:8889\" # Prometheus exporter prometheus: image: prom/prometheus:v2.45.3 volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml ports: - \"9090:9090\" depends_on: - otel-collector OpenTelemetry collector configuration otel-collector-config.yaml: receivers: otlp: protocols: grpc: http:processors: batch:exporters: prometheus: endpoint: \"0.0.0.0:8889\" # Prometheus exporter endpointservice: pipelines: metrics: receivers: [otlp] processors: [batch] exporters: [prometheus] Prometheus configuration file prometheus.yml: global: scrape_interval: 15sscrape_configs: - job_name: 'opentelemetry' scrape_interval: 5s static_configs: - targets: ['otel-collector:8889'] Confirm that Prometheus is available at http://localhost:9090/ Define environment variables for Terragrunt to report metrics to OpenTelemetry collector: export TG_TELEMETRY_METRIC_EXPORTER=grpcHttpexport TG_TELEMETRY_METRIC_EXPORTER_INSECURE_ENDPOINT=trueexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 Run terragrunt Verify that metrics are available in Prometheus UI Example configuration to export metrics to console: Set env variable to enable telemetry: export TG_TELEMETRY_METRIC_EXPORTER=console Run terragrunt In output will be printed messages like: {\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-41-g7185318bb11b\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.1\"}}],\"ScopeMetrics\":[]}{\"Resource\":[{\"Key\":\"service.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"terragrunt\"}},{\"Key\":\"service.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"v0.55.0-41-g7185318bb11b\"}},{\"Key\":\"telemetry.sdk.language\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"go\"}},{\"Key\":\"telemetry.sdk.name\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"opentelemetry\"}},{\"Key\":\"telemetry.sdk.version\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"1.23.1\"}}],\"ScopeMetrics\":[{\"Scope\":{\"Name\":\"terragrunt\",\"Version\":\"\",\"SchemaURL\":\"\"},\"Metrics\":[{\"Name\":\"run_bash_duration\",\"Description\":\"\",\"Unit\":\"\",\"Data\":{\"DataPoints\":[{\"Attributes\":[{\"Key\":\"args\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"[-c sleep 2]\"}},{\"Key\":\"command\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"bash\"}},{\"Key\":\"dir\",\"Value\":{\"Type\":\"STRING\",\"Value\":\"/projects/gruntwork/terragrunt-tests/trace-test/mod3\"}}],\"StartTime\":\"2024-02-12T14:38:14.85578658Z\",\"Time\":\"2024-02-12T14:38:17.853165589Z\",\"Count\":1,\"Bounds\":[0,5,10,25,50,75,100,250,500,750,1000,2500,5000,7500,10000],\"BucketCounts\":[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\"Min\":2005,\"Max\":2005,\"Sum\":2005}],\"Temporality\":\"CumulativeTemporality\"}},{\"Name\":\"run_bash_success_count\",\"Description\":\"\",\"Unit\":\"\",\"Data\":{\"DataPoints\":[{\"Attributes\":[],\"StartTime\":\"2024-02-12T14:38:16.860878555Z\",\"Time\":\"2024-02-12T14:38:17.853169359Z\",\"Value\":1}],\"Temporality\":\"CumulativeTemporality\",\"IsMonotonic\":true}}]}]} Edit page Previous Debugging Next Performance",
    "section": "troubleshooting",
    "lastUpdated": "2025-10-23T16:41:00.098Z"
  },
  {
    "title": "Performance",
    "url": "https://terragrunt.gruntwork.io/docs/troubleshooting/performance/",
    "content": "Performance Easy WinsSection titled “Easy Wins” Normally, it’s best practice to start by measuring performance before making any changes. This allows you to understand the impact of your changes, and to identify areas for improvement. However, given the nature of the problems that Terragrunt solves, there are some obvious wins that you can make without measuring performance, if you’re aware of the tradeoffs. Provider CacheSection titled “Provider Cache” One of the most expensive things that OpenTofu/Terraform does, from a bandwidth and disk utilization perspective, is download and install providers. These are large binary files that are downloaded from the internet, and not cached across units by default. You can significantly reduce the amount of time taken by Terragrunt runs by enabling the provider cache server, like this: terragrunt run --all plan --provider-cache Provider Cache - GotchasSection titled “Provider Cache - Gotchas” The provider cache server is a single server that is used by all Terragrunt runs being performed in a given Terragrunt invocation. You will see the most benefit if you are using it in a command that will perform multiple OpenTofu/Terraform operations, like with the --all flag and the --graph flag. When performing individual runs, like terragrunt plan, the provider cache server can be a net negative to performance, because starting and stopping the server might add more overhead than just downloading the providers. Whether this is the case depends on many factors, including network speed, the number of providers being downloaded, and whether or not the providers are already cached in the Terragrunt provider cache. When in doubt, measure the performance before and after enabling the provider cache server to see if it’s a net win for your use case. We are coordinating with the OpenTofu team to improve the behavior of concurrent access to provider cache, so that this flag will be unnecessary for most users in the future. See #1939 for more details. Fetching Output From StateSection titled “Fetching Output From State” Under the hood, Terragrunt dependency blocks leverage the OpenTofu/Terraform output -json command to fetch outputs from one unit and leverage them in another. The OpenTofu/Terraform output -json command does a bit more work than simply fetching output values from state, and a significant portion of that slowdown is loading providers, which it doesn’t really need in most cases. You can significantly improve the performance of dependency blocks by using the --dependency-fetch-output-from-state flag. When the flag is set, Terragrunt will directly fetch the backend state file from S3 and parse it directly, avoiding any overhead incurred by calling the output -json command. For example: terragrunt run --all plan --dependency-fetch-output-from-state Fetching Output From State - GotchasSection titled “Fetching Output From State - Gotchas” The first thing you need to be aware of when considering usage of the --dependency-fetch-output-from-state flag is that it only works for S3 backends. If you are using a different backend, this flag won’t do anything. Next, you should be aware that there is no guarantee that OpenTofu/Terraform will maintain the existing schema of their state files, so there is also no guarantee that the flag will work as expected in future versions of OpenTofu/Terraform. We are coordinating with the OpenTofu team to improve the performance of the output command, and we hope that this flag will be unnecessary for most users in the future. See #1549 for more details. Skip Dependency InputsSection titled “Skip Dependency Inputs” Terragrunt dependency blocks allow reading inputs directly from other dependencies. However, the mechanism required to support this capability introduces performance overhead during Terragrunt operations. Due to this performance impact, using this feature is heavily discouraged. The reason for this is that Terragrunt needs to recursively read inputs from dependencies, and their dependencies, all the way down to the ancestral Terragrunt unit they all have in common to be able to expose the inputs to the dependent unit from its direct dependency (as the inputs from the direct dependency might be derived from a dependency of the direct dependency). You can mitigate this performance penalty by using the skip-dependencies-inputs strict control. Enabling this strict control will activate a breaking change in Terragrunt behavior, making it so that dependency blocks no longer parse dependencies of dependencies, which is required to read inputs from direct dependencies. In the future, we will be removing this capability from Terragrunt, so you won’t need to use this strict control. Skip Dependency Inputs - GotchasSection titled “Skip Dependency Inputs - Gotchas” In general, you should use this strict control. The only reason not to use it is if you are using dependency blocks to read inputs from dependencies, which you should stop doing as soon as possible. Measuring PerformanceSection titled “Measuring Performance” Before diving into any particular performance optimization, it’s important to first measure performance, and to make sure that you measure performance after any changes so that you understand the impact of your changes. To measure performance, you can use multiple tools, depending on your role. End UserSection titled “End User” As an end user, you’re advised to use the following tools to get a better understanding of the performance of Terragrunt. OpenTelemetrySection titled “OpenTelemetry” Use OpenTelemetry to collect traces from Terragrunt runs so that you can analyze the performance of individual operations when using Terragrunt. This can be useful both to identify bottlenecks in Terragrunt, and to understand when performance changes can be attributed to integrations with other tools, like OpenTofu or Terraform. Benchmark UsageSection titled “Benchmark Usage” Use benchmarking tools like Hyperfine to run benchmarks of your Terragrunt usage to compare the performance of different versions of Terragrunt, or with different configurations. You can use configurations like the --warmup flag to do some warmup runs before the actual benchmarking. This is useful to get a more accurate measurement of the performance of Terragrunt with cache populated, etc. Here’s an example of how to use Hyperfine to benchmark the performance of Terragrunt with two different configurations: hyperfine -w 3 -r 5 'terragrunt run --all plan' 'terragrunt run --all plan --dependency-fetch-output-from-state' Terragrunt DeveloperSection titled “Terragrunt Developer” As a Terragrunt developer, you’re advised to use the following tools to improve the performance of Terragrunt when improving the codebase. Benchmark TestsSection titled “Benchmark Tests” Use Benchmark tests to measure the performance of particular subroutines in Terragrunt. These benchmarks give you a good indication of the performance of a particular part of Terragrunt, and can help you identify areas for improvement. You can run benchmark tests like this: go test -bench=BenchmarkSomeFunction You can also run benchmarks with different configurations, like the following for getting memory allocation information as well: go test -bench=BenchmarkSomeFunction -benchmem You can learn more about benchmarking in Go by reading the official documentation. ProfilingSection titled “Profiling” Use profiling tools like pprof to get a more detailed view of the performance of Terragrunt. For example, you could use the following command to profile a particular test: go test -run 'SomeTest' -cpuprofile=cpu.prof -memprofile=mem.prof You can then use the go tool pprof command to analyze the profile data: go tool pprof cpu.prof It can be helpful to use the web interface to view the profile data using flame graphs, etc. go tool pprof -http=:8080 cpu.prof You can learn more about profiling in Go by reading the official documentation. Edit page Previous OpenTelemetry Next Migrating from root terragrunt.hcl",
    "section": "troubleshooting",
    "lastUpdated": "2025-10-23T16:41:00.140Z"
  },
  {
    "title": "Migrating from root terragrunt.hcl",
    "url": "https://terragrunt.gruntwork.io/docs/migrate/migrating-from-root-terragrunt-hcl/",
    "content": "Migrating from root terragrunt.hcl ProblemSection titled “Problem” The recommended best practice for Terragrunt usage was previously that users defined two types of terragrunt.hcl files for any significantly large code base: A root terragrunt.hcl file that defined the Terragrunt configuration common to all units in the code base. Child terragrunt.hcl files that defined the Terragrunt configuration specific to each unit in the code base. This was a simple pattern that made it very obvious what these files were used for (Terragrunt), and certain Terragrunt features (like find_in_parent_folders) assumed this default structure. Over time, this caused confusion for users of Terragrunt, however. See #3181 for an example of the confusion this has caused. At the end of the day, from a functional perspective, it doesn’t actually help users to have the root configuration named terragrunt.hcl. It makes it more confusing to determine what is shared configuration and what is configuration for a unit. It also complicates Terragrunt usage, as commands like run --all need to be run from a directory where all child directories will be terragrunt.hcl files corresponding to units that need to be run. Recommended SolutionSection titled “Recommended Solution” To simplify Terragrunt usage and make it more clear what the root configuration is, it is now recommended that users rename the root terragrunt.hcl file to something else (e.g. root.hcl). This will simplify Terragrunt usage, as you will no longer need to carefully avoid running Terragrunt commands in a way that might make it think the root terragrunt.hcl file is unit configuration, and it will make it more obvious to users what is and isn’t a unit. Note that in addition to renaming the root terragrunt.hcl file, you will also need to update any Terragrunt configurations that assume the root configuration will be named terragrunt.hcl. The most common example of this would be usage of find_in_parent_folders without any arguments. By default, this will look for a terragrunt.hcl file, so you will need to update this to look for the new root configuration file name. e.g. include \"root\" { path = find_in_parent_folders()} To: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} Additional ConsiderationsSection titled “Additional Considerations” If you use Scaffold and Catalog, you may need to use additional flags to control how new units are generated. It was previously a safe assumption that most users would leverage a root terragrunt.hcl file, and thus, the default behavior was to generate a new unit that would look for a terragrunt.hcl file above it. You can use the --root-file-name and --no-include-root flags of both catalog and scaffold to explicitly control how new units are generated, and what they will look for as the root configuration file (or if they should look for one at all). e.g. terragrunt catalog To: terragrunt catalog --root-file-name root.hcl Strict ControlSection titled “Strict Control” To enforce this recommended pattern, you can also enable the root-terragrunt-hcl strict control to throw an error when Terragrunt detects that a root terragrunt.hcl file is being used. e.g. terragrunt plan To: terragrunt plan --strict-control=root-terragrunt-hcl Or: TG_STRICT_CONTROL=root-terragrunt-hcl terragrunt plan By enabling the strict control, you will also have the default behavior of scaffold and catalog commands changed to use root.hcl as the default root configuration file name if none are provided. Future BehaviorSection titled “Future Behavior” For now, warnings will be emitted when this pattern is detected in order to encourage users to change to the new pattern, but this behavior will be an explicit error in a future version of Terragrunt. Given how long this has been the standard pattern, we want to assure users that they will have a very long time to migrate to this new pattern. For the most part, using the old pattern results in very little practical difference in Terragrunt behavior, assuming Terragrunt usage is already working fine. As an explicit promise, Terragrunt will not remove support for the old pattern until at least Terragrunt 2.0, and even then, it will be a removal with a long warning period. You can take your time to migrate to the new pattern for older codebases, and are encouraged to share any feedback you have on this change so that we can make it as smooth a transition as possible for you. Frequently Asked QuestionsSection titled “Frequently Asked Questions” Could a different default value be used for find_in_parent_folders (e.g. root.hcl)?Section titled “Could a different default value be used for find_in_parent_folders (e.g. root.hcl)?” Yes, it could, but this would be a different, immediate breaking change as users might have both root.hcl files and terragrunt.hcl files in their repositories, and this could result in Terragrunt finding the wrong configuration file. It also doesn’t address a significant part of the problem, which is that the following frequently confuses new users to Terragrunt: include \"root\" { path = find_in_parent_folders()} It does not communicate what Terragrunt will look to include in parent folders, and having a hidden extra fallback value is not a good pattern for Terragrunt to encourage. Furthermore, find_in_parent_folders already supports a fallback value in the second parameter, when used. Having two different ways to specify a fallback value would be confusing. Lastly, the root include does not have any special meaning in Terragrunt, from a functional perspective, it’s merely a convention. Terragrunt users do not have to supply a root include, and users can have as many includes as they like. By requiring that users specify the root include filename explicitly, Terragrunt is encouraging users to think about what the root configuration is, and what they want in it. Is it better for the root configuration to be named root.hcl?Section titled “Is it better for the root configuration to be named root.hcl?” Naming the root file root.hcl is the recommended pattern, but it is not a requirement. Our documentation and examples are updated to reference this new pattern, and following this pattern will allow users to pattern match when writing their own configurations. Is there any name I shouldn’t use for the root configuration?Section titled “Is there any name I shouldn’t use for the root configuration?” The only names that we would strongly encourage you don’t adopt for root configuration is any name that begins with terragrunt (e.g. terragrunt.hcl or terragrunt.stack.hcl). It is not formally a reserved name, but there are currently only two special filenames in Terragrunt: terragrunt.hcl - The default configuration file name for a Terragrunt unit. terragrunt.stack.hcl - The default configuration file name for a Terragrunt stack. Using a name that begins with terragrunt could cause confusion for users, as they might expect that Terragrunt has special behavior for files with that name. Edit page Previous Performance Next Upgrading to Terragrunt 0.19.x",
    "section": "migrate",
    "lastUpdated": "2025-10-23T16:41:00.213Z"
  },
  {
    "title": "Upgrading to Terragrunt 0.19.x",
    "url": "https://terragrunt.gruntwork.io/docs/migrate/upgrading_to_terragrunt_0.19.x/",
    "content": "Upgrading to Terragrunt 0.19.x BackgroundSection titled “Background” Terraform 0.12 was released in May, 2019, and it included a few major changes: More strict rules around what can go in a .tfvars file. In particular, any variable defined in a .tfvars file that does not match a corresponding variable definition in your .tf files produces an error. A shift from HCL to HCL2 as the main syntax. This included support for first-class expressions (i.e., using variables and functions without having to wrap everything in ${...}). Before version 0.19.0, Terragrunt had you define its configuration in a terragrunt = { ... } variable in a terraform.tfvars file, but due to item (1) this no longer works with Terraform 0.12 and newer. That means we had to move to a new file format. This requires a migration, which is unfortunate, but as a nice benefit, item (2) gives us a nicer syntax and new functionality! Migration guideSection titled “Migration guide” The following sections outline the steps you may need to take in order to migrate from Terragrunt <= v0.18.x to Terragrunt 0.19.x and newer: Background Migration guide Move from terraform.tfvars to terragrunt.hcl Move input variables into inputs Use first-class expressions Check attributes vs blocks usage Rename a few built-in functions Use older Terraform Check out this PR in the terragrunt-infrastructure-live-example repo for an example of what the code changes look like. Move from terraform.tfvars to terragrunt.hclSection titled “Move from terraform.tfvars to terragrunt.hcl” Since Terraform 0.12 has more strict rules about what can go into terraform.tfvars files, you now need to move your Terragrunt configuration from terraform.tfvars to a terragrunt.hcl file, removing the terragrunt = { ... } wrapping along the way. For example, if you had the following in terraform.tfvars: terragrunt = { terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [\"apply\", \"plan\"] arguments = [\"-var\", \"foo=42\"] } } remote_state { backend = \"s3\" config = { bucket = \"my-terraform-state\" key = \"${path_relative_to_include()}/terraform.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" } }} You would migrate this to terragrunt.hcl as follows: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [\"apply\", \"plan\"] arguments = [\"-var\", \"foo=42\"] }}remote_state { backend = \"s3\" config = { bucket = \"my-terraform-state\" key = \"${path_relative_to_include()}/terraform.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} Move input variables into inputsSection titled “Move input variables into inputs” When we were using terraform.tfvars files for Terragrunt configuration, we were piggybacking on the fact that Terraform automatically loads variables from tfvars files to set variables for our modules: # Terragrunt configurationterragrunt = { terraform { # ... } remote_state { # ... }}# Input variables to set for your Terraform moduleinstance_type = \"t2.micro\"instance_count = 10 With the move to terragrunt.hcl, we no longer get this behavior for free. However, Terragrunt can simulate this behavior for you if you define your input variables by specifying inputs = { ... }: terraform { # ...}remote_state { # ...}# Input variables to set for your Terraform moduleinputs = { instance_type = \"t2.micro\" instance_count = 10} Whenever you run a Terragrunt command, such as terragrunt apply, Terragrunt will make these variables available to your Terraform module as environment variables. Use first-class expressionsSection titled “Use first-class expressions” Terraform 0.11 only allowed special behavior, such as function calls, using “interpolation syntax,” where you wrapped the code with ${...}. Terragrunt included a handful of functions you could call using interpolation syntax, but only within the terragrunt = { ... } block: terragrunt = { terraform { extra_arguments \"retry_lock\" { # Using a function within interpolation syntax commands = \"${get_terraform_commands_that_need_locking()}\" arguments = [\"-lock-timeout=20m\"] } }}# Using interpolation syntax outside of the terragrunt config did NOT work beforefoo = \"${get_env(\"FOO\", \"default\")}\" Terraform 0.12 has moved to HCL2, which has first-class support for expressions. That means you can call functions without having to wrap them in ${...}. Terragrunt embraces HCL2, and thanks to HCL2’s nice parser, that means we not only get first-class expressions, but we can also use those expressions everywhere in terragrunt.hcl! terraform { extra_arguments \"retry_lock\" { # Using a function within first-class expressions! commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] }}inputs = { # This now works with Terragrunt 0.19.x and newer! foo = get_env(\"FOO\", \"default\")} Check attributes vs blocks usageSection titled “Check attributes vs blocks usage” HCL2 is more strict about the difference between attributes: # Attributes use an equals sign before the curly bracefoo = { bar = \"baz\"} And blocks: # Blocks do not use equal signs before the curly bracefoo { bar = \"baz\"} Since Terragrunt uses HCL2, we now have to be more strict with which parts of the Terragrunt configuration are attributes and which are blocks: # terraform is a block, so make sure NOT to include an equals signterraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" # extra_arguments is a block, so make sure NOT to include an equals sign extra_arguments \"custom_vars\" { commands = [\"apply\", \"plan\"] arguments = [\"-var\", \"foo=42\"] }}# remote_state is a block, so make sure NOT to include an equals signremote_state { backend = \"s3\" # config is an attribute, so an equals sign is REQUIRED config = { bucket = \"foo\" # s3_bucket_tags is an attribute, so an equals sign is REQUIRED s3_bucket_tags = { owner = \"terragrunt integration test\" name = \"Terraform state storage\" } # dynamodb_table_tags is an attribute, so an equals sign is REQUIRED dynamodb_table_tags = { owner = \"terragrunt integration test\" name = \"Terraform lock table\" } # accesslogging_bucket_tags is an attribute, so an equals sign is REQUIRED accesslogging_bucket_tags = { owner = \"terragrunt integration test\" name = \"Terraform access log storage\" } }}# include is a block, so make sure NOT to include an equals signinclude { path = find_in_parent_folders(\"root.hcl\")}# dependencies is a block, so make sure NOT to include an equals signdependencies { paths = [\"../vpc\", \"../mysql\", \"../redis\"]}# Inputs is an attribute, so an equals sign is REQUIREDinputs = { instance_type = \"t2.micro\" instance_count = 10} Rename a few built-in functionsSection titled “Rename a few built-in functions” Two built-in functions were renamed: get_tfvars_dir() is now called get_terragrunt_dir(). get_parent_tfvars_dir() is now called get_parent_terragrunt_dir(). Make sure to make the corresponding updates in your terragrunt.hcl file! Use older TerraformSection titled “Use older Terraform” Although it is not officially supported and not tested, it is still possible to use terraform<0.12 with terragrunt >=0.19. Just install a different version of terraform into a directory of your choice outside of PATH and specify path to the binary in terragrunt.hcl as terraform_binary, plus you need to lower the version check constraint: terraform_binary = \"~/bin/terraform-v11/terraform\"terraform_version_constraint = \">= 0.11\" Edit page Previous Migrating from root terragrunt.hcl Next CLI Redesign",
    "section": "migrate",
    "lastUpdated": "2025-10-23T16:41:00.269Z"
  },
  {
    "title": "CLI Redesign",
    "url": "https://terragrunt.gruntwork.io/docs/migrate/cli-redesign/",
    "content": "CLI Redesign BackgroundSection titled “Background” As part of the redesign in #3445, several CLI adjustments have been made to improve user experience and consistency. This guide will help you understand the changes and how to adapt to them. Note that this guide is being written while deprecations are in effect, so some of the changes may not be breaking yet. We’ll do our best to keep this guide up to date as the changes are finalized. To opt in to breaking changes early, you can use the relevant strict control. The high-level changes made as a part of that RFC that require migration for current users are as follows: The terragrunt- prefix has been removed from all flags. All environment variables have had their prefixes renamed to TG_ instead of TERRAGRUNT_. A new run command has been introduced to the CLI that also handles the responsibilities of deprecated run-all and graph commands. A new backend command has been introduced to support users in interacting with backends. Infrequently used commands have been reorganized into a structure that makes it easier to find them. Arguments are no longer sent to tofu / terraform by default. Migration guideSection titled “Migration guide” All of the changes you need to make to adopt to this new CLI design involve changing how you invoke Terragrunt from the command line. Remove terragrunt- prefix from flagsSection titled “Remove terragrunt- prefix from flags” If you are currently using flags that are prefixed with terragrunt-, you will need to stop using that flag, and use a differently named one instead (usually the same exact flag with terragrunt- removed from the beginning, but not always). For example, if you are using the --terragrunt-non-interactive flag, you will need to switch to the --non-interactive flag instead. Before: terragrunt plan --terragrunt-non-interactive After: terragrunt plan --non-interactive Sometimes, the flag change might be slightly more involved than simply removing the terragrunt- prefix. For example, if you are using the --terragrunt-debug flag, you will need to switch to the --inputs-debug flag instead. Before: terragrunt plan --terragrunt-debug After: terragrunt plan --inputs-debug You can find the new flag names in the CLI reference (including the deprecated flags they replace). CLI Flag Migration TableSection titled “CLI Flag Migration Table” Below is a comprehensive mapping of old CLI flag names to their modern counterparts: Old FlagNew Flag--terragrunt-check--check--terragrunt-config--config--terragrunt-debug--inputs-debug--terragrunt-diff--diff--terragrunt-disable-bucket-update--disable-bucket-update--terragrunt-disable-command-validation--disable-command-validation--terragrunt-download-dir--download-dir--terragrunt-exclude-dir--queue-exclude-dir--terragrunt-excludes-file--queue-excludes-file--terragrunt-fail-on-state-bucket-creationremoved (no equivalent; backend provisioning is explicit)--terragrunt-fetch-dependency-output-from-state--dependency-fetch-output-from-state--terragrunt-forward-tf-stdout--tf-forward-stdout--terragrunt-hclfmt-exclude-dir--exclude-dir--terragrunt-hclfmt-file--file--terragrunt-hclfmt-stdin--stdin--terragrunt-hclvalidate-json--json--terragrunt-hclvalidate-show-config-path--show-config-path--terragrunt-iam-assume-role-duration--iam-assume-role-duration--terragrunt-iam-role--iam-assume-role--terragrunt-iam-web-identity-token--iam-assume-role-web-identity-token--terragrunt-ignore-dependency-errors--queue-ignore-errors--terragrunt-ignore-dependency-order--queue-ignore-dag-order--terragrunt-ignore-external-dependencies--queue-exclude-external--terragrunt-include-dir--queue-include-dir--terragrunt-include-external-dependencies--queue-include-external--terragrunt-json-disable-dependent-modules--disable-dependent-modules--terragrunt-json-out-dir--json-out-dir--terragrunt-json-out--out--terragrunt-log-custom-format--log-custom-format--terragrunt-log-disable--log-disable--terragrunt-log-format--log-format--terragrunt-log-level--log-level--terragrunt-log-show-abs-paths--log-show-abs-paths--terragrunt-modules-that-include--units-that-include--terragrunt-no-auto-approve--no-auto-approve--terragrunt-no-auto-init--no-auto-init--terragrunt-no-auto-retry--no-auto-retry--terragrunt-no-color--no-color--terragrunt-no-destroy-dependencies-check--no-destroy-dependencies-check--terragrunt-out-dir--out-dir--terragrunt-parallelism--parallelism--terragrunt-provider-cache-dir--provider-cache-dir--terragrunt-provider-cache-hostname--provider-cache-hostname--terragrunt-provider-cache-port--provider-cache-port--terragrunt-provider-cache-registry-names--provider-cache-registry-names--terragrunt-provider-cache-token--provider-cache-token--terragrunt-provider-cache--provider-cache--terragrunt-queue-include-units-reading--queue-include-units-reading--terragrunt-source-map--source-map--terragrunt-source-update--source-update--terragrunt-source--source--terragrunt-strict-include--queue-strict-include--terragrunt-strict-validate--strict-validate--terragrunt-tfpath--tf-path--terragrunt-use-partial-parse-config-cache--use-partial-parse-config-cache--terragrunt-working-dir--working-dir--terragrunt-non-interactive--non-interactive Update environment variablesSection titled “Update environment variables” If you are currently using environment variables to configure Terragrunt, you will need to stop using that environment variable, and use a differently named one instead (usually the same exact environment variable with TERRAGRUNT_ replaced with TG_, but not always). For example, if you are using the TERRAGRUNT_NON_INTERACTIVE environment variable, you will need to switch to the TG_NON_INTERACTIVE environment variable instead. Before: export TERRAGRUNT_NON_INTERACTIVE=true After: export TG_NON_INTERACTIVE=true Sometimes, the environment variable change might be slightly more involved than simply replacing TERRAGRUNT_ with TG_. For example, if you are using the TERRAGRUNT_DEBUG environment variable, you will need to switch to the TG_INPUTS_DEBUG environment variable instead. Before: export TERRAGRUNT_DEBUG=true After: export TG_INPUTS_DEBUG=true You can find the new environment variable names in the CLI reference (including the deprecated environment variables they replace). Use the new run commandSection titled “Use the new run command” Default behavior change (v0.88.0): Terragrunt no longer forwards unknown commands to OpenTofu/Terraform by default. If you previously ran commands like terragrunt workspace ls, use the explicit run form instead: terragrunt run -- workspace ls The run command has been introduced to the CLI to handle the responsibility currently held by the default command in Terragrunt. If you want to tell Terragrunt that what you are running is a command in the orchestrated IaC tool (OpenTofu/Terraform), you can use the run command to explicitly indicate this. For example, if you are currently using the terragrunt command to run plan, you can switch to the run plan command instead. Before: terragrunt plan After: terragrunt run plan Note that certain shortcuts will be supported out of the box, such as terragrunt plan, so you can continue to use most run commands without the run keyword, as you were doing before. For example, the following command will continue to work as expected: terragrunt plan The commands that will not receive shortcuts are OpenTofu/Terraform commands that are not recommended for usage with Terragrunt, or have a conflict with the Terragrunt CLI API. For example, the workspace command will not receive a shortcut, as you are encouraged not to use workspaces when working with Terragrunt. Terragrunt manages state isolation for you, so you don’t need to use them. If you would like to explicitly run a command that does not have a shortcut, you can use the run command to do so. We recommend separating Terragrunt flags from OpenTofu/Terraform arguments with --: terragrunt run -- workspace ls Similarly, commands like graph won’t be supported as a shortcut, as graph is a now deprecated command in the Terragrunt CLI. Supporting it as a shortcut would be misleading, so you can use the run command to run it explicitly: terragrunt run graph You might want to explicitly indicate that the flag you are using is one for OpenTofu/Terraform, and not a Terragrunt flag. To do this, you can use the -- argument to explicitly separate the Terragrunt flags from the OpenTofu/Terraform flags: terragrunt run -- apply -auto-approve This usually isn’t necessary, except when combining a complicated series of flags and arguments, which can be difficult to parse for the CLI. In addition to allowing for explicit invocation of OpenTofu/Terraform instead of using shortcuts, the run command also takes on the responsibilities of the now deprecated run-all and graph commands using flags. For example, if you are currently using the terragrunt run --all command, you can switch to the run command with the --all flag instead. Before: terragrunt run --all plan After: terragrunt run --all plan Take advantage of the new exec commandSection titled “Take advantage of the new exec command” Previously, Terragrunt only supported orchestrating the tofu and terraform binaries as the main program being executed when Terragrunt was invoked. With the introduction of the new exec command, this is no longer the case. You can now orchestrate any program you want, and integrate it with Terragrunt’s ability to fetch outputs, download OpenTofu/Terraform modules, set inputs, and more. For example, if you want to use Terragrunt to list the contents of an AWS S3 bucket, you can do the following: terragrunt exec -- bash -c 'aws s3 ls s3://$TF_VAR_bucket_name' With the following terragrunt.hcl file: inputs = { bucket_name = \"my-bucket\"} Terragrunt will load the inputs for the unit, and make them available as TF_VAR_ prefixed environment variables for the executed command. This offers a flexible way to integrate Terragrunt with other tools, besides just OpenTofu/Terraform for simple operational tasks. Use the new backend capabilitiesSection titled “Use the new backend capabilities” Previously, Terragrunt would automatically provision any backend resources defined in the remote_state block of a terragrunt.hcl file. This was a source of confusion for many users, as it was potentially performing additional actions that users did not intend without asking for it. As part of the CLI Redesign, Terragrunt now supports a dedicated backend command to handle processes involved with interacting with OpenTofu/Terraform backends. This includes the ability to bootstrap (provision) backend resources, migrate state between backend state files, and delete backend state files. In addition, the --backend-bootstrap flag has been introduced, which preserves the legacy behavior of automatically provisioning backend resources. As this flag requires explicit opt in, you will want to explicitly set this flag (or the corresponding environment variable TG_BACKEND_BOOTSTRAP to true) if you want to continue to have Terragrunt automatically provision backend resources. Before: terragrunt plan After: terragrunt plan --backend-bootstrap Use the new find and list commandsSection titled “Use the new find and list commands” The find and list commands have been introduced to help you discover configurations in your Terragrunt projects. The find command is useful when you want to perform programmatic discovery of a Terragrunt unit or configuration of that unit, and the list command is useful when you want to get a high-level overview of the Terragrunt units and configurations in your project. If you are currently using the output-module-groups command, you can switch to the find --dag --json command to get a more fine grained outlook on the nature of your Terragrunt configurations. You can also use the list --dag --tree command to get a better overview of how your units interact in the Directed Acyclic Graph (DAG) of Terragrunt units. Before: terragrunt output-module-groups{ \"Group 1\": [ \"/absolute/path/to/vpc\" ], \"Group 2\": [ \"/absolute/path/to/db\" ], \"Group 3\": [ \"/absolute/path/to/ec2\" ]} After: terragrunt find --dag --json[ { \"type\": \"unit\", \"path\": \"vpc\" }, { \"type\": \"unit\", \"path\": \"db\" }, { \"type\": \"unit\", \"path\": \"ec2\" }] terragrunt list --dag --tree.╰── vpc ├── db │ ╰── ec2 ╰── ec2 You might be wondering why these commands no longer reference “Groups”. That’s because the concurrency model of Terragrunt will change in a future release (see #3629), and at that point, Terragrunt will no longer run units in “Groups” of units, but instead run each unit when it is free to run. If you are currently relying on the output-module-groups to programmatically determine when units can run, you’ll want to switch to using the find --dag --json --dependencies command to get a detailed breakdown of dependencies between units, and use that information to determine when units can run. terragrunt find --dag --json --dependencies[ { \"type\": \"unit\", \"path\": \"vpc\" }, { \"type\": \"unit\", \"path\": \"db\", \"dependencies\": [ \"vpc\" ] }, { \"type\": \"unit\", \"path\": \"ec2\", \"dependencies\": [ \"vpc\", \"db\" ] }] Use the newly renamed commandsSection titled “Use the newly renamed commands” Aside from the adjustments listed above, you’ll also want to replace usage of deprecated commands with their newly renamed counterparts. hclfmt (use hcl fmt instead) hclvalidate (use hcl validate instead) validate-inputs (use hcl validate --inputs instead) terragrunt-info (use info print instead) render-json (use render --json -w instead) graph-dependencies (use dag graph instead) run-all (use run --all instead) graph (use run --graph instead) Edit page Previous Upgrading to Terragrunt 0.19.x Next Terragrunt Stacks",
    "section": "migrate",
    "lastUpdated": "2025-10-23T16:41:00.314Z"
  },
  {
    "title": "Terragrunt Stacks",
    "url": "https://terragrunt.gruntwork.io/docs/migrate/terragrunt-stacks/",
    "content": "Terragrunt Stacks Migrating from the terragrunt-infrastructure-live-example repositorySection titled “Migrating from the terragrunt-infrastructure-live-example repository” If you have an existing repository that was started using the terragrunt-infrastructure-live-example repository as a starting point, follow the steps below to migrate your existing configurations to take advantage of the patterns available using Terragrunt Stacks. Step 1: Assess your current infrastructureSection titled “Step 1: Assess your current infrastructure” Before you get started adjusting any of your existing configurations, it’s important to understand the current state of your infrastructure. How much of it do you regularly update? Does any of it result in frustration or difficulty? Why? Determine whether it’s a good time to be migrating your infrastructure to new patterns, and if so, how much of it you’re willing to migrate. If you are happy, and successful with your current patterns, you may not need to migrate any existing configuration, and that’s great! Consider this a best practice that you can adopt when you start to introduce new infrastructure, and that you may want to adjust your existing infrastructure configurations over time to take advantage of new patterns. The advantages of using the new paradigm with Terragrunt Stacks are: You can more easily manage your infrastructure at scale. You can more easily manage your infrastructure in different environments. You can more easily manage your infrastructure across multiple accounts and regions. You can more easily manage your infrastructure across multiple teams and organizations. We, at Gruntwork, generally consider this paradigm to be the best practice for managing Infrastructure as Code (IaC) at scale, which is why we’ve created this migration guide to help you transition to it. If you get overwhelmed at any point, read the Support docs to learn how you can get help. Step 2: Update your Terragrunt versionSection titled “Step 2: Update your Terragrunt version” Now that you’ve determined that you want to migrate some or all of your infrastructure to new patterns, the next step is to ensure that you have a version of Terragrunt that supports the terragrunt.stack.hcl file. You can do this by updating the version of Terragrunt you use to the latest available version. If you would like more information on how to update your Terragrunt version, see the Installation guide. Step 3: Add .terragrunt-stack directories to your repository .gitignore fileSection titled “Step 3: Add .terragrunt-stack directories to your repository .gitignore file” Now that you’re adopting Terragrunt Stacks, you’ll want to add the .terragrunt-stack directories to your repository .gitignore file. echo \".terragrunt-stack\" >> .gitignoregit add .gitignoregit commit -m \"Add .terragrunt-stack to .gitignore\" This will prevent you from accidentally committing .terragrunt-stack directories to your repository, which is good because you can always regenerate them on demand using the terragrunt stack generate command. All other terragrunt stack commands also automatically generate the .terragrunt-stack directory on demand, so you can safely ignore it. Step 4: Re-define existing infrastructure using terragrunt.stack.hcl filesSection titled “Step 4: Re-define existing infrastructure using terragrunt.stack.hcl files” The infrastructure that you already have can be re-defined using terragrunt.stack.hcl files, reducing the amount of code that you need to maintain in your repository. To do this, you’ll need to: Create an infrastructure-catalog repository if you don’t already have one to store your infrastructure configurations. Define the units that you want to reproduce from your infrastructure-catalog repository in your infrastructure-live repository via terragrunt.stack.hcl files. Find a collection of units that you want to abstract into a stack, and define a terragrunt.stack.hcl file for them. For example, say you have a collection of units like this, that you want to abstract into a stack: Directorynon-prod Directoryus-east-1 Directorystateful-ec2-asg-service Directoryservice terragrunt.hclDirectorydb terragrunt.hclDirectorysgs Directoryasg terragrunt.hcl This collection of units can be abstracted into a single stack by creating a terragrunt.stack.hcl file in the stateful-ec2-asg-service directory that references each unit configuration, as defined in your infrastructure-catalog repository (in this example, the infrastructure-catalog repository is hosted at git@github.com:acme/infrastructure-catalog.git): ## non-prod/us-east-1/stateful-ec2-asg-service/terragrunt.stack.hcl unit \"service\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/ec2-asg-stateful-service?ref=v1.0.0\" path = \"service\" no_dot_terragrunt_stack = true ## Add any additional configuration for the service unit here} unit \"db\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/mysql?ref=v1.0.0\" path = \"db\" no_dot_terragrunt_stack = true ## Add any additional configuration for the db unit here} unit \"asg-sg\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/security-group?ref=v1.0.0\" path = \"sgs/asg\" no_dot_terragrunt_stack = true ## Add any additional configuration for the asg-sg unit here} Note the use of the no_dot_terragrunt_stack attribute. This is used to prevent Terragrunt from automatically generating the units into a .terragrunt-stack directory. This is important, because you are probably using path_relative_to_include() in the key attribute of the remote_state block of the root root.hcl file, which is included in every unit. By specifying no_dot_terragrunt_stack = true, the generated units will be generated into the same directory as they were before, and the path_relative_to_include() function will resolve to the same path as before. Migrating to a terragrunt.stack.hcl file in this way allows you to migrate your infrastructure to the new patterns outlined here at your own pace, and to migrate state between the old and new patterns if you want to. Now, you can remove the existing unit configurations, and regenerate them on demand using the terragrunt stack generate command. cd non-prod/us-east-1/stateful-ec2-asg-servicerm -rf service db sgs If you have identical unit configurations after performing the following, you can remove the unit configurations again, add them to a .gitignore file, and commit the new terragrunt.stack.hcl file. terragrunt stack generate Directorynon-prod Directoryus-east-1 Directorystateful-ec2-asg-service terragrunt.stack.hclDirectoryservice terragrunt.hcl < This should be identical to the unit configuration beforeDirectorydb terragrunt.hcl < This should be identical to the unit configuration beforeDirectorysgs Directoryasg terragrunt.hcl < This should be identical to the unit configuration before Now that you’ve confirmed generation is working, you can remove the unit configurations again, add them to a .gitignore file, and commit the new terragrunt.stack.hcl file. rm -rf service db sgsgit add terragrunt.stack.hcl service db sgsgit commit -m \"Remove unit configurations and add terragrunt.stack.hcl\"echo \"service\" >> .gitignoreecho \"db\" >> .gitignoreecho \"sgs\" >> .gitignoregit add .gitignoregit commit -m \"Add unit configurations to .gitignore\" Your repository should now look like this: Directorynon-prod Directoryus-east-1 Directorystateful-ec2-asg-service .gitignoreterragrunt.stack.hcl You can repeat this process as much as you want, abstracting more and more of your infrastructure into Terragrunt Stacks. Step 5: Remove reliance on the _envcommon directorySection titled “Step 5: Remove reliance on the _envcommon directory” The _envcommon directory is no longer needed to create “Don’t Repeat Yourself” (DRY) configurations with Terragrunt, and is no longer recommended as a best practice. If you would like to remove usage of the _envcommon directory, you can do so by replacing usage of the include block referencing the _envcommon directory with content directly committed to terragrunt.hcl files. For example, say you have a terragrunt.hcl file that looks like this: ## non-prod/us-east-1/mysql/terragrunt.hcl include \"root\" { path = find_in_parent_folders(\"root.hcl\")} include \"envcommon\" { path = \"${dirname(find_in_parent_folders(\"root.hcl\"))}/_envcommon/mysql.hcl\" expose = true} terraform { source = \"${include.envcommon.locals.base_source_url}?ref=v0.8.0\"} inputs = { instance_class = \"db.t2.medium\" allocated_storage = 100} and an _envcommon/mysql.hcl file that looks like this: ## _envcommon/mysql.hcl locals { environment_vars = read_terragrunt_config(find_in_parent_folders(\"env.hcl\")) env = local.environment_vars.locals.environment base_source_url = \"git::git@github.com:acme/infrastructure-catalog.git//modules/mysql\"} inputs = { name = \"mysql_${local.env}\" instance_class = \"db.t2.micro\" allocated_storage = 20 storage_type = \"standard\" master_username = \"admin\"} This pattern was previously used to create “Don’t Repeat Yourself” (DRY) configurations with Terragrunt. However, this pattern is no longer recommended as a best practice, and is no longer needed to create DRY configurations with Terragrunt. Instead, you can create a terragrunt.hcl file in your infrastructure-catalog repository that looks like this: ## units/mysql/terragrunt.hcl include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \"git::git@github.com:acme/infrastructure-catalog.git//modules/mysql?ref=${values.version}\"} inputs = { ## Required inputs name = values.name instance_class = values.instance_class allocated_storage = values.allocated_storage storage_type = values.storage_type master_username = values.master_username master_password = values.master_password ## Optional inputs skip_final_snapshot = try(values.skip_final_snapshot, null) engine_version = try(values.engine_version, null)} Then reference that terragrunt.hcl file in your terragrunt.stack.hcl files, like so: ## non-prod/us-east-1/terragrunt.stack.hcl unit \"mysql\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/mysql?ref=v1.0.0\" path = \"mysql\" ## As discussed above, this prevents Terragrunt from automatically generating the units into a `.terragrunt-stack` directory. no_dot_terragrunt_stack = true values = { version = \"v0.8.0\" name = \"mysql_dev\" instance_class = \"db.t2.micro\" allocated_storage = 20 storage_type = \"standard\" master_username = \"admin\" }} Now, all your unit configurations can be found directly in the terragrunt.hcl file in the infrastructure-catalog repository, without having to bounce around between different included or referenced files, and you have an explicit interface for the values that can be set externally, via the values attribute. Different environments can pin different versions of the unit, and that allows for easy atomic updates (and rollbacks) of both OpenTofu/Terraform module versions and Terragrunt unit configurations if needed. Tip This is one of the main reasons why we recommend using Terragrunt Stacks over the old _envcommon directory pattern.In the old _envcommon directory pattern, there was no simple way to version the shared configuration referenced by all units in your live repository. All units always referenced the same version of the shared configuration in _envcommon. Now that you’re using Terragrunt Stacks, you can use Git tags to version the shared configuration you reference in your terragrunt.stack.hcl files, and different environments can pin the version of the shared configuration they want to use. Step 6: Update your CI/CD pipelineSection titled “Step 6: Update your CI/CD pipeline” Chances are, if you’re currently performing Terragrunt updates via a CI/CD pipeline (and you aren’t using Gruntwork Pipelines), your CI/CD pipeline doesn’t yet have integration with Terragrunt Stacks. There are a few options for how to proceed here. You can simply commit the generated .terragrunt-stack directories to your repository. This is the easiest option when managing CI/CD yourself, but it also means that you won’t gain some of the benefits that come from using Terragrunt Stacks. When getting started, however, this is a good way to avoid the additional technical debt that comes from having to update your CI/CD pipeline to support Terragrunt Stacks, while learning how to use terragrunt.stack.hcl files, and reorganizing your infrastructure configurations. To do this, remove the .terragrunt-stack entry from your .gitignore file, and commit the changes to your repository. You can then manually run the terragrunt stack generate command to generate the .terragrunt-stack directories on demand, and commit them to your repository, allowing your CI/CD pipeline to completely ignore the fact that you’re using Terragrunt Stacks. The units generated by the terragrunt stack generate command are completely compatible with units that you can author manually, so you don’t have to worry about any incompatibility issues that might arise from this approach. You can configure your CI/CD pipeline to run the terragrunt stack generate command whenever your pipeline runs, and leverage the generated .terragrunt-stack directories in your pipeline. Depending on the complexity of your CI/CD pipeline, this might be as simple as performing the following: terragrunt stack generateterragrunt run --all plan/apply --non-interactive This doesn’t account for destroys or the reduction of blast radius for changes by carefully inspecting Git diffs, but it’s a good start for users that don’t have CI/CD pipelines that are too complex. There is an open RFC in GitHub (Filter Flag) that would allow for this kind of complex filtering out of the box with Terragrunt, but at the moment, it’s still an open RFC. Edit page Previous CLI Redesign Next Bare Include",
    "section": "migrate",
    "lastUpdated": "2025-10-23T16:41:00.375Z"
  },
  {
    "title": "Bare Include",
    "url": "https://terragrunt.gruntwork.io/docs/migrate/bare-include/",
    "content": "Bare Include Migrating from bare includesSection titled “Migrating from bare includes” The earliest form of include support in Terragrunt was a bare include. e.g. include { path = find_in_parent_folders(\"root.hcl\")} Once Terragrunt supported the ability to define multiple includes, and to expose the values in includes as variables, users could optionally use named includes instead of a bare include. e.g. include \"root\" { path = find_in_parent_folders(\"root.hcl\")} HCL parsing does not support the ability to parse HCL configuration and accept that a configuration block has zero or more attributes, so a workaround in Terragrunt internals was to parse the configuration, then rewrite it internally to avoid breaking backwards compatibility for bare includes. e.g. include { path = find_in_parent_folders(\"root.hcl\")} becomes: include \"\" { path = find_in_parent_folders(\"root.hcl\")} Especially on large projects, this extra work is not worth the performance penalty, and Terragrunt has deprecated support for bare includes. In a future version of Terragrunt, users will be required to use named includes for all includes. Edit page Previous Terragrunt Stacks",
    "section": "migrate",
    "lastUpdated": "2025-10-23T16:41:00.414Z"
  },
  {
    "title": "tf-path",
    "url": "https://terragrunt.gruntwork.io/docs/reference/cli/commands/run#tf-path",
    "content": "run Usage Run a command, passing arguments to an orchestrated tofu/terraform binary.This is the explicit, and most flexible form of running an IaC command with Terragrunt. Shortcuts can be found in \"terragrunt --help\" for common use-cases. Examples Run the plan command. terragrunt run plan# Shortcut:# terragrunt planRun the plan command, and pass additional arguments. terragrunt run -- output -json# Shortcut:# terragrunt output -json Running multiple units Note that the run command is a more explicit and flexible way to run OpenTofu/Terraform commands in comparison to OpenTofu shortcuts. The run command also supports the following flags that can be used to drive runs in multiple units: --all: Run the provided OpenTofu/Terraform command against all units in the current stack. --graph: Run the provided OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. Separating Arguments You may, at times, need to explicitly separate the arguments used for Terragrunt from those used for OpenTofu/Terraform. In those circumstances, you can use the argument -- to separate the Terragrunt flags from the OpenTofu/Terraform flags. terragrunt run -- plan -no-color Caution When using run --all plan with units that have dependencies (e.g. via dependency or dependencies blocks), the command will fail if those dependencies have never been deployed. This is because Terragrunt cannot resolve dependency outputs without existing state.To work around this issue, use mock outputs in dependency blocks. Caution Do not set TF_PLUGIN_CACHE_DIR when using run --all. This can cause concurrent access issues with the provider cache. Instead, use Terragrunt’s built-in Provider Cache Server.We are working with the OpenTofu team to improve this behavior in the future. Caution When using run --all with apply or destroy, Terragrunt automatically adds the -auto-approve flag due to limitations with shared stdin making individual approvals impossible. Flags --all Run the specified OpenTofu/Terraform command on the stack of units in the current directory. When this flag is set, Terragrunt will run the specified OpenTofu/Terraform command against all units in the current stack. This is useful when you want to apply changes across multiple units at once. For example: terragrunt run --all plan This will run the plan command against all units in the current stack. To learn more about how to use this flag, see the Stacks feature documentation. Note Terragrunt has a notion of “external dependencies”. These are units that are not part of the current stack, from the perspective of the current working directory, but are dependencies of units that are part of the current stack.When running an --all command, Terragrunt will discover any external dependencies and prompt you to confirm that you want to run them as well.If you would like to suppress this prompt, you can use the --non-interactive flag. You can also prevent this behavior by setting --queue-exclude-external. Danger When running a run --all destroy command, Terragrunt will destroy all dependencies of the units under the current working directory, in addition to the units themselves by default!If you wish to prevent external dependencies from being destroyed, add the --queue-exclude-external flag, or use the --exclude-dir flag once for each directory you wish to exclude. Caution Use run --all with care if you have unapplied dependencies.If you have a stack of Terragrunt units with dependencies between them via dependency blocks and you’ve never deployed them, then commands like run --all plan will fail, as it won’t be possible to resolve outputs of dependency blocks without applying them first.The solution for this is to take advantage of mock outputs in dependency blocks. Note Using run --all with apply or destroy silently adds the -auto-approve flag to the command line arguments passed to OpenTofu/Terraform due to issues with shared stdin making individual approvals impossible. Note Using the OpenTofu/Terraform -detailed-exitcode flag with the run --all command results in an aggregate exit code being returned, rather than the exit code of any particular unit.The algorithm for determining the aggregate exit code is as follows: If any unit throws a 1, Terragrunt will throw a 1. If any unit throws a 2, but nothing throws a 1, Terragrunt will throw a 2. If nothing throws a non-zero, Terragrunt will throw a 0. Type: bool Environment Variables: TG_ALL --auth-provider-cmd Run the provided command and arguments to authenticate Terragrunt dynamically when necessary. The command and arguments used to obtain authentication credentials dynamically. If specified, Terragrunt runs this command whenever it might need authentication. This includes HCL parsing, where it might be useful to authenticate with a cloud provider before running HCL functions like get_aws_account_id where authentication has to already have taken place. It can also be useful for HCL functions like run_cmd where it may be useful to be authenticated before calling the function. The output must be valid JSON of the following schema: { \"awsCredentials\": { \"ACCESS_KEY_ID\": \"\", \"SECRET_ACCESS_KEY\": \"\", \"SESSION_TOKEN\": \"\" }, \"awsRole\": { \"roleARN\": \"\", \"sessionName\": \"\", \"duration\": 0, \"webIdentityToken\": \"\" }, \"envs\": { \"ANY_KEY\": \"\" }} This allows Terragrunt to acquire different credentials at runtime without changing any terragrunt.hcl configuration. You can use this flag to set arbitrary credentials for continuous integration, authentication with providers other than AWS and more. As long as the standard output of the command passed to auth-provider-cmd results in JSON matching the schema above, corresponding environment variables will be set (and/or roles assumed) before Terragrunt begins parsing an terragrunt.hcl file or running an OpenTofu/Terraform command. The simplest approach to leverage this flag is to write a script that fetches desired credentials, and emits them to STDOUT in the JSON format listed above: #!/usr/bin/env bash echo -n '{\"envs\": {\"KEY\": \"a secret\"}}' You can use any technology for the authentication provider you’d like, however, as long as Terragrunt can execute it. The expected pattern for using this flag is to author a script/program that will dynamically fetch secret values from a secret store, etc. then emit them to STDOUT for consumption by Terragrunt. Note that more specific configurations (e.g. awsCredentials) take precedence over less specific configurations (e.g. envs). If you would like to set credentials for AWS with this method, you are encouraged to use awsCredentials instead of envs, as these keys will be validated to conform to the officially supported environment variables expected by the AWS SDK. Similarly, if you would like Terragrunt to assume an AWS role on your behalf, you are encouraged to use the awsRole configuration instead of envs. Other credential configurations will be supported in the future, but until then, if your provider authenticates via environment variables, you can use the envs field to fetch credentials dynamically from a secret store, etc before Terragrunt executes any IAC. Note The awsRole configuration is only used when the awsCredentials configuration is not present. If both are present, the awsCredentials configuration will take precedence. Caution When using iam-assume-role, AWS authentication takes place right before a Terraform run, after terragrunt.hcl files are fully parsed. This means HCL functions like get_aws_account_id and run_cmd will not run with the assumed role credentials. If you need roles to be assumed prior to parsing configurations, use auth-provider-cmd instead. Type: string Environment Variables: TG_AUTH_PROVIDER_CMD --config The path to the Terragrunt config file. Default is terragrunt.hcl. This flag allows you to specify a custom path to your Terragrunt configuration file. By default, Terragrunt looks for a file named terragrunt.hcl in the current directory. This is useful when you: Have multiple Terragrunt configurations in the same directory. Want to use a different naming convention for your configuration files. Need to test alternative configurations without modifying the default file. Example usage: terragrunt run plan --config custom-config.hcl Type: string Environment Variables: TG_CONFIG --json-out-dir Directory to store JSON plan files generated from plans via show -json. Note This flag only does anything when used with the --all flag for stack runs. It does nothing in single-unit runs.To generate JSON plan files for a single unit run, use the standard OpenTofu/Terraform -out flag like so:terragrunt run -- plan -out=/tmp/tfplanterragrunt run -- show -json /tmp/tfplan > /tmp/tfplan.json Generates machine-readable plan outputs per unit as tfplan.json by invoking show -json after the plan is created. Requires a plan to exist (e.g. created with --out-dir). If the directory does not exist, Terragrunt creates it. Relative paths are resolved against the root working directory, and Terragrunt mirrors the unit path under this directory (e.g. <json-out-dir>/<relative-unit-path>/tfplan.json). Examples: # Create native plan files and JSON plans for all unitsterragrunt run --all --out-dir /tmp/all --json-out-dir /tmp/all plan # Or store JSON plans separatelyterragrunt run --all --out-dir /tmp/plan --json-out-dir /tmp/json plan See the Stacks docs for more usage examples. Type: string Environment Variables: TG_JSON_OUT_DIR --dependency-fetch-output-from-state Fetch dependency outputs directly from the state file instead of using `tofu output`. This flag modifies how Terragrunt retrieves output values from dependent units. When enabled, Terragrunt will read the outputs directly from the state file instead of running terraform output or tofu output. The main benefit this flag provides is performance. Reading directly from state is typically faster than executing the OpenTofu/Terraform binary to get the same outputs. The limitation of this approach is that it is only supported by the S3 backend, and OpenTofu/Terraform may change the schema of the state file in the future, breaking this functionality. Caution Avoid using this flag without pinning the version of OpenTofu/Terraform you are using.There is no guarantee that OpenTofu/Terraform will maintain the existing schema of their state files, so there is also no guarantee that the flag will work as expected in future versions of OpenTofu/Terraform. They have not changed the schema of the state file in a long time, but there is no guarantee that they will not change it in the future. We are coordinating with the OpenTofu team to encourage stability in the state file schema, unless significant performance improvements can be made to OpenTofu output fetching to make this flag unnecessary. Tip Direct output fetching is a performance optimization. For more details on performance optimizations, their tradeoffs, and other performance tips, read the dedicated Performance documentation. Type: bool Environment Variables: TG_DEPENDENCY_FETCH_OUTPUT_FROM_STATE --disable-bucket-update When this flag is set, Terragrunt will not update the remote state resources. When enabled, Terragrunt will throw an error if it detects that remote state resources need to be updated. This is useful in scenarios where: You want to ensure state bucket configurations remain unchanged during operations You have separate processes for managing state bucket configurations The flag acts as a safety mechanism to prevent unintended modifications to your state storage infrastructure. Type: bool Environment Variables: TG_DISABLE_BUCKET_UPDATE --disable-command-validation When this flag is set, Terragrunt will not validate the tofu/terraform command. This flag disables Terragrunt’s built-in validation of OpenTofu/Terraform commands. When enabled, Terragrunt will pass commands through to OpenTofu/Terraform without checking if they are valid or supported. Type: bool Environment Variables: TG_DISABLE_COMMAND_VALIDATION --download-dir The path to download OpenTofu/Terraform modules into. Default is .terragrunt-cache in the working directory. Specifies a custom directory where Terragrunt will download and cache OpenTofu/Terraform modules. By default, modules are downloaded to .terragrunt-cache in the working directory. Note The download directory (.terragrunt-cache by default) can grow significantly over time with multiple versions of modules.Remember to add this directory to your .gitignore file and consider periodic cleanup of old cached content. Type: string Environment Variables: TG_DOWNLOAD_DIR --out-dir Directory to store plan files generated by plan/show and consumed by apply/destroy. Note This flag only does anything when used with the --all flag for stack runs. It does nothing in single-unit runs.To generate plan files for a single unit run, use the standard OpenTofu/Terraform -out flag like so:terragrunt run -- plan -out=/tmp/tfplan Specifies where Terragrunt writes native OpenTofu/Terraform plan files for each unit when running stack operations. Plans are written as tfplan.tfplan per unit. If the directory does not exist, Terragrunt creates it. Relative paths are resolved against the root working directory, and Terragrunt mirrors the unit path under this directory (e.g. <out-dir>/<relative-unit-path>/tfplan.tfplan). Examples: # Save plans for all units under /tmp/tfplanterragrunt run --all plan --out-dir /tmp/tfplan # Apply using the previously generated plans (per-unit paths are inferred)terragrunt run --all apply --out-dir /tmp/tfplan See the Stacks docs for more usage examples. Type: string Environment Variables: TG_OUT_DIR --engine-cache-path Cache path for Terragrunt engine files. Specifies the directory where Terragrunt will store its engine cache files. The engine cache helps improve performance by storing compiled configurations and other internal data. When not specified, Terragrunt will use a default location in the system’s temporary directory. You must also set the experimental-engine flag to enable engine usage. Type: string Environment Variables: TG_ENGINE_CACHE_PATH --engine-log-level Terragrunt engine log level. Controls the verbosity of logs from the Terragrunt engine. This is separate from the main Terragrunt logging and specifically affects engine-related operations like configuration parsing and dependency resolution. By default, the engine log level is set to that of the main Terragrunt logging level. You must also set the experimental-engine flag to enable engine usage. Type: string Environment Variables: TG_ENGINE_LOG_LEVEL --engine-skip-check Skip checksum check for Terragrunt engine files. When enabled, Terragrunt will skip the checksum validation of engine files. This can be useful during development or testing, and is currently the only way to use self-developed engines. Terragrunt currently only has support for verifying the signatures of official engines, though this will change in the future. In the meantime, you are encouraged to download engines from a trusted source, verify the integrity of the engine manually, and then reference the local path to the engine in your Terragrunt configuration. You must also set the experimental-engine flag to use engines. Type: bool Environment Variables: TG_ENGINE_SKIP_CHECK --experimental-engine Enable Terragrunt experimental engine. Enables usage of Terragrunt IaC engines. IaC engines are still experimental, as the API is unstable and may change in future minor versions of Terragrunt. If you are using a remote custom engine, you must also set the engine-skip-check flag to skip the signature check for the engine. Type: bool Environment Variables: TG_EXPERIMENTAL_ENGINE --feature Set feature flags for the HCL code. Allows enabling or disabling specific features in the HCL code. To learn more about feature flags, see the Feature Flags feature documentation. Type: string Environment Variables: TG_FEATURE --graph Run the provided OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. When this flag is set, Terragrunt will run the specified OpenTofu/Terraform command against the graph of dependencies for the unit in the current working directory. The graph consists of all units that depend on the unit in the current working directory via a dependency or dependencies blocks, plus all the units that depend on those units, and all the units that depend on those units, and so on, recursively up the tree, up to the Git repository root. The command will be executed following the order of dependencies: it’ll run on the unit in the current working directory first, then on units that depend on it directly, then on the units that depend on those units, and so on. Note that if the command is destroy, it will run in the opposite order (the final dependents, then their dependencies, etc. up to the unit you ran the command in). Example Usage Given the following dependency graph: Running terragrunt run --graph plan in the eks module will lead to the following execution order: Group 1- Module project/eks Group 2- Module project/services/eks-service-1- Module project/services/eks-service-2 Group 3- Module project/services/eks-service-2-v2- Module project/services/eks-service-3- Module project/services/eks-service-5 Group 4- Module project/services/eks-service-3-v2- Module project/services/eks-service-4 Group 5- Module project/services/eks-service-3-v3 Notes: lambda units aren’t included in the graph because they are not dependent on the eks unit Execution is from bottom up based on dependencies Running terragrunt run --graph destroy in the eks unit will lead to the following execution order: Group 1- Module project/services/eks-service-2-v2- Module project/services/eks-service-3-v3- Module project/services/eks-service-4- Module project/services/eks-service-5 Group 2- Module project/services/eks-service-3-v2 Group 3- Module project/services/eks-service-3 Group 4- Module project/services/eks-service-1- Module project/services/eks-service-2 Group 5- Module project/eks Notes: Execution is in reverse order; first are destroyed “top” units and in the end eks lambda units aren’t affected at all To learn more about how to use this flag, see the Stacks feature documentation. Caution When running graph destroy, the execution order is reversed compared to other commands. Dependencies will be destroyed in reverse order to ensure resources are safely removed (dependents are destroyed before their dependencies).Always verify the execution plan before running destructive commands. Danger External dependencies (units outside your current working directory) are not automatically included in graph runs.You must explicitly include them using --queue-include-external if they need to be part of the execution. Type: bool Environment Variables: TG_GRAPH --iam-assume-role Assume the specified IAM role before executing OpenTofu/Terraform. Specifies an IAM role ARN that Terragrunt should assume before executing OpenTofu/Terraform commands. This is useful for managing resources across different AWS accounts or with different permission sets. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE --iam-assume-role-duration Session duration for IAM Assume Role session. Specifies how long the temporary credentials should remain valid when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_DURATION --iam-assume-role-session-name Name for the IAM Assumed Role session. Specifies a custom session name when assuming an IAM role. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_SESSION_NAME --iam-assume-role-web-identity-token For AssumeRoleWithWebIdentity, the WebIdentity token. Specifies the WebIdentity token to use when assuming an IAM role using web identity federation. This flag is only used when iam-assume-role is specified. For more information on how to use this flag, and how it differs from other ways of managing authentication with Terragrunt, see the Authentication feature documentation. Type: string Environment Variables: TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN --inputs-debug Write debug.tfvars to working folder to help root-cause issues. When enabled, Terragrunt will write a debug.tfvars file to the working directory. This file contains the resolved input values and can be useful for debugging configuration issues. To learn more about how to use this flag, see the Debugging guide. Type: bool Environment Variables: TG_INPUTS_DEBUG --no-auto-approve Don't automatically append '-auto-approve' to the underlying OpenTofu/Terraform commands run with 'run --all'. When enabled, Terragrunt will not automatically append the -auto-approve flag to destructive commands like apply or destroy when running with --all. This means you’ll be prompted for confirmation before making changes. Type: bool Environment Variables: TG_NO_AUTO_APPROVE --no-auto-init Don't automatically run init on tofu/terraform commands. When enabled, Terragrunt will not automatically run init before other OpenTofu/Terraform commands. To learn more about how to use this flag, see the Auto-init feature documentation. Caution When --no-auto-init is enabled, you must manually run terragrunt init if needed. Terragrunt will fail if it detects that initialization is required but auto-init is disabled.This is particularly important when using custom plugin directories or when working in CI/CD environments. Type: bool Environment Variables: TG_NO_AUTO_INIT --no-auto-provider-cache-dir Disable the auto-provider-cache-dir feature even when the experiment is enabled. When enabled, Terragrunt will disable the auto-provider-cache-dir feature. This flag allows you to selectively opt-out of the automatic provider caching behavior. This is useful when you want to maintain control over provider caching in specific environments or scenarios. Type: bool Environment Variables: TG_NO_AUTO_PROVIDER_CACHE_DIR --no-auto-retry Don't automatically retry commands which fail with transient errors. When enabled, Terragrunt will not automatically retry commands that fail with transient errors (like network timeouts). By default, Terragrunt will retry such commands a limited number of times. To learn more about how Terragrunt handles retries, see the Errors section of the Runtime Control feature documentation. Type: bool Environment Variables: TG_NO_AUTO_RETRY --no-destroy-dependencies-check Disables Terragrunt's dependency validation during destroy operations. When enabled, Terragrunt will not verify if there are dependent units that would be affected by destroying the current unit. This results in faster destroy operations, but may result in orphaned resources. Caution Disabling dependency checks during destroy operations can lead to orphaned resources or broken infrastructure dependencies. Type: bool Environment Variables: TG_NO_DESTROY_DEPENDENCIES_CHECK --parallelism Parallelism for --all commands. Sets the maximum number of concurrent operations when running commands with --all. This helps control resource usage and API rate limits when working with multiple units. Caution When using --parallelism with provider caching, terraform init is always executed sequentially if OpenTofu/Terraform provider plugin cache is configured.To safely run concurrent operations with provider caching, enable the Provider Cache Server instead. Type: integer Environment Variables: TG_PARALLELISM --provider-cache Enables Terragrunt's provider caching. When enabled, Terragrunt will use its provider caching system to speed up provider downloads. For more information, see the Provider Cache Server feature documentation. Note The tofu init command is always executed sequentially if the provider plugin cache is used without the Provider Cache Server. Type: bool Environment Variables: TG_PROVIDER_CACHE --provider-cache-dir The path to the Terragrunt provider cache directory. By default, 'terragrunt/providers' folder in the user cache directory. Specifies the directory where Terragrunt will store cached provider files. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_DIR --provider-cache-hostname The hostname of the Terragrunt Provider Cache server. By default, 'localhost'. Sets the hostname for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_HOSTNAME --provider-cache-port The port of the Terragrunt Provider Cache server. By default, assigned automatically. Specifies the port number for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_PORT --provider-cache-registry-names The list of remote registries to cached by Terragrunt Provider Cache server. By default, 'registry.terraform.io', 'registry.opentofu.org'. Specifies which provider registries should be cached by the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_REGISTRY_NAMES --provider-cache-token The token for authentication to the Terragrunt Provider Cache server. By default, assigned automatically. Sets the authentication token for connecting to the Provider Cache server. This flag is only used when the Provider Cache Server is enabled. Type: string Environment Variables: TG_PROVIDER_CACHE_TOKEN --queue-exclude-dir Unix-style glob of directories to exclude from the queue of Units to run. Specifies directories to exclude when running commands with all. This flag can be specified multiple times to exclude multiple directories. When using the TG_QUEUE_EXCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note When using --queue-exclude-dir, only the specified units are excluded, not their dependencies.Dependencies of excluded units will still be run unless they are also explicitly excluded. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_EXCLUDE_DIR --queue-exclude-external Exclude external dependencies from the queue of Units to run. When enabled, Terragrunt will exclude external dependencies (dependencies outside the current working directory) from the queue when running commands with all. Note that an external dependency is a dependency that is outside the current Terragrunt working directory and is not within any directories specified by queue-include-dir. This flag is useful when you want to limit the scope of execution to only units within your current working directory structure. Type: bool Environment Variables: TG_QUEUE_EXCLUDE_EXTERNAL --queue-excludes-file Path to a file with a list of directories that need to be excluded when running `run --all` commands. Units in these directories will be excluded during execution of the commands. If a relative path is specified, it should be relative from —working-dir. This will only exclude the module, not its dependencies. This flag has been designed to integrate nicely with the hcl validate command, which can return a list of invalid files delimited by newlines when passed the --show-config-path flag. To integrate the two, you can run something like the following using bash process substitution: terragrunt run --all plan --queue-excludes-file <(terragrunt hcl validate --show-config-path || true) Type: string Default: .terragrunt-excludes Environment Variables: TG_QUEUE_EXCLUDES_FILE --queue-ignore-dag-order Ignore DAG order for --all commands. When enabled, Terragrunt will ignore the dependency order when running commands with --all. This means units will be executed in arbitrary order, which can be useful for read-only operations like plan. Note that this can lead to errors if used with commands that modify state, as dependencies might be processed in the wrong order. To learn more about how to use this flag, see the Stacks feature documentation. Type: bool Environment Variables: TG_QUEUE_IGNORE_DAG_ORDER --queue-ignore-errors Continue processing Units even if a dependency fails. When enabled, Terragrunt will continue processing remaining units even if one fails. This can be useful when you want to see all potential errors in your configuration, rather than stopping at the first failure. Note that this may lead to incomplete or inconsistent states if used with commands that modify infrastructure. To learn more about how to use this flag, see the Stacks feature documentation. Danger Using --queue-ignore-errors can lead to inconsistent state if dependencies fail but dependent modules continue running.Use this flag with caution, especially with apply or destroy commands.For more fine-grained error handling, see the errors block in the HCL reference. Type: bool Environment Variables: TG_QUEUE_IGNORE_ERRORS --queue-include-dir Unix-style glob of directories to include in the queue of Units to run. Specifies directories to include when running commands with --all. This flag can be specified multiple times to include multiple directories. When using the TG_QUEUE_INCLUDE_DIR environment variable, specify the directories as a comma-separated list. To learn more about how to use this flag, see the Stacks feature documentation. Note Glob patterns must use Unix forward slashes / to separate directories, even on Windows machines.Glob behavior for ** and **/* will change in a future version of Terragrunt.See the double-star strict control for full details. Type: list(string) Environment Variables: TG_QUEUE_INCLUDE_DIR --queue-include-external Include external dependencies in the queue of Units to run. When enabled, Terragrunt will include external dependencies (dependencies discovered outside the current working directory) in the queue when running commands with --all or --graph. Type: bool Environment Variables: TG_QUEUE_INCLUDE_EXTERNAL --queue-include-units-reading If flag is set, 'run --all' will only run the command against Terragrunt units that read the specified file via an HCL function or include. This flag works very similarly to the --queue-units-that-include flag, but instead of looking only for included configurations, it also looks for configurations that read a given file. When passed in, the --all command will include all units (modules) that read a given file into the queue. This is useful when you want to trigger an update on all units that read or include a given file using HCL functions in their configurations. Consider the following folder structure: Directoryreading-shared-hcl terragrunt.hclDirectoryalso-reading-shared-hcl terragrunt.hclDirectorynot-reading-shared-hcl terragrunt.hclshared.hcl Suppose that reading-shared-hcl and also-reading-shared-hcl both read shared.hcl in their configurations, like so: locals { shared = read_terragrunt_config(find_in_parent_folders(\"shared.hcl\"))} If you run the command run --all init --queue-include-units-reading shared.hcl from the root folder, both reading-shared-hcl and also-reading-shared-hcl will be run; not not-reading-shared-hcl. This is because the read_terragrunt_config HCL function has a special hook that allows Terragrunt to track that it has read the file shared.hcl. This hook is used by all native HCL functions that Terragrunt supports which read files. Note, however, that there are certain scenarios where Terragrunt may not be able to track that a file has been read this way. For example, you may be using a bash script to read a file via run_cmd, or reading the file via OpenTofu code. To support these use-cases, the mark_as_read function can be used to manually mark a file as read. That would look something like this: locals { filename = mark_as_read(\"file-read-by-tofu.txt\")} inputs = { filename = local.filename} Caution Due to how Terragrunt parses configurations during run --all, functions will only properly mark files as read if they are used in the locals block.Reading a file directly in the inputs block will not mark the file as read, as the inputs block is not evaluated until after the queue has already been populated. Type: string Environment Variables: TG_QUEUE_INCLUDE_UNITS_READING --queue-strict-include Only process the directories matched by --queue-include-dir. When enabled, Terragrunt will only process directories that match the patterns specified by --queue-include-dir. For example, with the following directory structure: Directoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectorystaging Directoryapp terragrunt.hclDirectorydb terragrunt.hcl Running terragrunt run --all plan --queue-include-dir \"prod/*\" would process all directories, but the --all flag includes by default when no excludes are provided, so the stage stack would also be included by default. Running terragrunt run --all plan --queue-include-dir \"prod/*\" --queue-strict-include tells Terragrunt to exclude by default, so it only include units prod/app and prod/db. Danger When using --queue-strict-include, all dependencies of included directories will be excluded if they are not explicitly included. This can lead to unexpected behavior if you’re not careful with your include patterns.Always verify the modules that will be included before running destructive commands. Type: bool Environment Variables: TG_QUEUE_STRICT_INCLUDE --report-file The path where a run report should be generated. By default, the format of the report will be automatically detected based on the file extension. A .csv extension will generate a CSV report, and a .json extension will generate a JSON report. Anything else will default to generating a CSV report. To explicitly specify the format of the report, use the report-format flag. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_FILE --report-format The format of the run report. The supported formats are: csv json The default is csv. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_FORMAT --report-schema-file When passed in, a JSON schema for the report will be generated at the specified path. This is useful when you need a programmatic way to validate that the report conforms to an expected schema. Example terragrunt run --all plan --report-schema-file report.schema.json The schema will be generated at the given path in the current working directory. For more information, see the Run Report feature. Type: string Environment Variables: TG_REPORT_SCHEMA_FILE --source Download OpenTofu/Terraform configurations from the specified source into a temporary folder, and run Terraform in that temporary folder. Specifies a source URL where Terragrunt should download the OpenTofu/Terraform configuration. This overrides any source parameters specified in the Terragrunt configuration files. The configuration will be downloaded into a temporary folder, and Terragrunt will execute OpenTofu/Terraform commands in that folder. Example: terragrunt run plan --source github.com/example/infrastructure//modules/vpc This is particularly useful when: Testing changes to modules without modifying the source in your configuration. Using a specific version or branch of your infrastructure modules. Running configurations from a different source than what’s specified in your Terragrunt files. Type: string Environment Variables: TG_SOURCE --source-map Replace any source URL (including the source URL of a config pulled in with dependency blocks) that has root source with dest. Allows you to replace source URLs that match a specified pattern with a different URL. This affects both direct source references and sources specified in dependency blocks. The format is source-regex=replacement-source. For example: terragrunt run plan --source-map \"git::ssh://git@github.com/org/repo.git=../local/repo\" This will replace any source URL that matches git::ssh://git@github.com/org/repo.git with ../local/repo. Caution Source mapping only performs literal matches on the URL portion. For example, a map key of ssh://git@github.com/org/repo.git will not match sources of the form git::ssh://git@github.com/org/repo.git. The latter requires a map key of git::ssh://git@github.com/org/repo.git. Type: string Environment Variables: TG_SOURCE_MAP --source-update Delete the contents of the temporary folder to clear out any old, cached source code before downloading new source code into it. When enabled, Terragrunt will delete any existing content in the temporary download directory before fetching new source code into it. This ensures you’re working with a fresh copy of the source code. Example: terragrunt run plan --source-update Type: bool Environment Variables: TG_SOURCE_UPDATE --summary-disable Disables the summary output at the end of a run. When enabled, Terragrunt will disable the summary output at the end of a run. For more information, see the Run Report feature. Type: bool Environment Variables: TG_SUMMARY_DISABLE --summary-per-unit Summarize each unit in the run summary. When enabled, Terragrunt will break down the run summary by unit. The units are sorted by result, then duration, with the longest-running units shown first. For more information, see the Run Report feature. Type: bool Environment Variables: TG_SUMMARY_PER_UNIT --tf-forward-stdout If specified, the output of OpenTofu/Terraform commands will be printed as is, without being integrated into the Terragrunt log. By default, Terragrunt integrates OpenTofu/Terraform output into its own logging system. When this flag is enabled, OpenTofu/Terraform output will be printed directly to stdout without Terragrunt’s logging wrapper. For example, without --tf-forward-stdout: 14:19:25.081 INFO [app] Running command: tofu plan -input=false14:19:25.174 STDOUT [app] tofu: OpenTofu used the selected providers to generate the following execution14:19:25.174 STDOUT [app] tofu: plan. Resource actions are indicated with the following symbols:14:19:25.174 STDOUT [app] tofu: + create14:19:25.174 STDOUT [app] tofu: OpenTofu will perform the following actions: With --tf-forward-stdout: 14:19:25.081 INFO [app] Running command: tofu plan -input=false OpenTofu used the selected providers to generate the following executionplan. Resource actions are indicated with the following symbols: + create OpenTofu will perform the following actions: Type: bool Environment Variables: TG_TF_FORWARD_STDOUT --tf-path Path to the OpenTofu/Terraform binary. Default is tofu (on PATH). Specifies the path to the OpenTofu/Terraform binary that Terragrunt should use. By default, Terragrunt will look for a binary named tofu in your system’s PATH. This is useful when: You have multiple versions of OpenTofu/Terraform installed. The binary is not in your PATH. You want to use a specific version for certain operations. You want to switch between OpenTofu and Terraform binaries. Example: terragrunt run plan --tf-path=/usr/local/bin/tofu Note that if you only have terraform installed, and it is available in your PATH, Terragrunt will automatically use that binary. NOTE: This will override the terraform binary that is used by terragrunt in all instances, including dependency lookups. This setting will also override any terraform_binary configuration values specified in the terragrunt.hcl config for both the top level, and dependency lookups. Type: string Environment Variables: TG_TF_PATH --units-that-include If flag is set, 'run --all' will only run the command against Terragrunt units that include the specified file. When specified, Terragrunt will only run commands against units that include the specified configuration file. This is useful for targeting specific configurations that share common included files. For example, with the following directory structure: Directorydev Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectoryprod Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectory_envcommon app.hcldb.hcl If both dev/app/terragrunt.hcl and prod/app/terragrunt.hcl include files from _envcommon, you can target all units that include the app.hcl file: terragrunt run --all --units-that-include=./_envcommon/app.hcl plan This will only run the command against units that include the app.hcl file. Note: This flag is functionally covered by the --queue-include-units-reading flag, but is more explicitly focused on the include configuration block. Type: string Environment Variables: TG_UNITS_THAT_INCLUDE --use-partial-parse-config-cache Enables caching of includes during partial parsing operations. Will also be used for the --iam-assume-role option if provided. This flag can be used to drastically decrease time required for parsing Terragrunt configuration files. The effect will only show if a lot of similar includes are expected such as the root terragrunt configuration (e.g. root.hcl) include. NOTE: This is an experimental feature, use with caution. The reason you might want to use this flag is that Terragrunt frequently only needs to perform a partial parse of Terragrunt configurations. This is the case for scenarios like: Building the Directed Acyclic Graph (DAG) during a run --all command where only the dependency blocks need to be evaluated to determine run order. Parsing the terraform block to determine state configurations for fetching dependency outputs. Determining whether Terragrunt execution behavior has to change like for prevent_destroy or skip flags in configuration. These configurations are generally safe to cache, but due to the nature of HCL being a dynamic configuration language, there are some edge cases where caching these can lead to incorrect behavior. Once this flag has been tested thoroughly, we will consider making it the default behavior. Caution This is an experimental feature. While it can significantly improve performance with frequently included configurations, the caching behavior may lead to unexpected results in some edge cases due to HCL’s dynamic nature.Test thoroughly in your environment before using in production environments. Type: bool Environment Variables: TG_USE_PARTIAL_PARSE_CONFIG_CACHE --version-manager-file-name File names used during the computation of the cache key for the version manager files. By default, terragrunt is specifying: .terraform-version.tool-versions.mise.tomlmise.toml Type: string Environment Variables: TG_VERSION_MANAGER_FILE_NAME Previous OpenTofu Shortcuts Next exec",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:41:00.506Z"
  },
  {
    "title": "inputs",
    "url": "https://terragrunt.gruntwork.io/docs/reference/hcl/attributes/#inputs",
    "content": "Attributes Terragrunt HCL configuration uses attributes when there are values that need to be defined for Terragrunt as a whole. Think of attributes as the values used for Terragrunt configuration, such as the inputs to pass an orchestrated OpenTofu/Terraform binary, or the download directory to use. inputsSection titled “inputs” The inputs attribute is a map that is used to specify the input variables and their values to pass in to OpenTofu/Terraform. Each entry of the map is passed to OpenTofu/Terraform using the environment variable mechanism. This means that each input will be set using the form TF_VAR_variablename, with the value in json encoded format. Note that because the values are being passed in with environment variables and json, the type information is lost when crossing the boundary between Terragrunt and OpenTofu/Terraform. You must specify the proper type constraint on the variable in OpenTofu/Terraform in order for OpenTofu/Terraform to process the inputs as the right type. Example: inputs = { string = \"string\" number = 42 bool = true list_string = [\"a\", \"b\", \"c\"] list_number = [1, 2, 3] list_bool = [true, false] map_string = { foo = \"bar\" } map_number = { foo = 42 bar = 12345 } map_bool = { foo = true bar = false baz = true } object = { str = \"string\" num = 42 list = [1, 2, 3] map = { foo = \"bar\" } } from_env = get_env(\"FROM_ENV\", \"default\")} Using this attribute is roughly equivalent to setting the corresponding TF_VAR_ attribute. For example, setting this in your terragrunt.hcl: inputs = { instance_type = \"t2.micro\" instance_count = 10 tags = { Name = \"example-app\" }} And running: terragrunt apply This is roughly equivalent to running: TF_VAR_instance_type=\"t2.micro\" \\TF_VAR_instance_count=10 \\TF_VAR_tags='{\"Name\":\"example-app\"}' \\tofu apply # or terraform apply Variable PrecedenceSection titled “Variable Precedence” Variables loaded in OpenTofu/Terraform will consequently use the following precedence order (with the highest precedence being lowest on the list): inputs set in terragrunt.hcl files. Explicitly set TF_VAR_ environment variables (these will override the inputs set in terragrunt.hcl if they conflict). terraform.tfvars files if present. terraform.tfvars.json files if present. Any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames. Any -var and -var-file options on the command line, in the order they are provided. download_dirSection titled “download_dir” The terragrunt download_dir string option can be used to override the default download directory. The precedence is as follows: --download-dir command line option → TG_DOWNLOAD_DIR env variable → download_dir attribute of the terragrunt.hcl file in the module directory → download_dir attribute of the included terragrunt.hcl. It supports all terragrunt functions, i.e. path_relative_from_include(). prevent_destroySection titled “prevent_destroy” Terragrunt prevent_destroy boolean flag allows you to protect selected OpenTofu/Terraform module. It will prevent destroy or run --all destroy command from actually destroying resources of the protected module. This is useful for modules you want to carefully protect, such as a database, or a module that provides auth. Example: terraform { source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"} prevent_destroy = true skipSection titled “skip” DEPRECATED: Use exclude instead. The terragrunt skip boolean flag can be used to protect units you don’t want any changes to, or just to skip units that don’t define any infrastructure by themselves. When set to true, all terragrunt commands will skip the selected module. Consider the following file structure: Directoryroot terragrunt.hclDirectoryprod terragrunt.hclDirectorydev terragrunt.hclDirectoryqa terragrunt.hcl In some cases, the root level terragrunt.hcl file is solely used to DRY up your OpenTofu/Terraform configuration by being included in the other terragrunt.hcl files. In this case, you do not want the run --all commands to process the root level terragrunt.hcl since it does not define any infrastructure by itself. To make the run --all commands skip the root level terragrunt.hcl file, you can set skip = true: skip = true The skip flag can be inherited from an included terragrunt.hcl file if skip is defined there, unless it is explicitly redefined in the current’s module terragrunt.hcl file. iam_roleSection titled “iam_role” The iam_role attribute can be used to specify an IAM role that Terragrunt should assume before invoking OpenTofu/Terraform. The precedence is as follows: --iam-assume-role command line option → TG_IAM_ASSUME_ROLE env variable → iam_role attribute of the terragrunt.hcl file in the module directory → iam_role attribute of the included terragrunt.hcl. Example: iam_role = \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\" Notes: Value of iam_role can reference local variables Definitions of iam_role included from other HCL files through include iam_assume_role_durationSection titled “iam_assume_role_duration” The iam_assume_role_duration attribute can be used to specify the STS session duration, in seconds, for the IAM role that Terragrunt should assume before invoking OpenTofu/Terraform. The precedence is as follows: --iam-assume-role-duration command line option → TG_IAM_ASSUME_ROLE_DURATION env variable → iam_assume_role_duration attribute of the terragrunt.hcl file in the module directory → iam_assume_role_duration attribute of the included terragrunt.hcl. Example: iam_assume_role_duration = 14400 iam_assume_role_session_nameSection titled “iam_assume_role_session_name” The iam_assume_role_session_name attribute can be used to specify the STS session name, for the IAM role that Terragrunt should assume before running OpenTofu/Terraform. The precedence is as follows: --iam-assume-role-session-name command line option → TG_IAM_ASSUME_ROLE_SESSION_NAME env variable → iam_assume_role_session_name attribute of the terragrunt.hcl file in the module directory → iam_assume_role_session_name attribute of the included terragrunt.hcl. iam_web_identity_tokenSection titled “iam_web_identity_token” The iam_web_identity_token attribute can be used along with iam_role to assume a role using AssumeRoleWithWebIdentity. iam_web_identity_token can be set to either the token value (typically using get_env()), or the path to a file on disk. The precedence is as follows: --iam-assume-role-web-identity-token command line option → TG_IAM_ASSUME_ROLE_WEB_IDENTITY_TOKEN env variable → iam_web_identity_token attribute of the terragrunt.hcl file in the module directory → iam_web_identity_token attribute of the included terragrunt.hcl. The primary benefit of using AssumeRoleWithWebIdentity over regular AssumeRole is that it enables you to run terragrunt in your CI/CD pipelines without static AWS credentials. Git Provider ConfigurationSection titled “Git Provider Configuration” To use AssumeRoleWithWebIdentity in your CI/CD environment, you must first configure an AWS OpenID Connect provider to trust the OIDC service provided by your git provider. Follow the instructions below for whichever Git provider you use: GitLab: Configure OpenID Connect in AWS to retrieve temporary credentials GitHub: Configuring OpenID Connect in Amazon Web Services CircleCI: Using OpenID Connect tokens in jobs Once you have configured your OpenID Connect Provider and configured the trust policy of your IAM role according to the above instructions, you can configure Terragrunt to use the Web Identity Token in the following manner. If your Git provider provides the OIDC token as an environment variable, pass it in to the iam_web_identity_token as follows iam_role = \"arn:aws:iam::<AWS account number>:role/<IAM role name>\" iam_web_identity_token = get_env(\"<variable name>\") If your Git provider provides the OIDC token as a file, simply pass the file path to iam_web_identity_token iam_role = \"arn:aws:iam::<AWS account number>:role/<IAM role name>\" iam_web_identity_token = \"/path/to/token/file\" terraform_binarySection titled “terraform_binary” The terragrunt terraform_binary string option can be used to override the default binary Terragrunt calls (which is tofu). The precedence is as follows: --tf-path command line option → TG_TF_PATH env variable → terragrunt.hcl in the module directory → included terragrunt.hcl terraform_version_constraintSection titled “terraform_version_constraint” The terragrunt terraform_version_constraint string overrides the default minimum supported version of OpenTofu/Terraform. Terragrunt usually only officially supports the latest version of OpenTofu/Terraform, however, in some cases an older version of OpenTofu/Terraform is needed. Example: terraform_version_constraint = \">= 0.11\" terragrunt_version_constraintSection titled “terragrunt_version_constraint” The terragrunt terragrunt_version_constraint string can be used to specify which versions of the Terragrunt CLI can be used with your configuration. If the running version of Terragrunt doesn’t match the constraints specified, Terragrunt will produce an error and exit without taking any further actions. Example: terragrunt_version_constraint = \">= 0.23\" retryable_errorsSection titled “retryable_errors” DEPRECATED: Use errors instead. The terragrunt retryable_errors list can be used to override the default list of retryable errors with your own custom list. Default List: retryable_errors = [ \"(?s).*Failed to load state.*tcp.*timeout.*\", \"(?s).*Failed to load backend.*TLS handshake timeout.*\", \"(?s).*Creating metric alarm failed.*request to update this alarm is in progress.*\", \"(?s).*Error installing provider.*TLS handshake timeout.*\", \"(?s).*Error configuring the backend.*TLS handshake timeout.*\", \"(?s).*Error installing provider.*tcp.*timeout.*\", \"(?s).*Error installing provider.*tcp.*connection reset by peer.*\", \"NoSuchBucket: The specified bucket does not exist\", \"(?s).*Error creating SSM parameter: TooManyUpdates:.*\", \"(?s).*app.terraform.io.*: 429 Too Many Requests.*\", \"(?s).*ssh_exchange_identification.*Connection closed by remote host.*\", \"(?s).*Client\\\\.Timeout exceeded while awaiting headers.*\", \"(?s).*Could not download module.*The requested URL returned error: 429.*\",] Example: retryable_errors = [ \"(?s).*Error installing provider.*tcp.*connection reset by peer.*\", \"(?s).*ssh_exchange_identification.*Connection closed by remote host.*\"] Edit page Previous Blocks Next Functions",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:41:00.550Z"
  },
  {
    "title": "Features",
    "url": "https://terragrunt.gruntwork.io/docs/features/units",
    "content": "Units MotivationSection titled “Motivation” Consider the following file structure in a typical OpenTofu/Terraform project, which defines three environments (prod, qa, stage) with the same infrastructure in each one (an app, a MySQL database, and a VPC): Directorylive Directoryprod Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectoryqa Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectorystage Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf The contents of each environment could be more or less identical, except perhaps for a few settings (e.g. the prod environment may run bigger or more servers). As the size of the infrastructure grows, having to maintain all of this duplicated code between environments becomes more error prone. You can reduce the amount of copy paste using OpenTofu/Terraform modules, but even the code to instantiate a module and set up input variables, output variables, providers, and remote state can still create a lot of maintenance overhead. How can you keep your OpenTofu/Terraform code DRY so that you can maximize code reuse and minimize maintenance overhead? Moreover, how can you ensure that you are reproducing as close to the same infrastructure as possible across environments, so that you can be confident that what you test in qa will work when you deploy to prod? Terragrunt unitsSection titled “Terragrunt units” A unit in Terragrunt is a directory containing a terragrunt.hcl file. This hermetic unit of infrastructure is the smallest deployable entity in Terragrunt. It’s also the most important feature Terragrunt has. Units are designed to be contained, and can be operated on independently of other units. Infrastructure changes to units are also meant to be atomic. The interface you have with a unit is a single terragrunt.hcl file, and any change you make to it should result in one reproducible change to a limited subset of your infrastructure. Units are designed to work with immutable OpenTofu/Terraform modules. The OpenTofu/Terraform code referenced by a unit should be versioned, and that version of the module should be immutable. This ensures that the infrastructure you deploy is consistent across environments, and that you are confident you can reproduce the same pattern of infrastructure as many times as you need. Remote OpenTofu/Terraform modulesSection titled “Remote OpenTofu/Terraform modules” Terragrunt has the ability to download remote OpenTofu/Terraform configurations. The idea is that you define the OpenTofu/Terraform code for your infrastructure just once, in a single repo, called, for example, modules: Directorymodules Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf This repo contains typical OpenTofu/Terraform code, with one difference: anything in your code that should be different between environments should be exposed as an input variable. For example, the app module might expose the following variables: variable \"instance_count\" { description = \"How many servers to run\"}variable \"instance_type\" { description = \"What kind of servers to run (e.g. t3.large)\"} These variables allow you to run smaller/fewer servers in qa and stage to save money and larger/more servers in prod to ensure availability and scalability. They also define the variability of this infrastructure pattern. When instantiating the app module as a Terragrunt unit, you can be fairly confident that the only variance you are likely to see between environments is in the values of these variables. In a separate repo, called, for example, live, you define the code for all of your environments, which now consists of just one terragrunt.hcl file per unit (e.g. app/terragrunt.hcl, mysql/terragrunt.hcl, etc). This gives you the following file layout: Directorylive Directoryprod Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Notice how there are no OpenTofu/Terraform configurations (.tf files) in any of the folders. Instead, each terragrunt.hcl file specifies a terraform { …​ } block that specifies from where to download the OpenTofu/Terraform code, as well as the environment-specific values for the input variables in that OpenTofu/Terraform code. For example, stage/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.3 in stage source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"} inputs = { instance_count = 3 instance_type = \"t4g.micro\"} (Note: the double slash (//) in the source parameter is intentional and required. It’s part of OpenTofu/Terraform’s Git syntax for module sources. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) And prod/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.1 in prod source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.1\"} inputs = { instance_count = 10 instance_type = \"m8g.large\"} You can now deploy the modules in your live repo. For example, to deploy the app module in stage, you would do the following: cd live/stage/appterragrunt apply When Terragrunt finds the terraform block with a source parameter in live/stage/app/terragrunt.hcl file, it will: Download the configurations specified via the source parameter into the --download-dir folder (by default .terragrunt-cache in the working directory, which we recommend adding to .gitignore). This downloading is done by using the same go-getter library OpenTofu/Terraform uses, so the source parameter supports the same syntax as the module source parameter, including local file paths, Git URLs, and Git URLs with ref parameters (useful for checking out a specific tag, commit, or branch of Git repo). Terragrunt will download all the code in the repo (i.e. the part before the double-slash //) so that relative paths work correctly between modules in that repo. Copy all files from the current working directory into the temporary folder. Execute whatever OpenTofu/Terraform command you specified in that temporary folder (assuming you are performing a run). Set any variables defined in the inputs = { …​ } block as environment variables (prefixed with TF_VAR_) before running your OpenTofu/Terraform code. Notice how the inputs block in stage/app/terragrunt.hcl deploys fewer and smaller instances than prod. Check out the terragrunt-infrastructure-modules-example and terragrunt-infrastructure-live-example repos for fully-working sample code that demonstrates our recommended folder structure for successful infrastructure management. Immutable modules and atomic deploymentsSection titled “Immutable modules and atomic deployments” With this approach, copy/paste between environments is minimized. The terragrunt.hcl files contain solely the source URL of the module to deploy and the inputs to set for that module in the current environment. To create a new unit, you copy an old one and update just the environment-specific inputs in the terragrunt.hcl files, which is about as close to the “essential complexity” of the problem as you can get. Just as importantly, since the OpenTofu/Terraform module code is now defined in a single repo, you can version it (e.g., using Git tags and referencing them using the ref parameter in the source URL, as in the stage/app/terragrunt.hcl and prod/app/terragrunt.hcl examples above), and promote a single, immutable version through each environment (e.g., qa → stage → prod). This is especially powerful when thinking about how the pattern is deployed. Because all of the configuration for a unit is defined using a versioned URL and a set of inputs, it’s easy to reliably promote an infrastructure change across environments as one atomic change. It’s also easy to roll back to a previous version of the infrastructure by changing the ref parameter in the source URL. This idea is inspired by Kief Morris’ blog post Using Pipelines to Manage Environments with Infrastructure as Code. Working locallySection titled “Working locally” If you’re testing changes to a local copy of the modules repo, you can use the --source command-line option or the TG_SOURCE environment variable to override the source parameter. This is useful to point Terragrunt at a local checkout of your code so you can do rapid, iterative, make-a-change-and-rerun development: cd live/stage/appterragrunt apply --source ../../../modules//app (Note: the double slash (//) here too is intentional and required. Terragrunt downloads all the code in the folder before the double-slash into the temporary folder so that relative paths between modules work correctly. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) Working with lock filesSection titled “Working with lock files” Terraform 0.14 introduced lock files. These should mostly “just work” with Terragrunt version v0.27.0 and above: that is, the lock file (.terraform.lock.hcl) will be generated next to your terragrunt.hcl, and you should check it into version control. See the Lock File Handling docs for more details. Terragrunt cachingSection titled “Terragrunt caching” The first time you set the source parameter to a remote URL, Terragrunt will download the code from that URL into a tmp folder. It will NOT download it again afterwards unless you change that URL. That’s because downloading code—and more importantly, reinitializing remote state, redownloading provider plugins, and redownloading modules—can take a long time. To avoid adding 10-90 seconds of overhead to every Terragrunt command, Terragrunt assumes all remote URLs are immutable, and only downloads them once. Therefore, when working locally, you should use the --source parameter and point it at a local file path as described in the previous section. Terragrunt will copy the local files every time you run it, which is nearly instantaneous, and doesn’t require reinitializing everything, so you’ll be able to iterate quickly. If you need to force Terragrunt to redownload something from a remote URL, run Terragrunt with the --source-update flag, and it’ll delete the tmp folder, download the files from scratch, and reinitialize everything. This can take a while, so avoid it and use --source when you can! Working with relative file pathsSection titled “Working with relative file paths” One of the gotchas with downloading OpenTofu/Terraform configurations is that when you run terragrunt apply in folder foo, OpenTofu/Terraform will actually run in some temporary folder such as .terragrunt-cache/foo. That means you have to be especially careful with relative file paths, as they will be relative to that temporary folder and not the folder where you ran Terragrunt! In particular: Command line: When using file paths on the command line, such as passing an extra -var-file argument, you should use absolute paths: # Use absolute file paths on the CLI!terragrunt apply -var-file /foo/bar/extra.tfvars# Or use the PWD environment variable to construct# an absolute path before passing it to Terragrunt# $ terragrunt apply -var-file \"$PWD/extra.tfvars\" Terragrunt configuration: When using file paths directly in your Terragrunt configuration (terragrunt.hcl), such as in an extra_arguments block, you can’t use hard-coded absolute file paths, or it won’t work on your teammates’ computers. Therefore, you should utilize the Terragrunt built-in function get_terragrunt_dir() to use a relative file path: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] # With the get_terragrunt_dir() function, you can use relative paths! arguments = [ \"-var-file=${get_terragrunt_dir()}/../common.tfvars\", \"-var-file=example.tfvars\" ] }} See the get_terragrunt_dir() documentation for more details. Using Terragrunt with private Git reposSection titled “Using Terragrunt with private Git repos” The easiest way to use Terragrunt with private Git repos is to use SSH authentication. Configure your Git account so you can use it with SSH (see the guide for GitHub here) and use the SSH URL for your repo: terraform { source = \"git@github.com:foo/modules.git//path/to/module?ref=v0.0.1\"} Look up the Git repo for your repository to find the proper format. Note: In automated pipelines, you may need to run the following command for your Git repository prior to calling terragrunt to ensure that the ssh host is registered locally, e.g.: ssh -T -oStrictHostKeyChecking=accept-new git@github.com || true Generate blocksSection titled “Generate blocks” In an ideal world, all that units do would be to run versioned, immutable OpenTofu/Terraform modules with environment-specific inputs. In the real world, however, certain scenarios arise when you have to inject additional configurations to the immutable OpenTofu/Terraform modules you use. This is where generate blocks prove useful. When you define a generate block, Terragrunt will do the following before any run: Fetch any module referenced in a source URL in the terraform block into the .terragrunt-cache folder (if there is none, it will run in the current working directory). Generate the file specified in the generate block into the directory where Terragrunt will run OpenTofu/Terraform. Run the OpenTofu/Terraform command. The most common example of this is to dynamically generate a provider.tf file that includes provider configurations. Most OpenTofu/Terraform modules are authored in such a way that defining a provider is an exercise left to the consumer of the module. This is a good practice, as it allows the consumer to define the provider configuration in a way that suits their needs, and it may not make sense for a nested module to define a provider configuration that is not used by the consumer. Consider a setup where you want to always assume a specific role when calling out to a given OpenTofu/Terraform module. Not all modules expose the right variables for configuring the aws provider so that you can assume the role through OpenTofu/Terraform. In this situation, you can use Terragrunt generate blocks to generate a tf file called provider.tf that includes the provider configuration. Add an env.hcl file for each of the environments in the file layout for the live repo: Directorylive Directoryprod env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Each env.hcl file (the one at the environment level, e.g prod/env.hcl) should define a generate block to generate the AWS provider configuration to assume the role for that environment. For example, if you wanted to assume the role arn:aws:iam::0123456789:role/terragrunt in all the units for the prod account, you would put the following in prod/env.hcl: generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { assume_role { role_arn = \"arn:aws:iam::0123456789:role/terragrunt\" }}EOF} This instructs Terragrunt to create the file provider.tf in the working directory where Terragrunt calls tofu/terraform before it runs any of the OpenTofu/Terraform commands (e.g plan, apply, validate, etc). This allows you to inject this provider configuration for any unit that includes the env.hcl file. To include this in the child configurations (e.g app/terragrunt.hcl), you would update all the units to include this configuration using the include block: include \"env\" { path = find_in_parent_folders(\"env.hcl\")} The include block tells Terragrunt to use the exact same Terragrunt configuration from the env.hcl file specified via the path parameter. It behaves exactly as if you had copy/pasted the OpenTofu/Terraform configuration from the included file generate configuration into the child, but this approach is much easier to maintain! Further ReadingSection titled “Further Reading” Note that if you’re considering this solution because you’re struggling with dynamic provider authentication in AWS, you may be interested in the dedicated documentation on working with multiple AWS accounts. Edit page Previous Wrap Up Next Stacks",
    "section": "features",
    "lastUpdated": "2025-10-23T16:41:00.602Z"
  },
  {
    "title": "Configuration Reference",
    "url": "https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/",
    "content": "Blocks Terragrunt HCL configuration uses configuration blocks when there’s a structural configuration that needs to be defined for Terragrunt. Think of configuration blocks as a way to control different systems used by Terragrunt, whereas attributes are used to define values for those systems. terraformSection titled “terraform” The terraform block is used to configure how Terragrunt will interact with OpenTofu/Terraform. This includes specifying where to find the OpenTofu/Terraform configuration files, any extra arguments to pass to the tofu/terraform binary, and any hooks to run before or after calling OpenTofu/Terraform. The terraform block supports the following arguments: source (attribute): Specifies where to find OpenTofu/Terraform configuration files. This parameter supports the same syntax as the module source parameter for OpenTofu/Terraform module blocks except for the Terraform registry (see below note), including local file paths, Git URLs, and Git URLS with ref parameters. Terragrunt will download all the code in the repo (i.e. the part before the double-slash //) so that relative paths work correctly between modules in that repo. The source parameter can be configured to pull OpenTofu/Terraform modules from any Terraform module registry using the tfr protocol. The tfr protocol expects URLs to be provided in the format tfr://REGISTRY_HOST/MODULE_SOURCE?version=VERSION. For example, to pull the terraform-aws-modules/vpc/aws module from the public Terraform registry, you can use the following as the source parameter: tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=3.3.0. If you wish to access a private module registry (e.g., Terraform Cloud/Enterprise), you can provide the authentication to Terragrunt as an environment variable with the key TG_TF_REGISTRY_TOKEN. This token can be any registry API token. The tfr protocol supports a shorthand notation where the REGISTRY_HOST can be omitted to default to the public registry. The default registry depends on the wrapped executable: for Terraform, it is registry.terraform.io, and for Opentofu, it is registry.opentofu.org. Additionally, if the environment variable TG_TF_DEFAULT_REGISTRY_HOST is set, this value will be used as the default registry host instead, overriding the standard defaults for the wrapped executable. If you use tfr:/// (note the three /). For example, the following will fetch the terraform-aws-modules/vpc/aws module from the public registry: tfr:///terraform-aws-modules/vpc/aws?version=3.3.0. You can also use submodules from the registry using //. For example, to use the iam-policy submodule from the registry module terraform-aws-modules/iam, you can use the following: tfr:///terraform-aws-modules/iam/aws//modules/iam-policy?version=4.3.0. include_in_copy (attribute): A list of glob patterns (e.g., [\"*.txt\"]) that should always be copied from the same directory containing terragrunt.hcl into the OpenTofu/Terraform working directory. When you use the source param in your Terragrunt config and run terragrunt <command>, Terragrunt will download the code specified at source into a scratch folder (.terragrunt-cache, by default), copy the code in your current working directory into the same scratch folder, and then run tofu <command> (or terraform <command>) in that scratch folder. By default, Terragrunt excludes hidden files and folders during the copy step. This feature allows you to specify glob patterns of files that should always be copied from the Terragrunt working directory. Additional notes: The path should be specified relative to the source directory. This list is also used when using a local file source (e.g., source = \"../modules/vpc\"). For example, if your OpenTofu/Terraform module source contains a hidden file that you want to copy over (e.g., a .python-version file), you can specify that in this list to ensure it gets copied over to the scratch copy (e.g., include_in_copy = [\".python-version\"]). exclude_from_copy (attribute): A list of glob patterns (e.g., [\"*.txt\"]) that should always be skipped from the same directory containing terragrunt.hcl when copying into the OpenTofu/Terraform working directory. All examples valid for include_in_copy can be used here. Note that using include_in_copy and exclude_from_copy are not mutually exclusive. If a file matches a pattern in both include_in_copy and exclude_from_copy, it will not be included. If you would like to ensure that the file is included, make sure the patterns you use for include_in_copy do not match the patterns in exclude_from_copy. Note that if you wish to exclude files from being copied from a terraform module source, you should use the before_hook feature. copy_terraform_lock_file (attribute): In certain use cases, you don’t want to check the terraform provider lock file into your source repository from your working directory as described in Lock File Handling. This attribute allows you to disable the copy of the generated or existing .terraform.lock.hcl from the temp folder into the working directory. Default is true. extra_arguments (block): Nested blocks used to specify extra CLI arguments to pass to the tofu/terraform binary. Learn more about its usage in the Keep your CLI flags DRY use case overview. Supports the following arguments: arguments (required) : A list of CLI arguments to pass to tofu/terraform. commands (required) : A list of tofu/terraform sub commands that the arguments will be passed to. env_vars (optional) : A map of key value pairs to set as environment variables when calling tofu/terraform. required_var_files (optional): A list of file paths to OpenTofu/Terraform vars files (.tfvars) that will be passed in to terraform as -var-file=<your file>. optional_var_files (optional): A list of file paths to OpenTofu/Terraform vars files (.tfvars) that will be passed in to tofu/terraform like required_var_files, only any files that do not exist are ignored. before_hook (block): Nested blocks used to specify command hooks that should be run before tofu/terraform is called. Hooks run from the directory with the OpenTofu/Terraform module, except for hooks related to read-config and init-from-module. These hooks run in the terragrunt configuration directory (the directory where terragrunt.hcl lives). Supports the following arguments: commands (required) : A list of tofu/terraform sub commands for which the hook should run before. execute (required) : A list of command and arguments that should be run as the hook. For example, if execute is set as [\"echo\", \"Foo\"], the command echo Foo will be run. working_dir (optional) : The path to set as the working directory of the hook. Terragrunt will switch directory to this path before running the hook command. Defaults to the terragrunt configuration directory for read-config and init-from-module hooks, and the OpenTofu/Terraform module directory for other command hooks. run_on_error (optional) : If set to true, this hook will run even if a previous hook hit an error, or in the case of “after” hooks, if the OpenTofu/Terraform command hit an error. Default is false. suppress_stdout (optional) : If set to true, the stdout output of the executed commands will be suppressed. This can be useful when there are scripts relying on OpenTofu/Terraform’s output and any other output would break their parsing. if (optional) : hook will be skipped when the argument is set or evaluates to false. after_hook (block): Nested blocks used to specify command hooks that should be run after tofu/terraform is called. Hooks run from the terragrunt configuration directory (the directory where terragrunt.hcl lives). Supports the same arguments as before_hook. error_hook (block): Nested blocks used to specify command hooks that run when an error is thrown. The error must match one of the expressions listed in the on_errors attribute. Error hooks are executed after the before/after hooks. In addition to supporting before and after hooks for all OpenTofu/Terraform commands, the following specialized hooks are also supported: read-config (after hook only): read-config is a special hook command that you can use with the after_hook subblock to run an action immediately after terragrunt finishes loading the config. This hook will run on every invocation of terragrunt. Note that you can only use this hook with after_hooks. Any before_hooks with the command read-config will be ignored. The working directory for hooks associated with this command will be the terragrunt config directory. init-from-module and init: Terragrunt has two stages of initialization: one is to download remote configurations using go-getter; the other is Auto-Init, which configures the backend and downloads provider plugins and modules. If you wish to run a hook when Terragrunt is using go-getter to download remote configurations, use init-from-module for the command. If you wish to execute a hook when Terragrunt is using tofu init/terraform init for Auto-Init, use init for the command. For example, an after_hook for the command init-from-module will run after terragrunt clones the module, while an after_hook for the command init will run after terragrunt runs tofu init/terraform init on the cloned module. Hooks for both init-from-module and init only run if the requisite stage needs to run. That is, if terragrunt detects that the module is already cloned in the terragrunt cache, this stage will be skipped and thus the hooks will not run. Similarly, if terragrunt detects that it does not need to run init in the auto init feature, the init stage is skipped along with the related hooks. The working directory for hooks associated with init-from-module will run in the terragrunt config directory, while the working directory for hooks associated with init will be the OpenTofu/Terraform module. Complete Example: terraform { # Pull the OpenTofu/Terraform configuration at the github repo \"acme/infrastructure-modules\", under the subdirectory # \"networking/vpc\", using the git tag \"v0.0.1\". source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\" # For any OpenTofu/Terraform commands that use locking, make sure to configure a lock timeout of 20 minutes. extra_arguments \"retry_lock\" { commands = get_terraform_commands_that_need_locking() arguments = [\"-lock-timeout=20m\"] } # You can also specify multiple extra arguments for each use case. Here we configure terragrunt to always pass in the # `common.tfvars` var file located by the parent terragrunt config. extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] required_var_files = [\"${get_parent_terragrunt_dir()}/common.tfvars\"] } # The following are examples of how to specify hooks # Before apply or plan, run \"echo Foo\". before_hook \"before_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Foo\"] } # Before apply, run \"echo Bar\". Note that blocks are ordered, so this hook will run after the previous hook to # \"echo Foo\". In this case, always \"echo Bar\" even if the previous hook failed. before_hook \"before_hook_2\" { commands = [\"apply\"] execute = [\"echo\", \"Bar\"] run_on_error = true } # Note that you can use interpolations in subblocks. Here, we configure it so that before apply or plan, print out the # environment variable \"HOME\". before_hook \"interpolation_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", get_env(\"HOME\", \"HelloWorld\")] run_on_error = false } # After running apply or plan, run \"echo Baz\". This hook is configured so that it will always run, even if the apply # or plan failed. after_hook \"after_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Baz\"] run_on_error = true } # After an error occurs during apply or plan, run \"echo Error Hook executed\". This hook is configured so that it will run # after any error, with the \".*\" expression. error_hook \"error_hook_1\" { commands = [\"apply\", \"plan\"] execute = [\"echo\", \"Error Hook executed\"] on_errors = [ \".*\", ] } # A special after hook to always run after the init-from-module step of the Terragrunt pipeline. In this case, we will # copy the \"foo.tf\" file located by the parent terragrunt.hcl file to the current working directory. after_hook \"init_from_module\" { commands = [\"init-from-module\"] execute = [\"cp\", \"${get_parent_terragrunt_dir()}/foo.tf\", \".\"] } # A special after_hook. Use this hook if you wish to run commands immediately after terragrunt finishes loading its # configurations. If \"read-config\" is defined as a before_hook, it will be ignored as this config would # not be loaded before the action is done. after_hook \"read-config\" { commands = [\"read-config\"] execute = [\"bash\", \"script/get_aws_credentials.sh\"] }} Local File Path Example with allowed hidden files: terraform { # Pull the OpenTofu/Terraform configuration from the local file system. Terragrunt will make a copy of the source folder in the # Terragrunt working directory (typically `.terragrunt-cache`). source = \"../modules/networking/vpc\" # Always include the following file patterns in the Terragrunt copy. include_in_copy = [ \".security_group_rules.json\", \"*.yaml\", ]} A note about using modules from the registrySection titled “A note about using modules from the registry” The key design of Terragrunt is to act as a preprocessor to convert shared service modules in the registry into a root module. In OpenTofu/Terraform, modules can be loosely categorized into two types: Root Module: An OpenTofu/Terraform module that is designed for running tofu init/terraform init and the other workflow commands (apply, plan, etc.). This is the entrypoint module for deploying your infrastructure. Root modules are identified by the presence of key blocks that setup configuration about how OpenTofu/Terraform behaves, like backend blocks (for configuring state) and provider blocks (for configuring how OpenTofu/Terraform interacts with the cloud APIs). Shared Module: A OpenTofu/Terraform module that is designed to be included in other OpenTofu/Terraform modules through module blocks. These modules are missing many of the key blocks that are required for running the workflow commands of OpenTofu/Terraform. Terragrunt further distinguishes shared modules between service modules and modules: Shared Service Module: An OpenTofu/Terraform module that is designed to be standalone and applied directly. These modules are not root modules in that they are still missing the key blocks like backend and provider, but aside from that do not need any additional configuration or composition to deploy. For example, the terraform-aws-modules/vpc module can be deployed by itself without composing with other modules or resources. Shared Module: An OpenTofu/Terraform module that is designed to be composed with other modules. That is, these modules must be embedded in another OpenTofu/Terraform module and combined with other resources or modules. For example, the consul-security-group-rules module Terragrunt started off with features that help directly deploy Root Modules, but over the years have implemented many features that allow you to turn Shared Service Modules into Root Modules by injecting the key configuration blocks that are necessary for OpenTofu/Terraform modules to act as Root Modules. Modules on the Terraform Registry are primarily designed to be used as Shared Modules. That is, you won’t be able to git clone the underlying repository and run tofu init/terraform init or apply directly on the module without modification. Unless otherwise specified, almost all the modules will require composition with other modules/resources to deploy. When using modules in the registry, it helps to think about what blocks and resources are necessary to operate the module, and translating those into Terragrunt blocks that generate them. Note that often, Terragrunt may not be able to deploy modules from the registry. While Terragrunt has features to turn any Shared Module into a Root Module, there are two key technical limitations that prevent Terragrunt from converting ALL shared modules: Every complex input must have a type associated with it. Otherwise, OpenTofu/Terraform will interpret the input that Terragrunt passes through as string. This includes list and map. Derived sensitive outputs must be marked as sensitive. Refer to the terraform tutorial on sensitive variables for more information on this requirement. If you run into issues deploying a module from the registry, chances are that module is not a Shared Service Module, and thus not designed for use with Terragrunt. Depending on the technical limitation, Terragrunt may be able to support the transition to root module. Please always file an issue on the terragrunt repository with the module + error message you are encountering, instead of the module repository. remote_stateSection titled “remote_state” The remote_state block is used to configure how Terragrunt will set up the remote state configuration of your OpenTofu/Terraform code. You can read more about Terragrunt’s remote state functionality in Keep your remote state configuration DRY use case overview. The remote_state block supports the following arguments: backend (attribute): Specifies which remote state backend will be configured. This should be one of the available backends that Opentofu/Terraform supports. disable_init (attribute): When true, skip automatic initialization of the backend by Terragrunt. Some backends have support in Terragrunt to be automatically created if the storage does not exist. Currently, s3 and gcs are the two backends with support for automatic creation. Defaults to false. disable_dependency_optimization (attribute): When true, disable optimized dependency fetching for terragrunt modules using this remote_state block. See the documentation for dependency block for more details. generate (attribute): Configure Terragrunt to automatically generate a .tf file that configures the remote state backend. This is a map that expects two properties: path: The path where the generated file should be written. If a relative path, it’ll be relative to the Terragrunt working dir (where the OpenTofu/Terraform code lives). if_exists (attribute): What to do if a file already exists at path. Valid values are: overwrite (overwrite the existing file) overwrite_terragrunt (overwrite the existing file if it was generated by terragrunt; otherwise, error) skip (skip code generation and leave the existing file as-is) error (exit with an error) config (attribute): An arbitrary map that is used to fill in the backend configuration in OpenTofu/Terraform. All the properties will automatically be included in the OpenTofu/Terraform backend block (with a few exceptions: see below). encryption (attribute): A map that is used to configure state and plan encryption in OpenTofu. The properties will be transformed into an encryption block in the OpenTofu terraform block. The properties are specific to the respective key_provider (see below). For example, if you had the following remote_state block: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} This is equivalent to the following OpenTofu/Terraform code: terraform { backend \"s3\" { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} Note that remote_state can also be set as an attribute. This is useful if you want to set remote_state dynamically. For example, if in common.hcl you had: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" }} Then in a terragrunt.hcl file, you could dynamically set remote_state as an attribute as follows: locals { # Load the data from common.hcl common = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))} # Set the remote_state config dynamically to the remote_state config in common.hclremote_state = local.common.remote_state backendSection titled “backend” Note that Terragrunt does special processing of the config attribute for the s3 and gcs remote state backends, and supports additional keys that are used to configure the automatic initialization feature of Terragrunt. For the s3 backend, the following additional properties are supported in the config attribute: region - (Optional) The region of the S3 bucket. profile - (Optional) This is the AWS profile name as set in the shared credentials file. endpoint - (Optional) A custom endpoint for the S3 API. endpoints: (Optional) A configuration map for custom service API (starting with Terraform 1.6). s3 - (Optional) A custom endpoint for the S3 API. Overrides endpoint argument. dynamodb - (Optional) A custom endpoint for the DynamoDB API. Overrides dynamodb_endpoint argument. encrypt - (Optional) Whether to enable server-side encryption of the state file. If disabled, a log warning will be issued in the console output to notify the user. If skip_bucket_ssencryption is enabled, the log will be written as a debug log. role_arn - (Optional) The role to be assumed. shared_credentials_file - (Optional) This is the path to the shared credentials file. If this is not set and a profile is specified, ~/.aws/credentials will be used. external_id - (Optional) The external ID to use when assuming the role. session_name - (Optional) The session name to use when assuming the role. dynamodb_table - (Optional) The name of a DynamoDB table to use for state locking and consistency. The table must have a primary key named LockID. If not present, locking will be disabled. use_lockfile - (Optional) When true, enables native S3 locking using S3 object conditional writes for state locking. This feature requires OpenTofu >= 1.10. Can be used simultaneously with dynamodb_table during migration (both locks must be acquired successfully), but typically used as a replacement for DynamoDB locking. skip_bucket_versioning: When true, the S3 bucket that is created to store the state will not be versioned. skip_bucket_ssencryption: When true, the S3 bucket that is created to store the state will not be configured with server-side encryption. skip_bucket_accesslogging: DEPRECATED If provided, will be ignored. A log warning will be issued in the console output to notify the user. skip_bucket_root_access: When true, the S3 bucket that is created will not be configured with bucket policies that allow access to the root AWS user. skip_bucket_enforced_tls: When true, the S3 bucket that is created will not be configured with a bucket policy that enforces access to the bucket via a TLS connection. skip_bucket_public_access_blocking: When true, the S3 bucket that is created will not have public access blocking enabled. disable_bucket_update: When true, disable update S3 bucket if not equal configured in config block enable_lock_table_ssencryption: When true, the synchronization lock table in DynamoDB used for remote state concurrent access will be configured with server-side encryption. s3_bucket_tags: A map of key value pairs to associate as tags on the created S3 bucket. dynamodb_table_tags: A map of key value pairs to associate as tags on the created DynamoDB remote state lock table. accesslogging_bucket_tags: A map of key value pairs to associate as tags on the created S3 bucket to store de access logs. disable_aws_client_checksums: When true, disable computing and checking checksums on the request and response, such as the CRC32 check for DynamoDB. See #1059 for issue where this is a useful workaround. accesslogging_bucket_name: (Optional) When provided as a valid string, create an S3 bucket with this name to store the access logs for the S3 bucket used to store OpenTofu/Terraform state. If not provided, or string is empty or invalid S3 bucket name, then server access logging for the S3 bucket storing the Opentofu/Terraform state will be disabled. Note: When access logging is enabled supported encryption for state bucket is only AES256. Reference: S3 server access logging accesslogging_target_object_partition_date_source: (Optional) When provided as a valid string, it configures the PartitionDateSource option. This option is part of the TargetObjectKeyFormat and PartitionedPrefix AWS configurations, allowing you to configure the log object key format for the access log files. Reference: Logging requests with server access logging. accesslogging_target_prefix: (Optional) When provided as a valid string, set the TargetPrefix for the access log objects in the S3 bucket used to store Opentofu/Terraform state. If set to emptystring, then TargetPrefix will be set to empty string. If attribute is not provided at all, then TargetPrefix will be set to default value TFStateLogs/. This attribute won’t take effect if the accesslogging_bucket_name attribute is not present. skip_accesslogging_bucket_acl: When set to true, the S3 bucket where access logs are stored will not be configured with bucket ACL. skip_accesslogging_bucket_enforced_tls: When set to true, the S3 bucket where access logs are stored will not be configured with a bucket policy that enforces access to the bucket via a TLS connection. skip_accesslogging_bucket_public_access_blocking: When set to true, the S3 bucket where access logs are stored will not have public access blocking enabled. skip_accesslogging_bucket_ssencryption: When set to true, the S3 bucket where access logs are stored will not be configured with server-side encryption. bucket_sse_algorithm: (Optional) The algorithm to use for server-side encryption of the state bucket. Defaults to aws:kms. bucket_sse_kms_key_id: (Optional) The KMS Key to use when the encryption algorithm is aws:kms. Defaults to the AWS Managed aws/s3 key. assume_role: (Optional) A configuration map to use when assuming a role (starting with Terraform 1.6 for Terraform). Override top level arguments role_arn - (Required) The role to be assumed. duration - (Optional) The duration the credentials will be valid. external_id - (Optional) The external ID to use when assuming the role. policy - (Optional) Policy JSON to further restrict the role. policy_arns - (Optional) A list of policy ARNs to further restrict the role. session_name - (Optional) The session name to use when assuming the role. source_identity - (Optional) The source identity to use when assuming the role. tags - (Optional) A map of key value pairs used as assume role session tags. transitive_tag_keys - (Optional) A list of tag keys that to be passed. assume_role_with_web_identity - (Optional) A configuration map to use when assuming a role with a web identity token. role_arn - (Required) The role to be assumed. duration - (Optional) The duration the credentials will be valid. policy - (Optional) Policy JSON to further restrict the role. policy_arns - (Optional) A list of policy ARNs to further restrict the role. session_name - (Optional) The session name to use when assuming the role. web_identity_token - (Required) The web identity token to use when assuming the role. web_identity_token_file - (Optional) The path to the file containing the web identity token to use when assuming the role. For the gcs backend, the following additional properties are supported in the config attribute: skip_bucket_creation: When true, Terragrunt will skip the auto initialization routine for setting up the GCS bucket for use with remote state. skip_bucket_versioning: When true, the GCS bucket that is created to store the state will not be versioned. enable_bucket_policy_only: When true, the GCS bucket that is created to store the state will be configured to use uniform bucket-level access. project: The GCP project where the bucket will be created. location: The GCP location where the bucket will be created. gcs_bucket_labels: A map of key value pairs to associate as labels on the created GCS bucket. credentials: Local path to Google Cloud Platform account credentials in JSON format. access_token: A temporary [OAuth 2.0 access token] obtained from the Google Authorization server. Example with S3: # Configure OpenTofu/Terraform state to be stored in S3, in the bucket \"my-tofu-state\" in us-east-1 under a key that is# relative to included terragrunt config. For example, if you had the following folder structure:## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hcl## And the following is defined in the root terragrunt.hcl config that is included in the child, the state file for the# child module will be stored at the key \"child/tofu.tfstate\".## Note that since we are not using any of the skip args, this will automatically create the S3 bucket# \"my-tofu-state\" and DynamoDB table \"my-lock-table\" if it does not already exist.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { backend \"s3\" {}} Example with GCS: # Configure OpenTofu/Terraform state to be stored in GCS, in the bucket \"my-tofu-state\" in the \"my-tofu\" GCP project in# the eu region under a key that is relative to included terragrunt config. This will also apply the labels# \"owner=terragrunt_test\" and \"name=tofu_state_storage\" to the bucket if it is created by Terragrunt.## For example, if you had the following folder structure:## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hcl## And the following is defined in the root terragrunt.hcl config that is included in the child, the state file for the# child module will be stored at the key \"child/tofu.tfstate\".## Note that since we are not using any of the skip args, this will automatically create the GCS bucket# \"my-tofu-state\" if it does not already exist.remote_state { backend = \"gcs\" config = { project = \"my-tofu\" location = \"eu\" bucket = \"my-tofu-state\" prefix = \"${path_relative_to_include()}/tofu.tfstate\" gcs_bucket_labels = { owner = \"terragrunt_test\" name = \"tofu_state_storage\" } }} include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { backend \"gcs\" {}} Example with S3 using native S3 locking (OpenTofu >= 1.10): # Configure OpenTofu/Terraform state to be stored in S3 with native S3 locking instead of DynamoDB.# This uses S3 object conditional writes for state locking, which requires OpenTofu >= 1.10.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true use_lockfile = true }} Example with S3 using both DynamoDB and native S3 locking during migration (OpenTofu >= 1.10): # Configure OpenTofu/Terraform state with dual locking during migration from DynamoDB to S3 native locking.# Both locks must be successfully acquired before operations can proceed.# After the migration period, remove dynamodb_table to use only S3 native locking.# Note: This won't delete the DynamoDB table, it will just be unused.# You can delete it manually after the migration period.remote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" # Remove this after migration period use_lockfile = true # New native S3 locking }} encryptionSection titled “encryption” The encryption map needs a key_provider property, which can be set to one of pbkdf2, aws_kms or gcp_kms. Documentation for each provider type and its possible configuration can be found in the OpenTofu docs. A terragrunt.hcl file configuring PBKDF2 encryption could look like this: remote_state { backend = \"s3\" config = { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" } encryption = { key_provider = \"pbkdf2\" passphrase = get_env(\"PBKDF2_PASSPHRASE\") }} This would result in the following OpenTofu code: terraform { backend \"s3\" { bucket = \"mybucket\" key = \"path/to/my/key\" region = \"us-east-1\" } encryption { key_provider \"pbkdf2\" \"default\" { passphrase = \"SUPERSECRETPASSPHRASE\" } method \"aes_gcm\" \"default\" { keys = key_provider.pbkdf2.default } state { method = method.aes_gcm.default } plan { method = method.aes_gcm.default } }} includeSection titled “include” The include block is used to specify inheritance of Terragrunt configuration files. The included config (also called the parent) will be merged with the current configuration (also called the child) before processing. You can learn more about the inheritance properties of Terragrunt in the Filling in remote state settings with Terragrunt section of the “Keep your remote state configuration DRY” use case overview. You can have more than one include block, but each one must have a unique label. It is recommended to always label your include blocks. Bare includes (include block with no label - e.g., include {}) are currently supported for backward compatibility, but is deprecated usage and support may be removed in the future. include blocks support the following arguments: name (label): You can define multiple include blocks in a single terragrunt config. Each include block must be labeled with a unique name to differentiate it from the other includes. e.g., if you had a block include \"remote\" {}, you can reference the relevant exposed data with the expression include.remote. path (attribute): Specifies the path to a Terragrunt configuration file (the parent config) that should be merged with this configuration (the child config). expose (attribute, optional): Specifies whether or not the included config should be parsed and exposed as a variable. When true, you can reference the data of the included config under the variable include. Defaults to false. Note that the include variable is a map of include labels to the parsed configuration value. merge_strategy (attribute, optional): Specifies how the included config should be merged. Valid values are: no_merge (do not merge the included config), shallow (do a shallow merge - default), deep (do a deep merge of the included config). NOTE: At this time, Terragrunt only supports a single level of include blocks. That is, Terragrunt will error out if an included config also has an include block defined. If you are interested in this feature, please follow #1566 to be notified when nested include blocks are supported. Special case for shallow merge: When performing a shallow merge, all attributes and blocks are merged shallowly with replacement, except for dependencies blocks (NOT dependency block). dependencies blocks are deep merged: that is, all the lists of paths from included configurations are concatenated together, rather than replaced in override fashion. Examples: Single includeSection titled “Single include” # If you have the following folder structure, and the following contents for ./child/terragrunt.hcl, this will include# and merge the configurations in the root.hcl file.## .# ├── root.hcl# └── child# ├── main.tf# └── terragrunt.hclremote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} inputs = { remote_state_config = include.root.remote_state} terraform { backend \"s3\" {}} Multiple includesSection titled “Multiple includes” # If you have the following folder structure, and the following contents for ./child/terragrunt.hcl, this will include# and merge the configurations in the root.hcl, while only loading the data in the region.hcl# configuration.## .# ├── root.hcl# ├── region.hcl# └── child# └── terragrunt.hclremote_state { backend = \"s3\" config = { bucket = \"my-tofu-state\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-east-1\" encrypt = true dynamodb_table = \"my-lock-table\" }} locals { region = \"production\"} include \"remote_state\" { path = find_in_parent_folders(\"root.hcl\") expose = true} include \"region\" { path = find_in_parent_folders(\"region.hcl\") expose = true merge_strategy = \"no_merge\"} inputs = { remote_state_config = include.remote_state.remote_state region = include.region.locals.region} terraform { backend \"s3\" {}} Limitations on accessing exposed configSection titled “Limitations on accessing exposed config” In general, you can access all attributes on include when they are exposed (e.g., include.locals, include.inputs, etc.). However, to support run --all, Terragrunt is unable to expose all attributes when the included config has a dependency block. To understand this, consider the following example: dependency \"vpc\" { config_path = \"${get_terragrunt_dir()}/../vpc\"} inputs = { vpc_name = dependency.vpc.outputs.name} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} dependency \"alb\" { config_path = ( include.root.inputs.vpc_name == \"mgmt\" ? \"../alb-public\" : \"../alb-private\" )} inputs = { alb_id = dependency.alb.outputs.id} In the child terragrunt.hcl, the dependency path for the alb depends on whether the VPC is the mgmt VPC or not, which is determined by the dependency.vpc in the root config. This means that the output from dependency.vpc must be available to parse the dependency.alb config. This causes problems when performing a run --all apply operation. During a run --all operation, Terragrunt first parses all the dependency blocks to build a dependency tree of the Terragrunt modules to figure out the order of operations. If all the paths are static references, then Terragrunt can determine all the dependency paths before any module has been applied. In this case there is no problem even if other config blocks access dependency, as by the time Terragrunt needs to parse those blocks, the upstream dependencies would have been applied during the run --all apply. However, if those dependency blocks depend on upstream dependencies, then there is a problem as Terragrunt would not be able to build the dependency tree without the upstream dependencies being applied. Therefore, to ensure that Terragrunt can build the dependency tree in a run --all operation, Terragrunt enforces the following limitation to exposed include config: If the included configuration has any dependency blocks, only locals and include are exposed and available to the child include and dependency blocks. There are no restrictions for other blocks in the child config (e.g., you can reference inputs from the included config in child inputs). Otherwise, if the included config has no dependency blocks, there is no restriction on which exposed attributes you can access. For example, the following alternative configuration is valid even if the alb dependency is still accessing the inputs attribute from the included config: inputs = { vpc_name = \"mgmt\"} include \"root\" { path = find_in_parent_folders(\"root.hcl\") expose = true} dependency \"vpc\" { config_path = \"../vpc\"} dependency \"alb\" { config_path = ( include.root.inputs.vpc_name == \"mgmt\" ? \"../alb-public\" : \"../alb-private\" )} inputs = { vpc_name = dependency.vpc.outputs.name alb_id = dependency.alb.outputs.id} What is deep merge? When the merge_strategy for the include block is set to deep, Terragrunt will perform a deep merge of the included config. For Terragrunt config, deep merge is defined as follows: For simple types, the child overrides the parent. For lists, the two attribute lists are combined together in concatenation. For maps, the two maps are combined together recursively. That is, if the map keys overlap, then a deep merge is performed on the map value. For blocks, if the label is the same, the two blocks are combined together recursively. Otherwise, the blocks are appended like a list. This is similar to maps, with block labels treated as keys. However, due to internal implementation details, some blocks are not deep mergeable. This will change in the future, but for now, terragrunt performs a shallow merge (that is, block definitions in the child completely override the parent definition). The following blocks have this limitation: - remote_state - generate Similarly, the locals block is deliberately omitted from the merge operation by design. That is, you will not be able to access parent config locals in the child config, and vice versa in a merge. However, you can access the parent locals in child config if you use the expose feature. Finally, dependency blocks have special treatment. When doing a deep merge, dependency blocks from both child and parent config are accessible in both places. For example, consider the following setup: dependency \"vpc\" { config_path = \"../vpc\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id db_id = dependency.mysql.outputs.db_id} include \"root\" { path = find_in_parent_folders(\"root.hcl\") merge_strategy = \"deep\"} dependency \"mysql\" { config_path = \"../mysql\"} inputs = { security_group_id = dependency.vpc.outputs.security_group_id} In the example, note how the parent is accessing the outputs of the mysql dependency even though it is not defined in the parent. Similarly, the child is accessing the outputs of the vpc dependency even though it is not defined in the child. Full example: remote_state { backend = \"s3\" config = { encrypt = true bucket = \"__FILL_IN_BUCKET_NAME__\" key = \"${path_relative_to_include()}/tofu.tfstate\" region = \"us-west-2\" }} dependency \"vpc\" { # This will get overridden by child terragrunt.hcl configs config_path = \"\" mock_outputs = { attribute = \"hello\" old_attribute = \"old val\" list_attr = [\"hello\"] map_attr = { foo = \"bar\" } } mock_outputs_allowed_terraform_commands = [\"apply\", \"plan\", \"destroy\", \"output\"]} inputs = { attribute = \"hello\" old_attribute = \"old val\" list_attr = [\"hello\"] map_attr = { foo = \"bar\" test = dependency.vpc.outputs.new_attribute }} include \"root\" { path = find_in_parent_folders(\"root.hcl\") merge_strategy = \"deep\"} remote_state { backend = \"local\"} dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { attribute = \"mock\" new_attribute = \"new val\" list_attr = [\"mock\"] map_attr = { bar = \"baz\" } }} inputs = { attribute = \"mock\" new_attribute = \"new val\" list_attr = [\"mock\"] map_attr = { bar = \"baz\" } dep_out = dependency.vpc.outputs} # Merged terragrunt.hcl # Child override parent completely due to deep merge limitationremote_state { backend = \"local\"} # mock_outputs are merged together with deep mergedependency \"vpc\" { config_path = \"../vpc\" # Child overrides parent mock_outputs = { attribute = \"mock\" # Child overrides parent old_attribute = \"old val\" # From parent new_attribute = \"new val\" # From child list_attr = [ \"hello\", # From parent \"mock\", # From child ] map_attr = { foo = \"bar\" # From parent bar = \"baz\" # From child } } # From parent mock_outputs_allowed_terraform_commands = [\"apply\", \"plan\", \"destroy\", \"output\"]} # inputs are merged together with deep mergeinputs = { attribute = \"mock\" # Child overrides parent old_attribute = \"old val\" # From parent new_attribute = \"new val\" # From child list_attr = [ \"hello\", # From parent \"mock\", # From child ] map_attr = { foo = \"bar\" # From parent bar = \"baz\" # From child test = dependency.vpc.outputs.new_attribute # From parent, referencing dependency mock output from child } dep_out = dependency.vpc.outputs # From child} localsSection titled “locals” The locals block is used to define aliases for Terragrunt expressions that can be referenced elsewhere in configuration. The locals block does not have a defined set of arguments that are supported. Instead, all the arguments passed into locals are available under the reference local.<local name> throughout the file where the locals block is defined. Example: # Make the AWS region a reusable variable within the configurationlocals { aws_region = \"us-east-1\"} inputs = { region = local.aws_region name = \"${local.aws_region}-bucket\"} Complex localsSection titled “Complex locals” Some local variables can be complex types, such as list or map. For example: locals { # Define a list of regions regions = [\"us-east-1\", \"us-west-2\", \"eu-west-1\"] # Define a map of regions to their corresponding bucket names region_to_bucket_name = { us-east-1 = \"east-bucket\" us-west-2 = \"west-bucket\" eu-west-1 = \"eu-bucket\" } # The first region is accessed like this first_region = local.regions[0] # The bucket name for us-east-1 is accessed like this us_east_1_bucket = local.region_to_bucket_name[\"us-east-1\"]} These complex types can also arise when using values derived from reading other files. For example: locals { region = \"us-east-1\"} locals { # Load the data from region.hcl region_hcl = read_terragrunt_config(find_in_parent_folders(\"region.hcl\")) # Access the region from the loaded file region = local.region_hcl.locals.region} inputs = { bucket_name = \"${local.region}-bucket\"} Similarly, you might want to define this shared data using other serialization formats, like JSON or YAML: region: us-east-1 locals { # Load the data from region.json region_yml = yamldecode(file(find_in_parent_folders(\"region.yml\"))) # Access the region from the loaded file region = local.region_yml.region} inputs = { bucket_name = \"${local.region}-bucket\"} Computed localsSection titled “Computed locals” When reading Terragrunt HCL configurations, you might read in a computed configuration: locals { computed_value = run_cmd(\"--terragrunt-quiet\", \"python3\", \"-c\", \"print('Hello,')\")} locals { # Load the data from computed.hcl computed = read_terragrunt_config(find_in_parent_folders(\"computed.hcl\")) # Access the computed value from the loaded file computed_value = \"${local.computed.locals.computed_value} world!\" # <-- This will be \"Hello, world!\"} Note that this can be a powerful feature, but it can easily lead to performance issues if you are not careful, as each read will require a full parse of the HCL file and potentially execute expensive computation. Use this feature judiciously. dependencySection titled “dependency” The dependency block is used to configure module dependencies. Each dependency block exports the outputs of the target module as block attributes you can reference throughout the configuration. You can learn more about dependency blocks in the Dependencies between modules section of the “Execute Opentofu/Terraform commands on multiple modules at once” use case overview. You can define more than one dependency block. Each label you provide to the block identifies another dependency that you can reference in your config. The dependency block supports the following arguments: name (label): You can define multiple dependency blocks in a single terragrunt config. As such, each block needs a name to differentiate between the other blocks, which is what the first label of the block is used for. You can reference the specific dependency output by the name. E.g if you had a block dependency \"vpc\", you can reference the outputs and inputs of this dependency with the expressions dependency.vpc.outputs and dependency.vpc.inputs. config_path (attribute): Path to a Terragrunt module (folder with a terragrunt.hcl file) that should be included as a dependency in this configuration. enabled (attribute): When false, excludes the dependency from execution. Defaults to true. skip_outputs (attribute): When true, skip calling terragrunt output when processing this dependency. If mock_outputs is configured, set outputs to the value of mock_outputs. Otherwise, outputs will be set to an empty map. Put another way, setting skip_outputs means “use mocks all the time if mock_outputs are set.” mock_outputs (attribute): A map of arbitrary key value pairs to use as the outputs attribute when no outputs are available from the target module, or if skip_outputs is true. However, it’s generally recommended not to set skip_outputs if using mock_outputs, because skip_outputs means “use mocks all the time if they are set” whereas mock_outputs means “use mocks only if real outputs are not available.” Use locals instead when skip_outputs = true. mock_outputs_allowed_terraform_commands (attribute): A list of Terraform commands for which mock_outputs are allowed. If a command is used where mock_outputs is not allowed, and no outputs are available in the target module, Terragrunt will throw an error when processing this dependency. mock_outputs_merge_with_state (attribute): DEPRECATED. Use mock_outputs_merge_strategy_with_state. When true, mock_outputs and the state outputs will be merged. That is, the mock_outputs will be treated as defaults and the real state outputs will overwrite them if the keys clash. mock_outputs_merge_strategy_with_state (attribute): Specifies how any existing state should be merged into the mocks. Valid values are no_merge (default) - any existing state will be used as is. If the dependency does not have an existing state (it hasn’t been applied yet), then the mocks will be used shallow - the existing state will be shallow merged into the mocks. Mocks will only be used where the output does not already exist in the dependency’s state deep_map_only - the existing state will be deeply merged into the mocks. If an output is a map, the mock key will be used where that key does not exist in the state. Lists will not be merged Example: # Run `terragrunt output` on the module at the relative path `../vpc` and expose them under the attribute# `dependency.vpc.outputs`dependency \"vpc\" { config_path = \"../vpc\" # Configure mock outputs for the `validate` command that are returned when there are no outputs available (e.g the # module hasn't been applied yet. mock_outputs_allowed_terraform_commands = [\"validate\"] mock_outputs = { vpc_id = \"fake-vpc-id\" }} # Another dependency, available under the attribute `dependency.rds.outputs`dependency \"rds\" { config_path = \"../rds\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id db_url = dependency.rds.outputs.db_url} IMPORTANT: The dependency.<name>.inputs field has been deprecated and removed. You can only access dependency outputs via dependency.<name>.outputs. If you were previously using dependency.<name>.inputs, you should refactor your configuration to use dependency.<name>.outputs instead. Can I speed up dependency fetching? dependency blocks are fetched in parallel at each source level, but will serially parse each recursive dependency. For example, consider the following chain of dependencies: account --> vpc --> securitygroup --> ecs ^ / ecr -- In this chain, the ecr and securitygroup module outputs will be fetched concurrently when applying the ecs module, but the outputs for account and vpc will be fetched serially as terragrunt needs to recursively walk through the tree to retrieve the outputs at each level. This recursive parsing happens due to the necessity to parse the entire terragrunt.hcl configuration (including dependency blocks) in full before being able to call tofu output/terraform output. However, terragrunt includes an optimization to only fetch the lowest level outputs (securitygroup and ecr in this example) provided that the following conditions are met in the immediate dependencies: The remote state is managed using remote_state blocks. The dependency optimization feature flag is enabled (disable_dependency_optimization = false, which is the default). The remote_state block itself does not depend on any dependency outputs (locals and include are ok). You are not relying on before_hook, after_hook, or extra_arguments to the tofu init/terraform init call. NOTE: terragrunt will not automatically detect this and you will need to explicitly opt out of the dependency optimization flag. If these conditions are met, terragrunt will only parse out the remote_state blocks and use that to pull down the state for the target module without parsing the dependency blocks, avoiding the recursive dependency retrieval. dependenciesSection titled “dependencies” The dependencies block is used to enumerate all the Terragrunt modules that need to be applied in order for this module to be able to apply. Note that this is purely for ordering the operations when using run --all commands of OpenTofu/Terraform. This does not expose or pull in the outputs like dependency blocks. The dependencies block supports the following arguments: paths (attribute): A list of paths to modules that should be marked as a dependency. Example: # When applying this terragrunt config in an `run --all` command, make sure the modules at \"../vpc\" and \"../rds\" are# handled first.dependencies { paths = [\"../vpc\", \"../rds\"]} generateSection titled “generate” The generate block can be used to arbitrarily generate a file in the terragrunt working directory (where tofu/terraform is called). This can be used to generate common OpenTofu/Terraform configurations that are shared across multiple OpenTofu/Terraform modules. For example, you can use generate to generate the provider blocks in a consistent fashion by defining a generate block in the parent terragrunt config. The generate block supports the following arguments: name (label): You can define multiple generate blocks in a single terragrunt config. As such, each block needs a name to differentiate between the other blocks. path (attribute): The path where the generated file should be written. If a relative path, it’ll be relative to the Terragrunt working dir (where the OpenTofu/Terraform code lives). if_exists (attribute): What to do if a file already exists at path. Valid values are: overwrite (overwrite the existing file) overwrite_terragrunt (overwrite the existing file if it was generated by terragrunt; otherwise, error) skip (skip code generation and leave the existing file as-is) error (exit with an error) if_disabled (attribute): What to do if a file already exists at path and disable is set to true (skip by default) Valid values are: remove (remove the existing file) remove_terragrunt (remove the existing file if it was generated by terragrunt; otherwise, error) skip (skip removing and leave the existing file as-is). comment_prefix (attribute): A prefix that can be used to indicate comments in the generated file. This is used by terragrunt to write out a signature for knowing which files were generated by terragrunt. Defaults to #. Optional. disable_signature (attribute): When true, disables including a signature in the generated file. This means that there will be no difference between overwrite_terragrunt and overwrite for the if_exists setting. Defaults to false. Optional. contents (attribute): The contents of the generated file. disable (attribute): Disables this generate block. Example: # When using this terragrunt config, terragrunt will generate the file \"provider.tf\" with the aws provider block before# calling to OpenTofu/Terraform. Note that this will overwrite the `provider.tf` file if it already exists.generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\" version = \"= 2.3.1\" allowed_account_ids = [\"1234567890\"]}EOF} Note that generate can also be set as an attribute. This is useful if you want to set generate dynamically. For example, if in common.hcl you had: generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = <<EOFprovider \"aws\" { region = \"us-east-1\" version = \"= 2.3.1\" allowed_account_ids = [\"1234567890\"]}EOF} Then in a terragrunt.hcl file, you could dynamically set generate as an attribute as follows: locals { # Load the data from common.hcl common = read_terragrunt_config(find_in_parent_folders(\"common.hcl\"))} # Set the generate config dynamically to the generate config in common.hclgenerate = local.common.generate engineSection titled “engine” The engine block is used to configure experimental Terragrunt engine configuration. More details in engine section. featureSection titled “feature” The feature block is used to configure feature flags in HCL for a specific Terragrunt Unit. Each feature flag must include a default value. Feature flags can be overridden via the --feature CLI option. feature \"string_flag\" { default = \"test\"} feature \"run_hook\" { default = false} terraform { before_hook \"feature_flag\" { commands = [\"apply\", \"plan\", \"destroy\"] execute = feature.run_hook.value ? [\"sh\", \"-c\", \"feature_flag_script.sh\"] : [ \"sh\", \"-c\", \"exit\", \"0\" ] }} inputs = { string_feature_flag = feature.string_flag.value} Setting feature flags through CLI: terragrunt --feature run_hook=true apply terragrunt --feature run_hook=true --feature string_flag=dev apply Setting feature flags through env variables: export TG_FEATURE=run_hook=trueterragrunt apply export TG_FEATURE=run_hook=true,string_flag=devterragrunt apply Note that the default value of the feature block is evaluated as an expression dynamically. What this means is that the value of the flag can be set via a Terragrunt expression at runtime. This is useful for scenarios where you want to integrate with external feature flag services like LaunchDarkly, AppConfig, etc. feature \"feature_name\" { default = run_cmd(\"--terragrunt-quiet\", \"<command-to-fetch-feature-flag-value>\")} Feature flags are used to conditionally control Terragrunt behavior at runtime, including the inclusion or exclusion of units. More on that in the exclude block. excludeSection titled “exclude” The exclude block in Terragrunt provides advanced configuration options to dynamically determine when and how specific units in the Terragrunt dependency graph are excluded. This feature allows for fine-grained control over which actions are executed and can conditionally exclude dependencies. Syntax: exclude { if = <boolean> # Boolean to determine exclusion. no_run = <boolean> # Boolean to prevent the unit from running (even when not using `--all`). actions = [\"<action>\", ...] # List of actions to exclude (e.g., \"plan\", \"apply\", \"all\", \"all_except_output\"). exclude_dependencies = <boolean> # Boolean to determine if dependencies should also be excluded.} Attributes: AttributeTypeDescriptionifbooleanCondition to dynamically determine whether the unit should be excluded.actionslist(string)Specifies which actions to exclude when the condition is met. Options: plan, apply, all, all_except_output etc.exclude_dependenciesbooleanIndicates whether the dependencies of the excluded unit should also be excluded (default: false).no_runbooleanWhen true and if is true, prevents the unit from running entirely for both single unit commands and run --all commands, but only when the current action matches the actions list. Examples: exclude { if = feature.feature_name.value # Dynamically exclude based on a feature flag. actions = [\"plan\", \"apply\"] # Exclude `plan` and `apply` actions. exclude_dependencies = false # Do not exclude dependencies.} In this example, the unit is excluded for the plan and apply actions only when feature.feature_name.value evaluates to true. Dependencies are not excluded. exclude { if = feature.is_dev_environment.value # Exclude only for development environments. actions = [\"all\"] # Exclude all actions. exclude_dependencies = true # Exclude dependencies along with the unit.} This configuration ensures the unit and its dependencies are excluded from all actions in the Terragrunt graph when the feature is_dev_environment evaluates to true. exclude { if = true # Explicitly exclude. actions = [\"all_except_output\"] # Allow `output` actions nonetheless. exclude_dependencies = false # Dependencies remain active.} This setup is useful for scenarios where output evaluation is still needed, even if other actions like plan or apply are excluded. exclude { if = true no_run = true actions = [\"plan\"]} This configuration prevents the unit from running when if is true AND the current action is “plan”. The no_run attribute works for both single unit commands (like terragrunt plan) and run --all commands (like terragrunt run --all plan), but only when the current action matches the actions list. Consider using this for units that are expensive to continuously update, and can be opted in when necessary. errorsSection titled “errors” The errors block contains all the configurations for handling errors. It supports different nested configuration blocks like retry and ignore to define specific error-handling strategies. Retry ConfigurationSection titled “Retry Configuration” The retry block within the errors block defines rules for retrying operations when specific errors occur. This is useful for handling intermittent errors that may resolve after a short delay or multiple attempts. Example: Retry Configuration errors { retry \"retry_example\" { retryable_errors = [\".*Error: transient.*\"] # Matches errors containing 'Error: transient' max_attempts = 5 # Retry up to 5 times sleep_interval_sec = 10 # Wait 10 seconds between retries }} Parameters: retryable_errors: A list of regex patterns to match errors that are eligible to be retried. e.g. \".*Error: transient.*\" matches errors containing Error: transient. max_attempts: The maximum number of retry attempts. e.g. 5 retries. sleep_interval_sec: Time (in seconds) to wait between retries. e.g. 10 seconds. Ignore ConfigurationSection titled “Ignore Configuration” The ignore block within the errors block defines rules for ignoring specific errors. This is useful when certain errors are known to be safe and should not prevent the run from proceeding. Example: Ignore Configuration errors { ignore \"ignore_example\" { ignorable_errors = [ \".*Error: safe-to-ignore.*\", # Ignore errors containing 'Error: safe-to-ignore' \"!.*Error: critical.*\" # Do not ignore errors containing 'Error: critical' ] message = \"Ignoring safe-to-ignore errors\" # Optional message displayed when ignoring errors signals = { safe_to_revert = true # Indicates the operation is safe to revert on failure } }} Parameters: ignorable_errors: A list of regex patterns to define errors to ignore. \"Error: safe-to-ignore.*\": Ignores errors containing Error: safe-to-ignore. \"!Error: critical.*\": Ensures errors containing Error: critical are not ignored. message (Optional): A warning message displayed when an error is ignored. Example: \"Ignoring safe-to-ignore errors\". signals (Optional): Key-value pairs used to emit signals to external systems. Example: safe_to_revert = true indicates it is safe to revert the operation if it fails. Populating values into the signals attribute results in a JSON file named error-signals.json being emitted on failure. This file can be inspected in CI/CD systems to determine the recommended course of action to address the failure. Example: If an error occurs and the author of the unit has signaled safe_to_revert = true, the CI/CD system could follow a standard process: Identify all units with files named error-signals.json. Checkout the previous commit for those units. Apply the units in their previous state, effectively reverting their updates. This approach ensures consistent and automated error handling in complex pipelines. Combined ExampleSection titled “Combined Example” Below is a combined example showcasing both retry and ignore configurations within the errors block. errors { # Retry block for transient errors retry \"transient_errors\" { retryable_errors = [\".*Error: transient network issue.*\"] max_attempts = 3 sleep_interval_sec = 5 } # Ignore block for known safe-to-ignore errors ignore \"known_safe_errors\" { ignorable_errors = [ \".*Error: safe warning.*\", \"!.*Error: do not ignore.*\" ] message = \"Ignoring safe warning errors\" signals = { alert_team = false } }} Take note that: All retry and ignore configurations must be defined within a single errors block. The retry block is prioritized over legacy retry fields (retryable_errors, retry_max_attempts, retry_sleep_interval_sec). Conditional logic can be used within ignorable_errors to enable or disable rules dynamically. Evaluation Order: Ignore Rules: Errors are checked against the ignore rules first. If an error matches, it is ignored and will not trigger a retry. Retry Rules: Once ignore rules are applied, the retry rules handle any remaining errors. Note: Only the first matching rule is applied. If there are multiple conflicting rules, any matches after the first one are ignored. Errors during source fetchingSection titled “Errors during source fetching” In addition to handling errors during OpenTofu/Terraform runs, the errors block will also handle errors that occur during source fetching. This can be particularly useful when fetching from artifact repositories that may be temporarily unavailable. Example: terraform { source = \"https://unreliable-source.com/module.zip\"} errors { retry \"source_fetch\" { retryable_errors = [\".*Error: transient network issue.*\"] max_attempts = 3 sleep_interval_sec = 5 }} unitSection titled “unit” The unit block is used to define a deployment unit within a Terragrunt stack file (terragrunt.stack.hcl). Each unit represents a distinct infrastructure component that should be deployed as part of the stack. Purpose: Define a single, deployable piece of infrastructure. Use case: When you want to create a single piece of isolated infrastructure (e.g. a specific VPC, database, or application). Result: Generates a single terragrunt.hcl file in the specified path. The unit block supports the following arguments: name (label): A unique identifier for the unit. This is used to reference the unit elsewhere in your configuration. source (attribute): Specifies where to find the Terragrunt configuration files for this unit. This follows the same syntax as the source parameter in the terraform block. path (attribute): The relative path where this unit should be deployed within the stack directory (.terragrunt-stack). Also take note of the no_dot_terragrunt_stack attribute below, which can impact this. values (attribute, optional): A map of values that will be passed to the unit as inputs. no_dot_terragrunt_stack (attribute, optional): A boolean flag (true or false). When set to true, the unit will not be placed inside the .terragrunt-stack directory but will instead be generated in the same directory where terragrunt.stack.hcl is located. This allows for a soft adoption of stacks, making it easier for users to start using terragrunt.stack.hcl without modifying existing directory structures, or performing state migrations. no_validation (attribute, optional): A boolean flag (true or false) that controls whether Terragrunt should validate the unit’s configuration. When set to true, Terragrunt will skip validation checks for this unit. Example: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-units.git//networking/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} Note that each unit must have a unique name and path within the stack. When values are specified, generated units will have access to those values via a special terragrunt.values.hcl file generated next to the terragrunt.hcl file of the unit. terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.values.hclterragrunt.hcl The terragrunt.values.hcl file will contain the values specified in the values block as top-level attributes: vpc_name = \"main\"cidr = \"10.0.0.0/16\" The unit will be able to leverage those values via values variables. inputs = { vpc_name = values.vpc_name cidr = values.cidr} Example usage of no_dot_terragrunt_stack attribute: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-units.git//networking/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} unit \"rds\" { source = \"git::git@github.com:acme/infrastructure-units.git//database/rds?ref=v0.0.1\" path = \"rds\" values = { engine = \"postgres\" version = \"13\" } no_dot_terragrunt_stack = true} With the above configuration, the resulting directory structure will be: terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.values.hclterragrunt.hclDirectoryrds terragrunt.values.hclterragrunt.hcl The vpc unit is placed inside .terragrunt-stack, as expected. The rds unit is generated in the same directory as terragrunt.stack.hcl, rather than inside .terragrunt-stack, due to no_dot_terragrunt_stack = true. Notes: The source value can be updated dynamically using the --source-map flag, just like terraform.source. A pre-created terragrunt.values.hcl file can be provided in the unit source (sibling to the terragrunt.hcl file used as the source of the unit). If present, this file will be used as the default values for the unit. However, if the values attribute is defined in the unit block, the generated terragrunt.values.hcl will replace the pre-existing file. Comparison: unit vs stack blocksSection titled “Comparison: unit vs stack blocks” Aspectunit blockstack blockPurposeDefine a single infrastructure componentDefine a reusable collection of componentsWhen to useFor specific, one-off infrastructure piecesFor patterns of infrastructure pieces that you want provisioned togetherGenerated outputA directory with a single terragrunt.hcl fileA directory with a terragrunt.stack.hcl file stackSection titled “stack” The stack block is used to define a stack of deployment units in a Terragrunt configuration file (terragrunt.stack.hcl). Stacks allow for nesting, enabling the organization of infrastructure components into modular, reusable groups, reducing redundancy and improving maintainability. Purpose: Define a collection of related units that can be reused. Use case: When you have a common, multi-unit pattern (like “dev environment” or “three-tier web application”) that you want to deploy multiple times. Result: Generates another terragrunt.stack.hcl file that can contain more units or stacks. Stacks are designed to be nestable, helping to mitigate the risk of stacks becoming too large or too repetitive. When a stack is generated, it can include nested stacks, ensuring that the configuration scales efficiently. The stack block supports the following arguments: name (label): A unique identifier for the stack. This is used to reference the stack elsewhere in your configuration. source (attribute): Specifies where to find the Terragrunt configuration files for this stack. This follows the same syntax as the source parameter in the terraform block. path (attribute): The relative path within .terragrunt-stack where this stack should be generated.If an absolute path is provided here, Terragrunt will generate the stack in that location, instead of generating it in a path relative to the .terragrunt-stack directory. Also take note of the no_dot_terragrunt_stack attribute below, which can impact this. values (attribute, optional): A map of custom values that can be passed to the stack. These values can be referenced within the stack’s configuration files, allowing for customization without modifying the stack source. no_dot_terragrunt_stack (attribute, optional): A boolean flag (true or false). When set to true, the stack will not be placed inside the .terragrunt-stack directory but will instead be generated in the same directory where terragrunt.stack.hcl is located. This allows for a soft adoption of stacks, making it easier for users to start using terragrunt.stack.hcl without modifying existing directory structures, or performing state migrations. no_validation (attribute, optional): A boolean flag (true or false) that controls whether Terragrunt should validate the stack’s configuration. When set to true, Terragrunt will skip validation checks for this stack. Example: stack \"services\" { source = \"github.com/gruntwork-io/terragrunt-stacks//stacks/mock/services?ref=v0.0.1\" path = \"services\" values = { project = \"dev-services\" cidr = \"10.0.0.0/16\" }} # ...unit \"vpc\" { # ... values = { cidr = values.cidr }} In this example, the services stack is defined with path services, which will be generated at .terragrunt-stack/services. The stack is also provided with custom values for project and cidr, which can be used within the stack’s configuration files. Terragrunt will recursively generate a stack using the contents of the .terragrunt-stack/services/terragrunt.stack.hcl file until the entire stack is fully generated. Notes: The source value can be updated dynamically using the --source-map flag, just like terraform.source. A pre-created terragrunt.values.hcl file can be provided in the stack source (sibling to the terragrunt.stack.hcl file used as the source of the stack). If present, this file will be used as the default values for the stack. However, if the values attribute is defined in the stack block, the generated terragrunt.values.hcl will replace the pre-existing file. Edit page Previous Overview Next Attributes",
    "section": "reference",
    "lastUpdated": "2025-10-23T16:41:00.774Z"
  },
  {
    "title": "Keep Your Code DRY",
    "url": "https://terragrunt.gruntwork.io/docs/features/keep-your-terraform-code-dry/",
    "content": "Units MotivationSection titled “Motivation” Consider the following file structure in a typical OpenTofu/Terraform project, which defines three environments (prod, qa, stage) with the same infrastructure in each one (an app, a MySQL database, and a VPC): Directorylive Directoryprod Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectoryqa Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tfDirectorystage Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf The contents of each environment could be more or less identical, except perhaps for a few settings (e.g. the prod environment may run bigger or more servers). As the size of the infrastructure grows, having to maintain all of this duplicated code between environments becomes more error prone. You can reduce the amount of copy paste using OpenTofu/Terraform modules, but even the code to instantiate a module and set up input variables, output variables, providers, and remote state can still create a lot of maintenance overhead. How can you keep your OpenTofu/Terraform code DRY so that you can maximize code reuse and minimize maintenance overhead? Moreover, how can you ensure that you are reproducing as close to the same infrastructure as possible across environments, so that you can be confident that what you test in qa will work when you deploy to prod? Terragrunt unitsSection titled “Terragrunt units” A unit in Terragrunt is a directory containing a terragrunt.hcl file. This hermetic unit of infrastructure is the smallest deployable entity in Terragrunt. It’s also the most important feature Terragrunt has. Units are designed to be contained, and can be operated on independently of other units. Infrastructure changes to units are also meant to be atomic. The interface you have with a unit is a single terragrunt.hcl file, and any change you make to it should result in one reproducible change to a limited subset of your infrastructure. Units are designed to work with immutable OpenTofu/Terraform modules. The OpenTofu/Terraform code referenced by a unit should be versioned, and that version of the module should be immutable. This ensures that the infrastructure you deploy is consistent across environments, and that you are confident you can reproduce the same pattern of infrastructure as many times as you need. Remote OpenTofu/Terraform modulesSection titled “Remote OpenTofu/Terraform modules” Terragrunt has the ability to download remote OpenTofu/Terraform configurations. The idea is that you define the OpenTofu/Terraform code for your infrastructure just once, in a single repo, called, for example, modules: Directorymodules Directoryapp main.tfDirectorymysql main.tfDirectoryvpc main.tf This repo contains typical OpenTofu/Terraform code, with one difference: anything in your code that should be different between environments should be exposed as an input variable. For example, the app module might expose the following variables: variable \"instance_count\" { description = \"How many servers to run\"}variable \"instance_type\" { description = \"What kind of servers to run (e.g. t3.large)\"} These variables allow you to run smaller/fewer servers in qa and stage to save money and larger/more servers in prod to ensure availability and scalability. They also define the variability of this infrastructure pattern. When instantiating the app module as a Terragrunt unit, you can be fairly confident that the only variance you are likely to see between environments is in the values of these variables. In a separate repo, called, for example, live, you define the code for all of your environments, which now consists of just one terragrunt.hcl file per unit (e.g. app/terragrunt.hcl, mysql/terragrunt.hcl, etc). This gives you the following file layout: Directorylive Directoryprod Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage Directoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Notice how there are no OpenTofu/Terraform configurations (.tf files) in any of the folders. Instead, each terragrunt.hcl file specifies a terraform { …​ } block that specifies from where to download the OpenTofu/Terraform code, as well as the environment-specific values for the input variables in that OpenTofu/Terraform code. For example, stage/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.3 in stage source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"} inputs = { instance_count = 3 instance_type = \"t4g.micro\"} (Note: the double slash (//) in the source parameter is intentional and required. It’s part of OpenTofu/Terraform’s Git syntax for module sources. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) And prod/app/terragrunt.hcl may look like this: terraform { # Deploy version v0.0.1 in prod source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.1\"} inputs = { instance_count = 10 instance_type = \"m8g.large\"} You can now deploy the modules in your live repo. For example, to deploy the app module in stage, you would do the following: cd live/stage/appterragrunt apply When Terragrunt finds the terraform block with a source parameter in live/stage/app/terragrunt.hcl file, it will: Download the configurations specified via the source parameter into the --download-dir folder (by default .terragrunt-cache in the working directory, which we recommend adding to .gitignore). This downloading is done by using the same go-getter library OpenTofu/Terraform uses, so the source parameter supports the same syntax as the module source parameter, including local file paths, Git URLs, and Git URLs with ref parameters (useful for checking out a specific tag, commit, or branch of Git repo). Terragrunt will download all the code in the repo (i.e. the part before the double-slash //) so that relative paths work correctly between modules in that repo. Copy all files from the current working directory into the temporary folder. Execute whatever OpenTofu/Terraform command you specified in that temporary folder (assuming you are performing a run). Set any variables defined in the inputs = { …​ } block as environment variables (prefixed with TF_VAR_) before running your OpenTofu/Terraform code. Notice how the inputs block in stage/app/terragrunt.hcl deploys fewer and smaller instances than prod. Check out the terragrunt-infrastructure-modules-example and terragrunt-infrastructure-live-example repos for fully-working sample code that demonstrates our recommended folder structure for successful infrastructure management. Immutable modules and atomic deploymentsSection titled “Immutable modules and atomic deployments” With this approach, copy/paste between environments is minimized. The terragrunt.hcl files contain solely the source URL of the module to deploy and the inputs to set for that module in the current environment. To create a new unit, you copy an old one and update just the environment-specific inputs in the terragrunt.hcl files, which is about as close to the “essential complexity” of the problem as you can get. Just as importantly, since the OpenTofu/Terraform module code is now defined in a single repo, you can version it (e.g., using Git tags and referencing them using the ref parameter in the source URL, as in the stage/app/terragrunt.hcl and prod/app/terragrunt.hcl examples above), and promote a single, immutable version through each environment (e.g., qa → stage → prod). This is especially powerful when thinking about how the pattern is deployed. Because all of the configuration for a unit is defined using a versioned URL and a set of inputs, it’s easy to reliably promote an infrastructure change across environments as one atomic change. It’s also easy to roll back to a previous version of the infrastructure by changing the ref parameter in the source URL. This idea is inspired by Kief Morris’ blog post Using Pipelines to Manage Environments with Infrastructure as Code. Working locallySection titled “Working locally” If you’re testing changes to a local copy of the modules repo, you can use the --source command-line option or the TG_SOURCE environment variable to override the source parameter. This is useful to point Terragrunt at a local checkout of your code so you can do rapid, iterative, make-a-change-and-rerun development: cd live/stage/appterragrunt apply --source ../../../modules//app (Note: the double slash (//) here too is intentional and required. Terragrunt downloads all the code in the folder before the double-slash into the temporary folder so that relative paths between modules work correctly. OpenTofu/Terraform may display a “OpenTofu/Terraform initialized in an empty directory” warning, but you can safely ignore it.) Working with lock filesSection titled “Working with lock files” Terraform 0.14 introduced lock files. These should mostly “just work” with Terragrunt version v0.27.0 and above: that is, the lock file (.terraform.lock.hcl) will be generated next to your terragrunt.hcl, and you should check it into version control. See the Lock File Handling docs for more details. Terragrunt cachingSection titled “Terragrunt caching” The first time you set the source parameter to a remote URL, Terragrunt will download the code from that URL into a tmp folder. It will NOT download it again afterwards unless you change that URL. That’s because downloading code—and more importantly, reinitializing remote state, redownloading provider plugins, and redownloading modules—can take a long time. To avoid adding 10-90 seconds of overhead to every Terragrunt command, Terragrunt assumes all remote URLs are immutable, and only downloads them once. Therefore, when working locally, you should use the --source parameter and point it at a local file path as described in the previous section. Terragrunt will copy the local files every time you run it, which is nearly instantaneous, and doesn’t require reinitializing everything, so you’ll be able to iterate quickly. If you need to force Terragrunt to redownload something from a remote URL, run Terragrunt with the --source-update flag, and it’ll delete the tmp folder, download the files from scratch, and reinitialize everything. This can take a while, so avoid it and use --source when you can! Working with relative file pathsSection titled “Working with relative file paths” One of the gotchas with downloading OpenTofu/Terraform configurations is that when you run terragrunt apply in folder foo, OpenTofu/Terraform will actually run in some temporary folder such as .terragrunt-cache/foo. That means you have to be especially careful with relative file paths, as they will be relative to that temporary folder and not the folder where you ran Terragrunt! In particular: Command line: When using file paths on the command line, such as passing an extra -var-file argument, you should use absolute paths: # Use absolute file paths on the CLI!terragrunt apply -var-file /foo/bar/extra.tfvars# Or use the PWD environment variable to construct# an absolute path before passing it to Terragrunt# $ terragrunt apply -var-file \"$PWD/extra.tfvars\" Terragrunt configuration: When using file paths directly in your Terragrunt configuration (terragrunt.hcl), such as in an extra_arguments block, you can’t use hard-coded absolute file paths, or it won’t work on your teammates’ computers. Therefore, you should utilize the Terragrunt built-in function get_terragrunt_dir() to use a relative file path: terraform { source = \"git::git@github.com:foo/modules.git//frontend-app?ref=v0.0.3\" extra_arguments \"custom_vars\" { commands = [ \"apply\", \"plan\", \"import\", \"push\", \"refresh\" ] # With the get_terragrunt_dir() function, you can use relative paths! arguments = [ \"-var-file=${get_terragrunt_dir()}/../common.tfvars\", \"-var-file=example.tfvars\" ] }} See the get_terragrunt_dir() documentation for more details. Using Terragrunt with private Git reposSection titled “Using Terragrunt with private Git repos” The easiest way to use Terragrunt with private Git repos is to use SSH authentication. Configure your Git account so you can use it with SSH (see the guide for GitHub here) and use the SSH URL for your repo: terraform { source = \"git@github.com:foo/modules.git//path/to/module?ref=v0.0.1\"} Look up the Git repo for your repository to find the proper format. Note: In automated pipelines, you may need to run the following command for your Git repository prior to calling terragrunt to ensure that the ssh host is registered locally, e.g.: ssh -T -oStrictHostKeyChecking=accept-new git@github.com || true Generate blocksSection titled “Generate blocks” In an ideal world, all that units do would be to run versioned, immutable OpenTofu/Terraform modules with environment-specific inputs. In the real world, however, certain scenarios arise when you have to inject additional configurations to the immutable OpenTofu/Terraform modules you use. This is where generate blocks prove useful. When you define a generate block, Terragrunt will do the following before any run: Fetch any module referenced in a source URL in the terraform block into the .terragrunt-cache folder (if there is none, it will run in the current working directory). Generate the file specified in the generate block into the directory where Terragrunt will run OpenTofu/Terraform. Run the OpenTofu/Terraform command. The most common example of this is to dynamically generate a provider.tf file that includes provider configurations. Most OpenTofu/Terraform modules are authored in such a way that defining a provider is an exercise left to the consumer of the module. This is a good practice, as it allows the consumer to define the provider configuration in a way that suits their needs, and it may not make sense for a nested module to define a provider configuration that is not used by the consumer. Consider a setup where you want to always assume a specific role when calling out to a given OpenTofu/Terraform module. Not all modules expose the right variables for configuring the aws provider so that you can assume the role through OpenTofu/Terraform. In this situation, you can use Terragrunt generate blocks to generate a tf file called provider.tf that includes the provider configuration. Add an env.hcl file for each of the environments in the file layout for the live repo: Directorylive Directoryprod env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectoryqa env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hclDirectorystage env.hclDirectoryapp terragrunt.hclDirectorymysql terragrunt.hclDirectoryvpc terragrunt.hcl Each env.hcl file (the one at the environment level, e.g prod/env.hcl) should define a generate block to generate the AWS provider configuration to assume the role for that environment. For example, if you wanted to assume the role arn:aws:iam::0123456789:role/terragrunt in all the units for the prod account, you would put the following in prod/env.hcl: generate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite_terragrunt\" contents = <<EOFprovider \"aws\" { assume_role { role_arn = \"arn:aws:iam::0123456789:role/terragrunt\" }}EOF} This instructs Terragrunt to create the file provider.tf in the working directory where Terragrunt calls tofu/terraform before it runs any of the OpenTofu/Terraform commands (e.g plan, apply, validate, etc). This allows you to inject this provider configuration for any unit that includes the env.hcl file. To include this in the child configurations (e.g app/terragrunt.hcl), you would update all the units to include this configuration using the include block: include \"env\" { path = find_in_parent_folders(\"env.hcl\")} The include block tells Terragrunt to use the exact same Terragrunt configuration from the env.hcl file specified via the path parameter. It behaves exactly as if you had copy/pasted the OpenTofu/Terraform configuration from the included file generate configuration into the child, but this approach is much easier to maintain! Further ReadingSection titled “Further Reading” Note that if you’re considering this solution because you’re struggling with dynamic provider authentication in AWS, you may be interested in the dedicated documentation on working with multiple AWS accounts. Edit page Previous Wrap Up Next Stacks",
    "section": "features",
    "lastUpdated": "2025-10-23T16:41:00.935Z"
  },
  {
    "title": "Multiple Modules",
    "url": "https://terragrunt.gruntwork.io/docs/features/execute-terraform-commands-on-multiple-modules-at-once/",
    "content": "Stacks What are Stacks?Section titled “What are Stacks?” A stack in Terragrunt is a collection of related units that can be managed together. Stacks provide a way to: Deploy multiple infrastructure components with a single command Manage dependencies between units automatically Control the blast radius of changes Organize infrastructure into logical groups Terragrunt supports two approaches to defining stacks: Implicit Stacks: Created by organizing units in a directory structure. Explicit Stacks: Defined using terragrunt.stack.hcl files. Implicit Stacks: Directory-Based OrganizationSection titled “Implicit Stacks: Directory-Based Organization” The simplest way to create a stack is to organize your units in a directory structure. When you have multiple units in a directory, Terragrunt automatically treats that directory as a stack. Converting Terraform Modules to UnitsSection titled “Converting Terraform Modules to Units” Let’s say your infrastructure is defined across multiple OpenTofu/Terraform root modules: Directoryroot Directorybackend-app main.tfDirectoryfrontend-app main.tfDirectorymysql main.tfDirectoryvalkey main.tfDirectoryvpc main.tf To convert these to Terragrunt units, simply add a terragrunt.hcl file to each directory: Directoryroot Directorybackend-app main.tfterragrunt.hclDirectoryfrontend-app main.tfterragrunt.hclDirectorymysql main.tfterragrunt.hclDirectoryvalkey main.tfterragrunt.hclDirectoryvpc main.tfterragrunt.hcl Now you have an implicit stack! The root directory contains all your units and can be managed as a single stack. Working with Implicit StacksSection titled “Working with Implicit Stacks” Use the --all flag to run an OpenTofu/Terraform command on all units in the implicit stack in the current working directory: # Deploy all units discovered in the current working directoryterragrunt run --all apply # Plan changes across all units discovered in the current working directoryterragrunt run --all plan # Destroy all units discovered in the current working directoryterragrunt run --all destroy # View outputs from all units discovered in the current working directoryterragrunt run --all output You can also use the --graph flag to run an OpenTofu/Terraform command on all units in the DAG of the unit in the current working directory. # Run an OpenTofu/Terraform command on all units in the DAG of the unit in the current working directoryterragrunt run --graph apply Advantages of Implicit StacksSection titled “Advantages of Implicit Stacks” Simple: Just organize units in directory trees. Familiar: Works like recommended best practices for OpenTofu/Terraform module organization. Flexible: Easy to add/remove units by creating/deleting directories. Version Control Friendly: Each unit is a separate directory with its own history. Backwards Compatible: This has been the default way to work with Terragrunt for over eight years, and the majority of existing Terragrunt configurations use this approach. Limitations of Implicit StacksSection titled “Limitations of Implicit Stacks” Manual Management: Each unit must be manually created and configured. No Reusability: Patterns can’t be easily shared across environments. Repetitive: Similar configurations must be duplicated or referenced from includes. Explicit Stacks: Blueprint-Based GenerationSection titled “Explicit Stacks: Blueprint-Based Generation” For a more modern approach, you can define stacks using terragrunt.stack.hcl files. These are blueprints that programmatically generate units. What is a terragrunt.stack.hcl file?Section titled “What is a terragrunt.stack.hcl file?” A terragrunt.stack.hcl file is a blueprint that defines how to generate Terragrunt configuration programmatically. It tells Terragrunt: What units to create. Where to get their configurations from. Where to place them in the directory structure. What values to pass to each unit. The Two Types of BlocksSection titled “The Two Types of Blocks” unit blocks - Define Individual Infrastructure ComponentsSection titled “unit blocks - Define Individual Infrastructure Components” Purpose: Define a single, deployable piece of infrastructure. Use case: When you want to create a single piece of isolated infrastructure (e.g. a specific VPC, database, or application). Result: Generates a single terragrunt.hcl file in the specified path. stack blocks - Define Reusable Infrastructure PatternsSection titled “stack blocks - Define Reusable Infrastructure Patterns” Purpose: Define a collection of related units that can be reused. Use case: When you have a common, multi-unit pattern (like “dev environment” or “three-tier web application”) that you want to deploy multiple times. Result: Generates another terragrunt.stack.hcl file that can contain more units or stacks. Comparison: unit vs stack blocksSection titled “Comparison: unit vs stack blocks” Aspectunit blockstack blockPurposeDefine a single infrastructure component.Define a reusable collection of components.When to useFor specific, one-off infrastructure pieces.For patterns of infrastructure pieces that you want provisioned together.Generated outputA directory with a single terragrunt.hcl file.A directory with a terragrunt.stack.hcl file. Example: Simple Stack with UnitsSection titled “Example: Simple Stack with Units” unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = \"main\" cidr = \"10.0.0.0/16\" }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { engine = \"postgres\" version = \"13\" vpc_path = \"../vpc\" }} Running terragrunt stack generate creates: terragrunt.stack.hclDirectory.terragrunt-stack Directoryvpc terragrunt.hclterragrunt.values.hclDirectorydatabase terragrunt.hclterragrunt.values.hcl Example: Nested Stack with Reusable PatternsSection titled “Example: Nested Stack with Reusable Patterns” stack \"dev\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"dev\" values = { environment = \"development\" cidr = \"10.0.0.0/16\" }} stack \"prod\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"prod\" values = { environment = \"production\" cidr = \"10.1.0.0/16\" }} The referenced stack might contain: unit \"vpc\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/vpc?ref=v0.0.1\" path = \"vpc\" values = { vpc_name = values.environment cidr = values.cidr }} unit \"database\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//units/database?ref=v0.0.1\" path = \"database\" values = { environment = values.environment vpc_path = \"../vpc\" }} Working with Explicit StacksSection titled “Working with Explicit Stacks” # Generate units from the `terragrunt.stack.hcl` file in the current working directory.terragrunt stack generate # Deploy all generated units defined using the `terragrunt.stack.hcl` file in the current working directory (and any units generated by stacks in this file).terragrunt stack run apply # Or combine generation and deployment.terragrunt stack run apply # Automatically generates first Advantages of Explicit StacksSection titled “Advantages of Explicit Stacks” Reusability: Define patterns once, use them across environments. Consistency: Ensure all environments follow the same structure. Version Control: Version collections of infrastructure patterns alongside the units of infrastructure that make them up. Automation: Generate complex infrastructure from simple blueprints. Flexibility: Easy to create variations with different values. Limitations of Explicit StacksSection titled “Limitations of Explicit Stacks” Complexity: Requires understanding another configuration file. Generation Overhead: Units must be generated before use. Debugging: Generated files can be harder to debug if you accidentally generate files that are not what you intended. Choosing Between Implicit and Explicit StacksSection titled “Choosing Between Implicit and Explicit Stacks” Use Implicit Stacks When:Section titled “Use Implicit Stacks When:” You have a small number of units (1-10). Each unit is unique and not repeated across environments. You don’t mind a high file count. You’re just getting started with Terragrunt. You need maximum flexibility and transparency. Use Explicit Stacks When:Section titled “Use Explicit Stacks When:” You have multiple environments (dev, staging, prod). You want to reuse infrastructure patterns. You have many similar units that differ only in values. You want to version your infrastructure patterns. You’re building infrastructure catalogs or templates. The Complete WorkflowSection titled “The Complete Workflow” For Implicit Stacks:Section titled “For Implicit Stacks:” Organize: Create directories for each unit with terragrunt.hcl files. Configure: Set up inputs, dependencies, etc. in each unit. Deploy: Use terragrunt run --all apply to deploy all units. For Explicit Stacks:Section titled “For Explicit Stacks:” Catalog: Create a catalog of infrastructure patterns (using terragrunt.hcl files, terragrunt.stack.hcl files, etc.) in a git repository. Author: Write a terragrunt.stack.hcl file with unit and/or stack blocks. Generate: Run terragrunt stack generate to create the actual units*. Deploy: Run terragrunt stack run apply to deploy all units**. * Multiple commands (like stack run or run --all) automatically generate units from terragrunt.stack.hcl files for you. ** You can also just use run --all apply to deploy all units in the stack like you can with implicit stacks. Common PatternsSection titled “Common Patterns” Note These are simplified examples that show common high-level patterns.For more detailed examples, see the Gruntwork Terragrunt Infrastructure Catalog Stack Examples. Environment-based StacksSection titled “Environment-based Stacks” Create reusable environment patterns that can be instantiated for dev, staging, and production: stack \"dev\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"dev\" values = { environment = \"development\" cidr = \"10.0.0.0/16\" }} stack \"staging\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"staging\" values = { environment = \"staging\" cidr = \"10.1.0.0/16\" }} stack \"prod\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/environment?ref=v0.0.1\" path = \"prod\" values = { environment = \"production\" cidr = \"10.2.0.0/16\" }} Application StacksSection titled “Application Stacks” Define complete application patterns with all required components: stack \"web-app\" { source = \"git::git@github.com:acme/infrastructure-catalog.git//stacks/web-application?ref=v0.0.1\" path = \"web-app\" values = { app_name = \"my-web-app\" domain = \"example.com\" environment = \"production\" }} Benefits of Using StacksSection titled “Benefits of Using Stacks” Code ReusabilitySection titled “Code Reusability” Define infrastructure patterns once and reuse them across environments. Reduce duplication and maintenance overhead. Ensure consistency across deployments. Simplified ManagementSection titled “Simplified Management” Generate multiple units from a single configuration file. Deploy entire environments with a single command. Manage dependencies automatically. Version ControlSection titled “Version Control” Version your infrastructure patterns alongside your application code. Track changes to infrastructure patterns. Roll back infrastructure changes when needed. Team CollaborationSection titled “Team Collaboration” Share infrastructure patterns across teams. Standardize deployment approaches. Reduce onboarding time for new team members. Passing outputs between unitsSection titled “Passing outputs between units” Consider the following file structure: Directoryroot Directorybackend-app terragrunt.hclDirectorymysql terragrunt.hclDirectoryvalkey terragrunt.hclDirectoryvpc terragrunt.hcl Suppose that you wanted to pass in the VPC ID of the VPC that is created from the vpc unit in the folder structure above to the mysql unit as an input variable. Or that you wanted to pass in the subnet IDs of the private subnet that is allocated as part of the vpc unit. You can use the dependency block to extract those outputs and use them as inputs to the mysql unit. For example, suppose the vpc unit outputs the ID under the output named vpc_id. To access that output, you would specify in mysql/terragrunt.hcl: dependency \"vpc\" { config_path = \"../vpc\"} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} When you apply this unit, the output will be read from the vpc unit and passed in as an input to the mysql unit right before calling tofu apply/terraform apply. You can also specify multiple dependency blocks to access the outputs of multiple units. For example, in the above folder structure, you might want to reference the domain output of the valkey and mysql units for use as inputs in the backend-app unit. To access those outputs, you would specify the following in backend-app/terragrunt.hcl: dependency \"mysql\" { config_path = \"../mysql\"} dependency \"valkey\" { config_path = \"../valkey\"} inputs = { mysql_url = dependency.mysql.outputs.domain valkey_url = dependency.valkey.outputs.domain} Note that each dependency block results in a relevant status in the Terragrunt DAG. This means that when you run run --all apply on a config that has dependency blocks, Terragrunt will not attempt to deploy the config until all the units referenced in dependency blocks have been applied. So for the above example, the order for the run --all apply command would be: Deploy the VPC Deploy MySQL and valkey in parallel Deploy the backend-app If any of the units failed to deploy, then Terragrunt will not attempt to deploy the units that depend on them. Note: Not all blocks can access outputs passed by dependency blocks. See the section on Configuration parsing order for more information. Unapplied dependency and mock outputsSection titled “Unapplied dependency and mock outputs” Terragrunt will return an error if the unit referenced in a dependency block has not been applied yet. This is because you cannot actually fetch outputs out of an unapplied unit, even if there are no resources being created in the unit. This is most problematic when running commands that do not modify state (e.g run --all plan and run --all validate) on a completely new setup where no infrastructure has been deployed. You won’t be able to plan or validate a unit if you can’t determine the inputs. If the unit depends on the outputs of another unit that hasn’t been applied yet, you won’t be able to compute the inputs unless the dependencies are all applied. Of course, in real life usage, you typically need the ability to run run --all validate or run --all plan on a completely new set of infrastructure. To address this, you can provide mock outputs to use when a unit hasn’t been applied yet. This is configured using the mock_outputs attribute on the dependency block and it corresponds to a map that will be injected in place of the actual dependency outputs if the target config hasn’t been applied yet. Using a mock output is typically the best solution here, as you typically don’t actually care that an accurate value is used for a given value at this stage, just that it will plan successfully. When you actually apply the unit, that’s when you want to be sure that a real value is used. For example, in the previous scenario with a mysql unit and vpc unit, suppose you wanted to mock a value for the vpc_id during a run --all validate for the mysql unit. You can specify that in mysql/terragrunt.hcl: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"mock-vpc-id\" }} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} You can now run validate on this config before the vpc unit is applied because Terragrunt will use the map {vpc_id = \"mock-vpc-id\"} as the outputs attribute on the dependency instead of erroring out. What if you wanted to restrict this behavior to only the validate command? For example, you might not want to use the defaults for a plan operation because the plan doesn’t give you any indication of what is actually going to be created. You can use the mock_outputs_allowed_terraform_commands attribute to indicate that the mock_outputs should only be used when running those OpenTofu/Terraform commands. So to restrict the mock_outputs to only when validate is being run, you can modify the above terragrunt.hcl file to: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" } mock_outputs_allowed_terraform_commands = [\"validate\"]} inputs = { vpc_id = dependency.vpc.outputs.vpc_id} Note that indicating validate means that the mock_outputs will be used either with validate or with run --all validate. You can also use skip_outputs on the dependency block to specify the dependency without pulling in the outputs: dependency \"vpc\" { config_path = \"../vpc\" skip_outputs = true} When skip_outputs is used with mock_outputs, mocked outputs will be returned without attempting to load outputs from OpenTofu/Terraform. This can be useful when you disable backend initialization (remote_state.disable_init) in CI for example. dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" } skip_outputs = true} You can also use mock_outputs_merge_strategy_with_state on the dependency block to merge mocked outputs and real outputs: dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"temporary-dummy-id\" new_output = \"temporary-dummy-value\" } mock_outputs_merge_strategy_with_state = \"shallow\"} If real outputs only contain vpc_id, dependency.outputs will contain a real value for vpc_id and a mocked value for new_output. Stack outputsSection titled “Stack outputs” When defining a stack using a terragrunt.stack.hcl file, you also have the ability to interact with the aggregated outputs of all the units in the stack. To do this, use the stack output command (not the stack run output command). $ terragrunt stack outputbackend_app = { domain = \"backend-app.example.com\"}frontend_app = { domain = \"frontend-app.example.com\"}mysql = { endpoint = \"terraform-20250504140737772400000001.abcdefghijkl.us-east-1.rds.amazonaws.com\"}valkey = { endpoint = \"serverless-valkey-01.amazonaws.com\"}vpc = { vpc_id = \"vpc-1234567890\"} This will return a single aggregated value for all the outputs of all the units in the stack. Dependencies between unitsSection titled “Dependencies between units” You can also specify dependencies without accessing any of the outputs of units. Consider the following file structure: Directoryroot Directorybackend-app terragrunt.hclDirectoryfrontend-app terragrunt.hclDirectorymysql terragrunt.hclDirectoryvalkey terragrunt.hclDirectoryvpc terragrunt.hcl Let’s assume you have the following dependencies between OpenTofu/Terraform units: backend-app depends on mysql, valkey, and vpc frontend-app depends on backend-app and vpc mysql depends on vpc valkey depends on vpc vpc has no dependencies You can express these dependencies in your terragrunt.hcl config files using a dependencies block. For example, in backend-app/terragrunt.hcl you would specify: dependencies { paths = [\"../vpc\", \"../mysql\", \"../valkey\"]} Similarly, in frontend-app/terragrunt.hcl, you would specify: dependencies { paths = [\"../vpc\", \"../backend-app\"]} Once you’ve specified these dependencies in each terragrunt.hcl file, Terragrunt will be able to perform updates respecting the DAG of dependencies. For the example at the start of this section, the order of runs for the run --all apply command would be: Deploy the VPC Deploy MySQL and valkey in parallel Deploy the backend-app Deploy the frontend-app Any error encountered in an individual unit during a run --all command will prevent Terragrunt from proceeding with the deployment of any dependent units. To check all of your dependencies and validate the code in them, you can use the run --all validate command. Note During destroy runs, Terragrunt will try to find all dependent units and show a confirmation prompt with a list of detected dependencies.This is because Terragrunt knows that once resources in a dependency are destroyed, any commands run on dependent units may fail.For example, if destroy was called on the Valkey unit, you’d be asked for confirmation, as the backend-app depends on Valkey. You can suppress the prompt by using the --non-interactive flag. Visualizing the DAGSection titled “Visualizing the DAG” To visualize the dependency graph you can use the dag graph command (similar to the terraform graph command). The graph is output in DOT format. The typical program used to render this file format is GraphViz, but many web services are available that can do this as well. terragrunt dag graph | dot -Tsvg > graph.svg The example above generates the following graph: Note that this graph shows the dependency relationship in the direction of the arrow, with the tip pointing to the dependency (e.g. frontend-app depends on backend-app). For most commands, Terragrunt will run in the opposite direction, however (e.g. backend-app would be applied before frontend-app). The exception to this rule is during the destroy (and plan -destroy) command, where Terragrunt will run in the direction of the arrow (e.g. frontend-app would be destroyed before backend-app). Testing multiple units locallySection titled “Testing multiple units locally” If you are using Terragrunt to download remote OpenTofu/Terraform modules and all of your units have the source parameter set to a Git URL, but you want to test with a local checkout of the code, you can use the --source parameter to override that value: terragrunt run --all plan --source /source/modules If you set the --source parameter, the run --all command will assume that parameter is pointing to a folder on your local file system that has a local checkout of all of your OpenTofu/Terraform modules. For each unit that is being processed via a run --all command, Terragrunt will: Read in the source parameter in that unit’s terragrunt.hcl file. Parse out the path (the portion after the double-slash). Append the path to the --source parameter to create the final local path for that unit. For example, consider the following terragrunt.hcl file: terraform { source = \"git::git@github.com:acme/infrastructure-modules.git//networking/vpc?ref=v0.0.1\"} Running the following: terragrunt run --all apply --source /source/infrastructure-modules Will result in a unit with the configuration for the source above being resolved to /source/infrastructure-modules//networking/vpc. Limiting run parallelismSection titled “Limiting run parallelism” By default, Terragrunt will not impose a limit on the number of units it executes when it traverses the dependency graph, meaning that if it finds 5 units without dependencies, it’ll run OpenTofu/Terraform 5 times in parallel, once in each unit. Sometimes, this can create a problem if there are a lot of units in the dependency graph, like hitting a rate limit on a cloud provider. To limit the maximum number of unit executions at any given time use the --parallelism [number] flag terragrunt run --all apply --parallelism 4 Saving OpenTofu/Terraform plan outputSection titled “Saving OpenTofu/Terraform plan output” A powerful feature of OpenTofu/Terraform is the ability to save the result of a plan as a binary or JSON file using the -out flag. Terragrunt provides special tooling in run --all execution to ensure that the saved plan for a run --all against a stack has a corresponding entry for each unit in the stack in a directory structure that mirrors the stack structure. To save plan against a stack, use the --out-dir flag (or TG_OUT_DIR environment variable) as demonstrated below: $ terragrunt run --all plan --out-dir /tmp/tfplan Directoryapp1 tfplan.tfplanDirectoryapp2 tfplan.tfplanDirectoryapp3 tfplan.tfplanDirectoryproject-2 Directoryproject-2-app1 tfplan.tfplan $ terragrunt run --all --out-dir /tmp/tfplan apply For planning a destroy operation, use the following commands: terragrunt run --all --out-dir /tmp/tfplan plan -destroyterragrunt run --all --out-dir /tmp/tfplan apply To save plan in json format use --json-out-dir flag (or TG_JSON_OUT_DIR environment variable): terragrunt run --all --json-out-dir /tmp/json plan Directoryapp1 tfplan.jsonDirectoryapp2 tfplan.jsonDirectoryapp3 tfplan.jsonDirectoryproject-2 Directoryproject-2-app1 tfplan.json terragrunt run --all --out-dir /tmp/all --json-out-dir /tmp/all plan Directoryapp1 tfplan.jsontfplan.tfplanDirectoryapp2 tfplan.jsontfplan.tfplanDirectoryapp3 tfplan.jsontfplan.tfplanDirectoryproject-2 Directoryproject-2-app1 tfplan.jsontfplan.tfplan To recap: The plan for each unit in a stack is saved in the same hierarchy as the unit structure. The file name for plan binaries are tfplan.tfplan and tfplan.json for plan JSON. JSON plan files can’t be used with terragrunt run --all apply command, only binary plan files can be used. Output directories can be combined which will lead to saving both binary and JSON plans. Nested StacksSection titled “Nested Stacks” Note that you can also have nested stacks. For example, consider the following file structure: Directoryroot Directoryus-east-1 Directoryapp terragrunt.hclDirectorydb terragrunt.hclDirectoryus-west-2 Directoryapp terragrunt.hclDirectorydb terragrunt.hcl In this example, there’s the root stack, that contains all the infrastructure you’ve defined so far, and there’s also the us-east-1 and us-west-2 stacks, that contain the infrastructure for the app and db units in those regions. You can run run --all commands at any depth of the stack to run the units in that stack and all of its children. For example, to run all the units in the us-east-1 stack, you can run: cd root/us-east-1terragrunt run --all apply Terragrunt will only include the units in the us-east-1 stack and its children in the queue of units to run (unless external dependencies are pulled in, as discussed in the run —all command). Generally speaking, this is the primary tool Terragrunt users use to control the blast radius of their changes. For the most part, it is the current working directory that determines the blast radius of a run --all command. In addition to using your working directory to control what’s included in a run queue, you can also use flags like —include-dir and —exclude-dir to explicitly control what’s included in a run queue within a stack, or outside of it. There are more flags that control the behavior of the run command, which you can find in the run docs. Using Local State with StacksSection titled “Using Local State with Stacks” When using Terragrunt Stacks, you might want to use local state files instead of remote state for development, testing, or specific use cases. However, this presents a challenge because the generated .terragrunt-stack directory can be safely deleted and regenerated using terragrunt stack clean && terragrunt stack generate, which would normally cause local state files to be lost. To solve this problem, you can configure your stack to store state files outside of the .terragrunt-stack directory, in a persistent location that survives stack regeneration. ConfigurationSection titled “Configuration” Here’s how to configure local state that persists across stack regeneration: 1. Create a root.hcl file with local backend configuration: remote_state { backend = \"local\" generate = { path = \"backend.tf\" if_exists = \"overwrite_terragrunt\" } config = { path = \"${get_parent_terragrunt_dir()}/.terragrunt-local-state/${path_relative_to_include()}/tofu.tfstate\" }} 2. Create your stack definition: unit \"vpc\" { source = \"${find_in_parent_folders(\"units/vpc\")}\" path = \"vpc\"} unit \"database\" { source = \"${find_in_parent_folders(\"units/database\")}\" path = \"database\"} unit \"app\" { source = \"${find_in_parent_folders(\"units/app\")}\" path = \"app\"} 3. Configure your units to include the root configuration: include \"root\" { path = find_in_parent_folders(\"root.hcl\")} terraform { source = \".\"} 4. Add a .gitignore file to exclude state files from version control: .terragrunt-local-state Important: Local state files should never be committed to version control as they may contain sensitive information and can cause conflicts when multiple developers work on the same infrastructure. How It WorksSection titled “How It Works” The key insight is using path_relative_to_include() in the state path configuration. This function returns the relative path from each unit to the root.hcl file, creating unique state file paths like: .terragrunt-local-state/live/.terragrunt-stack/vpc/tofu.tfstate.terragrunt-local-state/live/.terragrunt-stack/database/tofu.tfstate.terragrunt-local-state/live/.terragrunt-stack/app/tofu.tfstate Since these state files are stored in .terragrunt-local-state/ (outside of .terragrunt-stack/), they persist when you run: terragrunt stack clean && terragrunt stack generate Directory StructureSection titled “Directory Structure” After running the stack, your directory structure will look like this: Directory. root.hcl.gitignore (Excludes .terragrunt-local-state)Directory.terragrunt-local-state/ (Persistent state files - ignored by git) Directorylive/ Directory.terragrunt-stack/ Directoryvpc/ tofu.tfstateDirectorydatabase/ tofu.tfstateDirectoryapp/ tofu.tfstateDirectorylive/ terragrunt.stack.hclDirectory.terragrunt-stack/ (Generated stack - can be deleted) Directoryvpc/ terragrunt.hclmain.tfDirectorydatabase/ terragrunt.hclmain.tfDirectoryapp/ terragrunt.hclmain.tfDirectoryunits/ (Reusable unit definitions) Directoryvpc/…Directorydatabase/…Directoryapp/… BenefitsSection titled “Benefits” This approach provides several advantages: State Persistence: State files survive stack regeneration Isolation: Each unit gets its own state file Consistency: Directory structure mirrors the stack layout Flexibility: You can switch between local and remote state easily by changing the backend configuration Example WorkflowSection titled “Example Workflow” # Initial setupterragrunt stack generateterragrunt stack run apply # Later, regenerate the stack without losing stateterragrunt stack cleanterragrunt stack generate # Verify existing resources are recognizedterragrunt stack run plan # Should show \"No changes\" This pattern is particularly useful for development environments, testing scenarios, or when you need to maintain local state for compliance or security reasons while still benefiting from Terragrunt’s stack management capabilities. Next StepsSection titled “Next Steps” Now that you understand both implicit and explicit stacks, you can: Learn about the detailed syntax for unit and stack blocks Explore the stack commands for generating and managing stacks Understand how to pass values between units Tip Pro Tip: Start with implicit stacks to get familiar with the concept, then gradually introduce explicit stacks for reusable patterns as your infrastructure grows. Learning moreSection titled “Learning more” If you’d like more advanced examples on stacks, check out the terragrunt-infrastructure-catalog-example repository. These have full-featured examples of stacks that deploy real, stateful infrastructure in an AWS account. Edit page Previous Units Next Includes",
    "section": "features",
    "lastUpdated": "2025-10-23T16:41:01.098Z"
  }
]